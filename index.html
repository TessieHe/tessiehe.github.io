<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>凛冬将至</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="从爪印判断，这是头雄狮">
<meta property="og:type" content="website">
<meta property="og:title" content="凛冬将至">
<meta property="og:url" content="http://wangdongdong122.github.io/index.html">
<meta property="og:site_name" content="凛冬将至">
<meta property="og:description" content="从爪印判断，这是头雄狮">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Dongdong Wang">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="凛冬将至" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 5.4.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">凛冬将至</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">冬天的故事</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS 订阅"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="搜索"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://wangdongdong122.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-2021-06-23-搭建hexo博客" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/06/23/2021-06-23-%E6%90%AD%E5%BB%BAhexo%E5%8D%9A%E5%AE%A2/" class="article-date">
  <time class="dt-published" datetime="2021-06-23T05:02:38.000Z" itemprop="datePublished">2021-06-23</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/06/23/2021-06-23-%E6%90%AD%E5%BB%BAhexo%E5%8D%9A%E5%AE%A2/">搭建hexo博客</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="安装hexo"><a href="#安装hexo" class="headerlink" title="安装hexo"></a>安装hexo</h2><p>参考内容：<a target="_blank" rel="noopener" href="https://www.jianshu.com/p/9bbae1d105be">https://www.jianshu.com/p/9bbae1d105be</a></p>
<h2 id="配置hexo与更换Next主题"><a href="#配置hexo与更换Next主题" class="headerlink" title="配置hexo与更换Next主题"></a>配置hexo与更换Next主题</h2><p><strong>下载主题</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/iissnan/hexo-theme-next themes/next</span><br></pre></td></tr></table></figure>
<p>由于github被墙，使用代理科学上网后，clone经常报<code>OpenSSL SSL_connect</code>的错，因此我直接在网页上下载的zip包，解压到<code>themes/next</code>文件夹。</p>
<p><strong>页面报错</strong></p>
<p>跟换主题后重新<code>hexo s</code>，页面无法显示内容，提示以下信息：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;% extends &#x27;_layout.swig&#x27; %&#125; &#123;% import &#x27;_macro/post.swig&#x27; as post_template %&#125; &#123;% import &#x27;_macro/sidebar.swig&#x27; as sidebar_template %&#125; &#123;% block title %&#125;&#123;&#123; page.title &#125;&#125; | &#123;&#123; config.title &#125;&#125;&#123;% endblock %&#125; &#123;% block page_class %&#125;page-post-detail&#123;% endblock %&#125; &#123;% block content %&#125;</span><br><span class="line">&#123;&#123; post_template.render(page) &#125;&#125;</span><br><span class="line">&#123;% if theme.jiathis %&#125; &#123;% include &#x27;_partials/share/jiathis.swig&#x27; %&#125; &#123;% elseif theme.baidushare %&#125; &#123;% include &#x27;_partials/share/baidushare.swig&#x27; %&#125; &#123;% elseif theme.add_this_id %&#125; &#123;% include &#x27;_partials/share/add-this.swig&#x27; %&#125; &#123;% elseif theme.duoshuo_shortname and theme.duoshuo_share %&#125; &#123;% include &#x27;_partials/share/duoshuo_share.swig&#x27; %&#125; &#123;% endif %&#125;</span><br><span class="line">&#123;% endblock %&#125; &#123;% block sidebar %&#125; &#123;&#123; sidebar_template.render(true) &#125;&#125; &#123;% endblock %&#125; &#123;% block script_extra %&#125; &#123;% include &#x27;_scripts/pages/post-details.swig&#x27; %&#125; &#123;% endblock %&#125;</span><br></pre></td></tr></table></figure>
<p>根据<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_39898645/article/details/109181736">博客</a>介绍，原因是hexo在5.0之后把swig给删除了需要自己手动安装，安装后解决了该问题。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm i hexo-renderer-swig</span><br></pre></td></tr></table></figure>
<p><strong>显示公式</strong></p>
<p>由于我的博客中有大量公式，页面中无法显示。参考<a target="_blank" rel="noopener" href="https://www.jianshu.com/p/9b9c241146bc">博客</a>解决了该问题，问题的核心是配置好两个因素：mathjax和kramed。</p>
<p>添加mathjax</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm uninstall hexo-math --save</span><br><span class="line">npm install hexo-renderer-mathjax --save</span><br></pre></td></tr></table></figure>
<p>换渲染引擎</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm uninstall hexo-renderer-marked --save</span><br><span class="line">npm install hexo-renderer-kramed --save</span><br></pre></td></tr></table></figure>
<p>由于我之前瞎试，装过pandoc，也要一起卸掉：<code>npm uninstall hexo-renderer-pandoc --save</code></p>
<p>修改渲染引擎的bug：到博客根目录下，找到node_modules\kramed\lib\rules\inline.js，第11行的 escape 变量：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">//escape: /^\\([\\`*&#123;&#125;\[\]()#$+\-.!_&gt;])/,</span><br><span class="line">escape: /^\\([`*\[\]()#$+\-.!_&gt;])/,</span><br></pre></td></tr></table></figure>
<p>第20行的em变量：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">//em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,</span><br><span class="line">em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,</span><br></pre></td></tr></table></figure>
<p>配置.\themes\next\_config.yml</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># MathJax Support</span><br><span class="line">mathjax:</span><br><span class="line">  enable: true</span><br><span class="line">  per_page: true</span><br><span class="line">  engine: mathjax</span><br><span class="line">  cdn: //cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML</span><br></pre></td></tr></table></figure>
<p>在文章的Front-matter里打开mathjax开关</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">title: Hierarchical Attention Networks for Document Classification</span><br><span class="line">date: 2021-06-23 09:26:17</span><br><span class="line">tags:</span><br><span class="line">    - 深度学习</span><br><span class="line">    - Attention</span><br><span class="line">    - Transformer</span><br><span class="line">    - 机器学习</span><br><span class="line">    - 每日论文</span><br><span class="line">    - 经典算法</span><br><span class="line">    - NLP</span><br><span class="line">mathjax: true</span><br></pre></td></tr></table></figure>
<p>重启以下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hexo clean</span><br><span class="line">hexo generate</span><br><span class="line">hexo s</span><br></pre></td></tr></table></figure>
<p>到这里我的公式一部分能显示，一部分不能显示。继续查了查，发现别人也有这个情况，比如多行公式的时候显示不了，是因为不能出现连续的大括号<code>&#123;&#123;</code>​。我怀疑我的也是类似的问题，于是试了一下在我的<code>:公式</code>的结构中，把冒号删了，居然所有公式都正常显示了，然后我又把冒号加回去，还是都可以显示！！！不知道上面那堆设置需要时间起作用还是我的修改触发了什么，总之问题解决了。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://wangdongdong122.github.io/2021/06/23/2021-06-23-%E6%90%AD%E5%BB%BAhexo%E5%8D%9A%E5%AE%A2/" data-id="ckq9m113m000gqoppg8fc5nrw" data-title="搭建hexo博客" class="article-share-link">分享</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/bug/" rel="tag">bug</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-2021-06-22-Decomposable-Attention" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/06/22/2021-06-22-Decomposable-Attention/" class="article-date">
  <time class="dt-published" datetime="2021-06-22T01:29:17.000Z" itemprop="datePublished">2021-06-22</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/06/22/2021-06-22-Decomposable-Attention/">A Decomposable Attention Model for Natural Language Inference</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>Self-Attention谁先提出的，各文章里写的不一样吗，<a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">Attention Is All You Need</a>中说是<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1606.01933.pdf">Jakob et al.2016</a>年提出的，<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1904.02874.pdf">An Attentive Survey of Attention Models</a>中说是<a target="_blank" rel="noopener" href="https://www.aclweb.org/anthology/N16-1174.pdf">Yang et al. 2016</a>，本篇介绍前者。</p>
<p>Abstract：simpler and more lightweight，比之前的模型参数少一个数量级，而且不依赖与任何词的顺序信息</p>
<p>本文的Decomposable是指其将自然语言中的三个类型的任务分解了，一共是 </p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>近年来，Natural language inference(NLI)领域中提出的模型都数据量巨大，计算成本非常高。但实际上NLI中，往往是只需要少量的局域信息，然后将这些局域信息汇总起来进行预测即可。</p>
<h2 id="Approach"><a href="#Approach" class="headerlink" title="Approach"></a>Approach</h2><p><strong>模型结构</strong></p>
<p>  <img src="../images/Decomposable-Attention.images/1624336820545_src" alt="img" style="zoom: 67%;" /> </p>
<p><strong>Input representation</strong></p>
<p>输入是两个句子（不一定等长，$l_a$和$l_b$），句子是由每个词的embedding向量（长度$d$）组成的矩阵，label是多分类的one-hot标签（类别数为$C$）。</p>
<script type="math/tex; mode=display">
\begin{eqnarray*}
input\ sentence\ matrix1&:& \textbf{a}=(a_1,...,a_{l_a}) ,\ 
l_a:length
\\         
input\ sentence\ matrix2&:& \textbf{b}=(b_1,...,b_{l_b}) ,\ 
l_b:length     
\\
word\ embedding\ vector&:&a_i, b_j ∈ R^d
\\
indicator\ vector&:& \textbf{y}^{(n)}=(y^{(n)}_1,...,y^{(n)}_C),\ C:classes\ number 
\\

training\ data&:&\{ \textbf{a}^{(n)},\textbf{b}^{(n)},\textbf{y}^{(n)} \}^N_{n=1}
\\
test\ data&:&(\textbf{a},\textbf{b})

\end{eqnarray*}</script><p>$\textbf{a}$和$\textbf{b}$可以做一些变换之后再输入模型，标准版模型就输入$\textbf{a}$和$\textbf{b}$了。</p>
<h3 id="Attend"><a href="#Attend" class="headerlink" title="Attend"></a>Attend</h3><p>首先要计算相关性权重</p>
<script type="math/tex; mode=display">
e_{ij}:=F^{'}{(a_i,b_j)}

:=F(a_i)^TF(b_j)</script><p>这里有一个计算简化，如果按照$F^{‘}{(a_i,b_j)}$计算，则需要计算$l_a×l_b$次$F^{‘}{(·)}$，但按照后者计算则只用计算$l_a+l_b$次$F{(·)}$。</p>
<p>然后对weight标准化，并根据标准化的权重加权得到新的词表达：$\beta_i,\alpha_j$。需要注意的是，此处命名有点反直觉，$\beta_i$的计算中，query是$a_i$，Key和Value都是$\textbf{b}$。$\alpha_j$则相反，query是$b_i$，Key和Value都是$\textbf{a}$。</p>
<script type="math/tex; mode=display">
\begin{eqnarray*}
\beta_i:=\sum^{l_b}_{j=1}
\frac{exp(e_{ij})}{\sum^{l_b}_{k=1}exp(e_{ik})}b_j
\\
\\
\alpha_j:=\sum^{l_a}_{i=1}
\frac{exp(e_{ij})}{\sum^{l_a}_{k=1}exp(e_{kj})}a_i
\end{eqnarray*}</script><h3 id="Compare"><a href="#Compare" class="headerlink" title="Compare"></a>Compare</h3><p>接下来，分别比较$\{(a_i,\beta_i)\}^{l_a}_{i=1}$和$\{(b_j,\alpha_j)\}^{l_b}_{j=1}$中每个pair，也就是看看用$\textbf{b}$表示出来的$a_i$，即$\beta_i$和真正的$a_i$有多像，如果很像（反映在$\textbf{v}_{1,i}$中），则证明句子$\textbf{b}$中有$a_i$的信息。</p>
<script type="math/tex; mode=display">
\begin{eqnarray*}
\textbf{v}_{1,i}:=G([a_i,\beta_i])
\ \ \forall i\in [1,...,l_a]
\\
\\
\textbf{v}_{2,j}:=G([b_j,\alpha_j])
\ \ \forall j\in [1,...,l_b]
\end{eqnarray*}</script><p>其中$[·，·]$表示concatenation，$G$在论文中是全连接。因为这个计算次数是线性的，所以不用像前面一样将$a_i,\beta_i$拆开计算了。</p>
<h3 id="Aggregate"><a href="#Aggregate" class="headerlink" title="Aggregate"></a>Aggregate</h3><p>将每个词的比较向量聚合成句子的比较向量</p>
<script type="math/tex; mode=display">
\textbf{v}_1 = \sum^{l_a}_{i=1}\textbf{v}_{1,i}\ \ \ ,\ \ \ 
\textbf{v}_2 = \sum^{l_b}_{j=1}\textbf{v}_{2,j}</script><p>然后concat起来过分类器，得到预测结果</p>
<script type="math/tex; mode=display">
{\widehat{\textbf y}} = H([\textbf v_1,\textbf v_2])
,\ \ \ \ 
{\widehat{\textbf y}} \in R^C</script><h3 id="Intra-Sentence-Attention"><a href="#Intra-Sentence-Attention" class="headerlink" title="Intra-Sentence Attention"></a>Intra-Sentence Attention</h3><p>重点<strong>self-attention</strong>来了，前面的模型里，输入是简单的word embedding。这里提出一种增强输入表达的方法：intra-sentence attention，将句子中每个词之间的关系表示出来。</p>
<script type="math/tex; mode=display">
f_{ij}:=F_{intra}(a_i)^TF_{intra}(a_j),
\\
\\
a^{'}_i=\sum^{l_a}_{j=1}\frac{exp(f_{ij}+d_{i-j})}{exp(f_{ik}+d_{i-k})}a_j
\\\\</script><p>其中，$F_{intra}$是一个全连接，$f_{ij}$就表示$a_i$和$a_j$的相似程度。$d_{i-j}$是距离敏感度偏置项，作用是不让某个词的权重过小。</p>
<p>学点巴洛克风格的词</p>
<p>vanilla version：The “<strong>vanilla</strong> <strong>version</strong>“ is generally the version that has no customisation applied - it is the “regular”,  “ordinary” or “plain old” version. For a lot of consumer based software - this would be the only version. You would not build custom versions for every user.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://wangdongdong122.github.io/2021/06/22/2021-06-22-Decomposable-Attention/" data-id="ckq9m113m000fqoppe01b2f8z" data-title="A Decomposable Attention Model for Natural Language Inference" class="article-share-link">分享</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Attention/" rel="tag">Attention</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/" rel="tag">NLP</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Transformer/" rel="tag">Transformer</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%AF%8F%E6%97%A5%E8%AE%BA%E6%96%87/" rel="tag">每日论文</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">深度学习</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95/" rel="tag">经典算法</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-2021-06-21-Hierarchical-Attention" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/06/21/2021-06-21-Hierarchical-Attention/" class="article-date">
  <time class="dt-published" datetime="2021-06-21T01:26:17.000Z" itemprop="datePublished">2021-06-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/06/21/2021-06-21-Hierarchical-Attention/">Hierarchical Attention Networks for Document Classification</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>Self-Attention谁先提出的，各文章里写的不一样吗，<a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">Attention Is All You Need</a>中说是<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1606.01933.pdf">Jakob.2016</a>年提出的，<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1904.02874.pdf">An Attentive Survey of Attention Models</a>中说是<a target="_blank" rel="noopener" href="https://www.aclweb.org/anthology/N16-1174.pdf">Yang et al. 2016</a>，本篇介绍后者。</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>核心思路：</p>
<ol>
<li>分层（hierarchical structure）：先构建“词 → 句子”级的表达，再聚合到文档级，即“句子 → 文档”</li>
<li>Attention：不同的词和句子包含的信息和重要程度都依赖于上下文，为了将其考虑进来，所以作者用两层的Attention</li>
</ol>
<p>作者没有提self-attention，应该是还没意识到这一点的牛逼之处。</p>
<h2 id="Hierarchical-Attention-Networks"><a href="#Hierarchical-Attention-Networks" class="headerlink" title="Hierarchical Attention Networks"></a>Hierarchical Attention Networks</h2><h3 id="encoder"><a href="#encoder" class="headerlink" title="encoder"></a>encoder</h3><p>encoder采用GRU产生，原理及结构省略</p>
<h3 id="Hierarchical-Attention"><a href="#Hierarchical-Attention" class="headerlink" title="Hierarchical Attention"></a>Hierarchical Attention</h3><p>数据表达</p>
<ul>
<li>sentences $\vec{s_i}$ ,$i=1,2,…L$</li>
<li>words represents: $w_{it}$, $t ∈ [1, T]$,   $\vec{s_i}$ contains $T$ words</li>
</ul>
<h4 id="Word-Encoder"><a href="#Word-Encoder" class="headerlink" title="Word Encoder"></a>Word Encoder</h4><p>先embedding，过双向GRU，将隐层concatenate起来</p>
<ol>
<li>word embedding: $W_e$, $x_{ij}=W_ew_{ij}$</li>
<li>forward GRU: $\overset{\rightarrow}{h_{it}}=\overset{\rightarrow}{GRU}(x_{it}),\ t ∈ [1, T]$</li>
<li>backward GRU:  $\overset{\leftarrow}{h_{it}}=\overset{\leftarrow}{GRU}(x_{it}),\ t ∈ [T, 1]$</li>
<li>concatenate:  $h_{it}=[\overset{\rightarrow}{h_{it}},\overset{\leftarrow}{h_{it}}]$</li>
</ol>
<h4 id="Word-Attention"><a href="#Word-Attention" class="headerlink" title="Word Attention"></a>Word Attention</h4><p>将对句子含义起重要作用的词提取出来，聚合成一个句子向量。先将所有的（$t ∈ [1, T]$）$h_{it}$过全连接得到Key: $u_{it}$；然后和随机变量的query: $u_w$求相似度分布: $\alpha$；最后将最开始的  $h_{it}$作为Value，加权得到sentence vector: $s_i$。所有信息都是从$h_{it}$中得到。</p>
<script type="math/tex; mode=display">
\begin{eqnarray*}
u_{it} &=& tanh(W_wh_{it}+b_w) \tag{FC layer}  \\                 
\\
\alpha_{it} &=& \frac{exp(u^{T}_{it}u_w)}{\sum_{t}{exp(u^{T}_{it}u_w)}}  
\tag{measure similarity & normalize}
\\
\\
s_i &=& \sum_{t}{\alpha_{it}h_{it}}
\tag{weighted sum}


\end{eqnarray*}</script><p>其中$ u_w$(word context vector)是随机初始化，然后在训练过程中学习的，可以当做是一个固定的query，用来表示这个句子中重要的信息。</p>
<p>维度信息：每个句子只产生一个向量$s_i$，其长度和单个词的BiGRU隐层concat之后的向量$h_{it}$长度相同（不一定等于词向量$w_{it}$的长度）。</p>
<h4 id="Sentence-Encoder"><a href="#Sentence-Encoder" class="headerlink" title="Sentence Encoder"></a>Sentence Encoder</h4><p>句子的encoder也和词的类似，先过bidirectional GRU然后concatenate。</p>
<ol>
<li>forward GRU：$\overset{\rightarrow}{h_{i}}=\overset{\rightarrow}{GRU}(s_{i}),\ i ∈ [1, L]$</li>
<li>backward GRU: $\overset{\leftarrow}{h_{i}}=\overset{\leftarrow}{GRU}(s_{i}),\ i ∈ [L, 1]$</li>
<li>concatenate: $h_{i}=[\overset{\rightarrow}{h_{i}},\overset{\leftarrow}{h_{i}}]$</li>
</ol>
<h4 id="Sentence-Attention"><a href="#Sentence-Attention" class="headerlink" title="Sentence Attention"></a>Sentence Attention</h4><p>这部分也和Word Attention部分一样，只是换了个层次</p>
<script type="math/tex; mode=display">
\begin{eqnarray*}u_{i} &=& tanh(W_sh_{i}+b_s) 
 \tag{FC layer}  
 \\                 
 \\
 \alpha_{i} &=& \frac{exp(u^{T}_{i}u_s)}{\sum_{i}{exp(u^{T}_{i}u_s)}}  
 \tag{measure similarity & normalize}
 \\
 \\
 v &=& \sum_{i}{\alpha_{i}h_{i}}
 \tag{weighted sum}

 \end{eqnarray*}</script><p>这里就将一个文档表示成一个向量$v$， 其长度和单个句子的BiGRU隐层concat之后的向量$h_{i}$长度相同。</p>
<h3 id="Document-Classification"><a href="#Document-Classification" class="headerlink" title="Document Classification"></a>Document Classification</h3><p>这部分很简单，文档向量$v$过softmax，然后用log loss训练。</p>
<script type="math/tex; mode=display">
\begin{eqnarray*}

p &=& softmax(W_cv+b_c) 
 \tag{softmax}  
 \\                 
 \\
L &=& -\sum_{d}{log\ p_{dj}}
 \tag{log loss}

 \end{eqnarray*}</script><p>其中，$j$是文档$d$的标签，只对正确标签计算loss。</p>
<h2 id="Results-and-analysis"><a href="#Results-and-analysis" class="headerlink" title="Results and analysis"></a>Results and analysis</h2><p>  Yelp 2013上的两个文档，左边是给出了5星好评的，右边是0星差评的。模型可以捕捉到那些词重要。<img src="../images/Hierarchical-Attention.images/1624323245414_src-1624427147187" alt="img" style="zoom: 50%;" /></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://wangdongdong122.github.io/2021/06/21/2021-06-21-Hierarchical-Attention/" data-id="ckq9m11380000qopp0o158znw" data-title="Hierarchical Attention Networks for Document Classification" class="article-share-link">分享</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Attention/" rel="tag">Attention</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/" rel="tag">NLP</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Transformer/" rel="tag">Transformer</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%AF%8F%E6%97%A5%E8%AE%BA%E6%96%87/" rel="tag">每日论文</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">深度学习</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95/" rel="tag">经典算法</a></li></ul>

    </footer>
  </div>
  
</article>



  


</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Attention/" rel="tag">Attention</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NLP/" rel="tag">NLP</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Transformer/" rel="tag">Transformer</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/bug/" rel="tag">bug</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%AF%8F%E6%97%A5%E8%AE%BA%E6%96%87/" rel="tag">每日论文</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">深度学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95/" rel="tag">经典算法</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签云</h3>
    <div class="widget tagcloud">
      <a href="/tags/Attention/" style="font-size: 20px;">Attention</a> <a href="/tags/NLP/" style="font-size: 20px;">NLP</a> <a href="/tags/Transformer/" style="font-size: 20px;">Transformer</a> <a href="/tags/bug/" style="font-size: 10px;">bug</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 20px;">机器学习</a> <a href="/tags/%E6%AF%8F%E6%97%A5%E8%AE%BA%E6%96%87/" style="font-size: 20px;">每日论文</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="font-size: 20px;">深度学习</a> <a href="/tags/%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95/" style="font-size: 20px;">经典算法</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/06/">六月 2021</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2021/06/23/2021-06-23-%E6%90%AD%E5%BB%BAhexo%E5%8D%9A%E5%AE%A2/">搭建hexo博客</a>
          </li>
        
          <li>
            <a href="/2021/06/22/2021-06-22-Decomposable-Attention/">A Decomposable Attention Model for Natural Language Inference</a>
          </li>
        
          <li>
            <a href="/2021/06/21/2021-06-21-Hierarchical-Attention/">Hierarchical Attention Networks for Document Classification</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2021 Dongdong Wang<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>