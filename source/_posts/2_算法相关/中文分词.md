---
title: 中文分词
date: 2022-03-01 11:32:38
tags:
    - 算法相关
    - default
categories: 
    - 算法相关
---

中文分词

<!-- more -->


[TOC]

# jieba分词

```python
import jieba
import jieba.analyse

def get_corpus(f_corpus):
    '''

    :param f_corpus: txt
    :return: list
    '''
    with open(f_corpus, encoding='utf-8') as f:
        lines = f.readlines()
    print('f_corpus lines: {}'.format(len(lines)))
    print(lines[0])
    return lines

def get_stopwords(f_stopwords):
    '''

    :param f_stopwords: txt
    :return: list
    '''
    with open(f_stopwords, encoding='utf-8') as f:
        stopwords = f.readlines()
    print('stop words lines: {}'.format(len(stopwords)))
    return stopwords
def get_keyword_dict(filename):
    dict = {}
    keys = []
    num=0
    with open(filename,'r',encoding='utf-8') as f:
        line = f.readline()
        while line :
            num +=1
            if num % 10000 ==0:
                print('line {}'.format(num))
            word = line.strip()
            word = get_keyword(word).strip('【').strip('】')
            if word not in keys:
                keys.append(word)
                dict[word] = 1
            else:
                dict[word] += 1
            line = f.readline()
    dict = sorted(dict.items(),key=lambda s:s[1],reverse=True)
    return dict
def main_count():
    '''
    统计词频
    '''
    f_corpus = 'part1.txt'
    f_stopwords = 'stopwords.txt'
    f_count_words = 'wordsCount_part1.txt'
    corpus=get_corpus(f_corpus) #list
    stopwords=get_stopwords(f_stopwords) #list
    word_dic=count_word(corpus,stopwords) #list
    print (word_dic)
    if not os.path.exists(f_count_words):
        os.system(r"touch {}".format(f_count_words))
    with open(f_count_words,'w') as f:
        for i in word_dic:
            res = i[0].strip()+'\t'+str(i[1])
            f.write(res+'\n')
    print("done!")

```

