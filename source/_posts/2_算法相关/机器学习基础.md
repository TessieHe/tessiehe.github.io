---
title: 机器学习基础
date: 2022-03-01 11:32:38
tags:
    - 算法相关
    - default
categories: 
    - 算法相关
---

机器学习基础

<!-- more -->


[TOC]

# Woe

WOE的全称是“weight of evidence”，即证据权重, WOE表示的含义即是"**当前分组中响应客户占所有响应客户的比例"和"当前分组中没有响应的客户占所有没有响应客户的比例**"的差异。先把分析变量进行分箱，每个分箱内的$w_{oe}$为
$$
woe_i=\frac{当前分组中响应客户占所有响应客户的比例}{当前分组中没有响应的客户占所有没有响应客户的比例}=ln\frac{P_{y_i}}{P_{n_i}}=ln\frac{y_1/y_2}{n_i/n_s}
$$

$$
woe_i=\frac{sum(y_i)/sum(y_s)}{sum(1-y_i)/sum(1-y_s)}
$$



该值绝对值越大说明变量区分能力越强

# IV

IV衡量的是某一个变量的信息量，从公式来看的话，相当于是自变量WOE值的一个加权求和，其值的大小决定了自变量对于目标变量的影响程度
$$
IV_i=(P_{y_i}-P_{n_i})*woe_i
$$
WOE 和 IV 都能表达某个分组对目标变量的预测能力。但实际中，我们通常选择 IV 而不是 WOE 的和来衡量变量预测的能力，这是为什么呢？首先，因为我们在衡量一个变量的预测能力时，我们所使用的指标值不应该是负数。从这意义上来说，IV 比 WOE 多乘以前面那个因子，就保证了它不会是负数；然后，乘以(Pyi−Pni)这个因子，体现出了变量当前分组中个体的数量占整体的比例，从而很好考虑了这个分组中样本占整体的比例，比例越低，这个分组对变量整体预测能力的贡献越低。相反，如果直接用 WOE 的绝对值加和，会因为该分组出现次数偏少的影响而得到一个很高的指标。

# AUC & KS

# 信息熵（information entropy）

衡量样本纯度，熵越小越纯,样本D有K类样本，其信息熵为
$$
Ent(D)=-\sum_{k=1}^{K}p_klog_2p_k
$$
