---
title: 自然语言处理笔记
date: 2022-03-01 11:32:38
tags:
    - 算法相关
    - default
categories: 
    - 算法相关
---

自然语言处理笔记

<!-- more -->


[TOC]

# 文本特征提取

## TF-IDF

[链接](<https://blog.csdn.net/The_lastest/article/details/79093407>)

> TF-IDF是传统的统计算法，用于评估一个词在一个文档集中对于某一个文档的重要程度。它与这个词在当前文档中的词频成正比，与文档集中的其他词频成反比

x:多个文档，如多个电影评论

y:针对每个文档可以提取关键词

# gensim

<https://blog.csdn.net/CoderPai/article/details/80250380>

```python
#train
from gensim.test.utils import common_texts, get_tmpfile
from gensim.models import Word2Vec
model_name='test'
path = get_tmpfile(model_name + '.model')
if params is None:
    params = {
        'size': 100,
        'window': 10,
        'min_count': 1,
        'workers': 4,
        'compute_loss': True
    }
model = Word2Vec([['我','是','天才'],['你'，'是'，'呵呵']] **params)  # get_latest_training_loss().
model.save(path)
#pred
model=Word2Vec.load(path)
 
```

- size: 词向量的维度。
- alpha: 模型初始的学习率。
- window: 表示在一个句子中，当前词于预测词在一个句子中的最大距离。
- min_count: 用于过滤操作，词频少于 min_count 次数的单词会被丢弃掉，默认值为 5。
- max_vocab_size: 设置词向量构建期间的 RAM 限制。如果所有的独立单词数超过这个限定词，那么就删除掉其中词频最低的那个。根据统计，每一千万个单词大概需要 1GB 的RAM。如果我们把该值设置为 None ，则没有限制。
- sample: 高频词汇的随机降采样的配置阈值，默认为 1e-3，范围是 (0, 1e-5)。
- seed: 用于随机数发生器。与词向量的初始化有关。
- workers: 控制训练的并行数量。
- min_alpha: 随着训练进行，alpha 线性下降到 min_alpha。
- sg: 用于设置训练算法。当 sg=0，使用 CBOW 算法来进行训练；当 sg=1，使用 skip-gram 算法来进行训练。
- hs: 如果设置为 1 ，那么系统会采用 hierarchica softmax 技巧。如果设置为 0（默认情况），则系统会采用 negative samping 技巧。
- negative: 如果这个值大于 0，那么 negative samping 会被使用。该值表示 “noise words” 的数量，一般这个值是 5 - 20，默认是 5。如果这个值设置为 0，那么 negative samping 没有使用。
- cbow_mean: 如果这个值设置为 0，那么就采用上下文词向量的总和。如果这个值设置为 1 （默认情况下），那么我们就采用均值。但这个值只有在使用 CBOW 的时候才起作用。
- hashfxn: hash函数用来初始化权重，默认情况下使用 Python 自带的 hash 函数。
- iter: 算法迭代次数，默认为 5。
- trim_rule: 用于设置词汇表的整理规则，用来指定哪些词需要被剔除，哪些词需要保留。默认情况下，如果 word count < min_count，那么该词被剔除。这个参数也可以被设置为 None，这种情况下 min_count 会被使用。
- sorted_vocab: 如果这个值设置为 1（默认情况下），则在分配 word index 的时候会先对单词基于频率降序排序。
- batch_words: 每次批处理给线程传递的单词的数量，默认是 10000。