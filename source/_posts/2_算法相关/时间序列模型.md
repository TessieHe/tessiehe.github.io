---
title: 时间序列模型
date: 2022-03-01 11:32:38
tags:
    - 算法相关
    - default
categories: 
    - 算法相关
---

时间序列模型

<!-- more -->


---
typora-copy-images-to: ./image
---

# 时间序列模型

## WEEK1

- 符号解释

  $x^{(i)<t>}$:  第i个样本的第t维分量

  $T_x^{(i)}$ : 第i个样本x的维度

- 主体抓取

  1. 多对多模型

  2. 不能用全连接，因为输入和输出的长度不定，而且输入矩阵太大

     ![E36DF04F-C0BD-42D3-9A0D-CA2F2B1C7DE9](/Users/tessiehe/Documents/study_notes/吴恩达时间序列笔记/image/E36DF04F-C0BD-42D3-9A0D-CA2F2B1C7DE9.png)


$a^{<0>} = \vec{0}$

$a^{<1>} = g(W_{aa}a^{<0>} +W_{ax}x^{<1>} +b_a)$

$\hat{y}^{<1>} = g(W_{ya}a^{<1>}+b_y)$

-  Forward propagation

$a^{<t>} = g(W_{aa}a^{<t-1>} +W_{ax}x^{<t-1>}+b_a)$

$\hat{y^{<t>}} = g(W_{ya}a^{<t>}+b_y)$

为了简化模型，可把$W_{ax},W_{aa}$横向排列成为$W_a$，$a^{<t-1>},x^{t}$纵向排列

![D71818E3-4031-4EF4-99F4-BD47FC6BD0C5](/Users/tessiehe/Documents/study_notes/吴恩达时间序列笔记/image/D71818E3-4031-4EF4-99F4-BD47FC6BD0C5.png)

-  Back propagation

   $L^{<t>} (\hat{y}^{<t>},y^{t}) = -y^{<t>}log(\hat{y})-(1-y^{<t>})log(1-\hat{y}^{<t>})$

   $L(\hat{y},y) = \sum_{t=1}^{T_x}L^{<t>}(\hat{y}^{<t>},y^{<t>})$

-  Different types of RNN

   ![FF3C6BD2-518A-480C-ADE5-3B71224C7DDB](/Users/tessiehe/Documents/study_notes/吴恩达时间序列笔记/image/FF3C6BD2-518A-480C-ADE5-3B71224C7DDB.png)

-  Language model

   -  tokenize (one hot)

   -  <UNK>来编码非常用单词

   -  目标：判断一个句子的概率

      -  训练：

         ![ßFC45A4AF-B524-425F-8C28-14FF8C16B802](/Users/tessiehe/Documents/study_notes/吴恩达时间序列笔记/image/FC45A4AF-B524-425F-8C28-14FF8C16B802.png)

-  Sample a sequence model from trained RNN

   -  初始化输入（零向量）
   -  按照预测softmax后的概率sample出一个词
   -  以新词作为输入，softmax预测下一个词的概率，按照概率分布sample出第二个词

-  RNN的梯度消失

   梯度爆炸可使用gradient clipping

-  GRU（Gradient Recurrent Unit）

   -  c:memory cell

      $c^{<t>} = a^{<t>}$

      $\hat{c}^{<t>}=tanh(W_c[c^{<t-1>},x^{<t>}]+b_c)$

      $\Gamma_u=\sigma(W_u[c^{<t-1>},x^{<t>}]+b_u)$   (u: update,$\Gamma$ 约为0或1)

      $c^{<t>} = \Gamma_u\hat{c}^{<t>} +(1-\Gamma_u)c^{<t-1>}$  （$\Gamma$维度和c一样；elemet wise multiply）

      ![05728660-E7EF-4289-9665-45E6653B03F5](/Users/tessiehe/Documents/study_notes/吴恩达时间序列笔记/image/05728660-E7EF-4289-9665-45E6653B03F5.png)

   -  Full GRU

      $\hat{c}^{<t>} = tanh(Wc[\Gamma_r*c^{<t-1>},x^{<t>}]+b_c)$

      $\Gamma_r=\sigma(W_r[c^{<t-1>},x^{t}]+b_c)$

      $\Gamma _u=\sigma(W_u[c^{<t-1>},x^{<t>}]+b_u)$

      $c^{<t>} = \Gamma_u*\hat{c}^{<t>}+(1-\Gamma_u)*c^{<t-1>}$

      $a^{<t>} = c^{<t>}$

      ​

-  LSTM (Long Short Term Memory)

   $\hat{c}^{<t>} = tanh(W_c[a^{<t-1>},x^{<t>}]+b_c)$

   $\Gamma_u=\sigma(W_u[a^{<t-1>},x^{<t>}]+b_u)$

   $\Gamma_f=\sigma(W_f[a^{<t-1>},x^{<t>}]+b_f)$

   $\Gamma_o=\sigma(W_o[a^{<t-1>},x^{<t>}]+b_o)$

   $c^{<t>}=\Gamma_u*\hat{c}^{<t>}+\Gamma_f*c^{<t-1>}$

   $a^{<t>}=\Gamma_o*c^{<t>}$

   ![C9ED60FC-BEA6-49F4-A735-90C7B76F782D](/Users/tessiehe/Documents/study_notes/吴恩达时间序列笔记/image/C9ED60FC-BEA6-49F4-A735-90C7B76F782D.png)