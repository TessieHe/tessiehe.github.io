<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"wangdongdong122.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="本篇为学习强化学习笔记，主要是学习李宏毅老师的课程的笔记。内容是强化学习的简单介绍，了解一下框架。">
<meta property="og:type" content="article">
<meta property="og:title" content="强化学习简介">
<meta property="og:url" content="http://wangdongdong122.github.io/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/index.html">
<meta property="og:site_name" content="凛冬将至">
<meta property="og:description" content="本篇为学习强化学习笔记，主要是学习李宏毅老师的课程的笔记。内容是强化学习的简单介绍，了解一下框架。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://wangdongdong122.github.io/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Scenario-of-Reinforcement-Learning.png">
<meta property="og:image" content="http://wangdongdong122.github.io/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Looking-for-a-Function.png">
<meta property="og:image" content="http://wangdongdong122.github.io/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Playing-Video-Game.png">
<meta property="og:image" content="http://wangdongdong122.github.io/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/RL-Outline.png">
<meta property="og:image" content="http://wangdongdong122.github.io/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/image-20210830234507908.png">
<meta property="og:image" content="http://wangdongdong122.github.io/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Neural-network-as-Actor.png">
<meta property="og:image" content="http://wangdongdong122.github.io/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Goodness-of-Actor.png">
<meta property="og:image" content="http://wangdongdong122.github.io/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Problem-statement.png">
<meta property="og:image" content="http://wangdongdong122.github.io/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/policy-gredient-define.png">
<meta property="og:image" content="http://wangdongdong122.github.io/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/gradient-of-log-p-1627997683506.png">
<meta property="og:image" content="http://wangdongdong122.github.io/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/gradient-of-R.png">
<meta property="og:image" content="http://wangdongdong122.github.io/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/classify-data.png">
<meta property="og:image" content="http://wangdongdong122.github.io/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/classify-task.png">
<meta property="og:image" content="http://wangdongdong122.github.io/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Add-a-Baseline.png">
<meta property="og:image" content="http://wangdongdong122.github.io/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Assign-Suitable-Credit1.png">
<meta property="og:image" content="http://wangdongdong122.github.io/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Assign-Suitable-Credit-r.png">
<meta property="og:image" content="http://wangdongdong122.github.io/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Assign-Suitable-Credit-A.png">
<meta property="og:image" content="http://wangdongdong122.github.io/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/PPO-prc.png">
<meta property="og:image" content="http://wangdongdong122.github.io/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/PPO-algorithm.png">
<meta property="og:image" content="http://wangdongdong122.github.io/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/PPO2-algorithm.png">
<meta property="og:image" content="http://wangdongdong122.github.io/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/sai2-1635782987756.png">
<meta property="og:image" content="http://wangdongdong122.github.io/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Monte-Carlo-based-approach.png">
<meta property="og:image" content="http://wangdongdong122.github.io/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Temporal-difference-approach.png">
<meta property="og:image" content="http://wangdongdong122.github.io/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/MC-vs-TD-1.png">
<meta property="og:image" content="http://wangdongdong122.github.io/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/MC-vs-TD-2.png">
<meta property="og:image" content="http://wangdongdong122.github.io/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/use-Q-learning-1632410014443.png">
<meta property="og:image" content="http://wangdongdong122.github.io/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Q-learning-Better-Actor.png">
<meta property="og:image" content="http://wangdongdong122.github.io/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Q-learning-Better-Actor-proof-1632410694932.png">
<meta property="og:image" content="http://wangdongdong122.github.io/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Target-Network.png">
<meta property="og:image" content="http://wangdongdong122.github.io/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Never-explore.png">
<meta property="og:image" content="http://wangdongdong122.github.io/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Exploration.png">
<meta property="og:image" content="http://wangdongdong122.github.io/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Replay-Buffer.png">
<meta property="og:image" content="http://wangdongdong122.github.io/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/over-estimated.png">
<meta property="og:image" content="http://wangdongdong122.github.io/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Q_prime.png">
<meta property="og:image" content="http://wangdongdong122.github.io/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/over-estimated-vs.png">
<meta property="og:image" content="http://wangdongdong122.github.io/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Dueling-DQN-1635808866948.png">
<meta property="og:image" content="http://wangdongdong122.github.io/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/dueling-table.png">
<meta property="og:image" content="http://wangdongdong122.github.io/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Prioritized-Reply.png">
<meta property="og:image" content="http://wangdongdong122.github.io/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Multi-step.png">
<meta property="og:image" content="http://wangdongdong122.github.io/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Noisy-Net.png">
<meta property="og:image" content="http://wangdongdong122.github.io/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Distributional-Q-function.png">
<meta property="og:image" content="http://wangdongdong122.github.io/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Rainbow-1636166579356.png">
<meta property="og:image" content="http://wangdongdong122.github.io/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/ContinuousActions.png">
<meta property="og:image" content="http://wangdongdong122.github.io/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/AdvantageActor-Critic.png">
<meta property="og:image" content="http://wangdongdong122.github.io/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/AAC-tips.png">
<meta property="og:image" content="http://wangdongdong122.github.io/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/A3C.png">
<meta property="og:image" content="http://wangdongdong122.github.io/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/PathwiseDerivativePolicyGradient.png">
<meta property="article:published_time" content="2021-07-29T13:56:44.000Z">
<meta property="article:modified_time" content="2021-11-18T14:05:28.923Z">
<meta property="article:author" content="Dongdong Wang">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="学习笔记">
<meta property="article:tag" content="强化学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://wangdongdong122.github.io/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Scenario-of-Reinforcement-Learning.png">

<link rel="canonical" href="http://wangdongdong122.github.io/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>强化学习简介 | 凛冬将至</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">凛冬将至</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">从简单的例子开始</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="th fa-fw"></i>分类</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://wangdongdong122.github.io/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Dongdong Wang">
      <meta itemprop="description" content="从爪印判断，这是头雄狮">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="凛冬将至">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          强化学习简介
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-07-29 21:56:44" itemprop="dateCreated datePublished" datetime="2021-07-29T21:56:44+08:00">2021-07-29</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-11-18 22:05:28" itemprop="dateModified" datetime="2021-11-18T22:05:28+08:00">2021-11-18</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%A8%A1%E5%9D%97%E7%AE%80%E4%BB%8B/" itemprop="url" rel="index"><span itemprop="name">模块简介</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>本篇为学习强化学习笔记，主要是学习李宏毅老师的<a target="_blank" rel="noopener" href="http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML20.html">课程</a>的笔记。内容是强化学习的简单介绍，了解一下框架。</p>
<span id="more"></span>
<h2 id="场景描述"><a href="#场景描述" class="headerlink" title="场景描述"></a>场景描述</h2><p>强化学习的基本场景是Agent和Environment之间交互，Environment给出一个state，Agent看到了这个state（等同于observation，更好理解）并根据这个state做出某个Action，这个Action会影响Environment，state会发生改变，同时Environment会反馈一个Reward。</p>
<img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Scenario-of-Reinforcement-Learning.png" class="" title="Scenario-of-Reinforcement-Learning">
<p>强化学习的目的，就是找到一个Actor或者叫Policy，让Reward最大，其输入使observation，输出是Action。</p>
<img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Looking-for-a-Function.png" class="" title="Looking-for-a-Function">
<h3 id="一些基本问题"><a href="#一些基本问题" class="headerlink" title="一些基本问题"></a>一些基本问题</h3><ol>
<li>为什么不直接学习人类的behavior，将问题转化为监督学习？<ul>
<li>机器一般没有办法完全模仿人类，此时机器很难知道哪些行为重要，哪些行为不重要。</li>
<li>即使机器可以完全拷贝人类，也没办法通过这种方式超过人类。</li>
</ul>
</li>
</ol>
<p>​    </p>
<h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><p>以玩游戏为例。</p>
<img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Playing-Video-Game.png" class="" title="Playing-Video-Game">
<p>有以下概念：</p>
<ul>
<li>state: observation</li>
<li>玩一局游戏称为一个episode</li>
</ul>
<p>​    </p>
<h2 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h2><h3 id="Model-based-amp-Model-free"><a href="#Model-based-amp-Model-free" class="headerlink" title="Model-based &amp; Model-free"></a>Model-based &amp; Model-free</h3><p>根据是否需要建模来模拟环境，可以分为Model-free Approach（不需要对环境建模）和Model-based Approach，前者不需要对环境建模，后者需要。</p>
<p>Model-free Approach中，可以直接学习什么Actor(policy)最好，即Policy-based。也可以学习一个评估函数，来判断不同决策的价值是多少，即可得到每一步最好的Action。如果将两者结合起来则可以综合两方面的优势。</p>
<img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/RL-Outline.png" class="" title="RL-Outline">
<p>​    </p>
<h3 id="On-Policy-amp-Off-Policy"><a href="#On-Policy-amp-Off-Policy" class="headerlink" title="On Policy &amp; Off Policy"></a>On Policy &amp; Off Policy</h3><ul>
<li><p>On-policy: 需要学习的Actor和与环境互动的Actor一样.</p>
</li>
<li><p>Off-policy: 需要学习的Actor和与环境互动的Actor不一样.</p>
</li>
</ul>
<img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/image-20210830234507908.png" class="" title="image-20210830234507908">
<p>off-policy的好处：可以通过一批数据学习多次Actor。Off-policy需要基于<a href="/2021/09/18/2021-09-18-%E9%87%8D%E8%A6%81%E9%87%87%E6%A0%B7/" title="重要采样">重要采样</a>。 </p>
<p>​    </p>
<h2 id="Policy-based-Approach"><a href="#Policy-based-Approach" class="headerlink" title="Policy-based Approach"></a>Policy-based Approach</h2><h3 id="问题介绍"><a href="#问题介绍" class="headerlink" title="问题介绍"></a>问题介绍</h3><p>前面介绍了Policy-based Approach 就是要学习一个Actor，它会根据看到的情况来判断下一步动作，即输入是state（等同于observation），输出是Action。将神经网络的模型作为一个actor。以上面的游戏为例，其输入就是当前游戏画面，输出是需要进行的游戏操作，用$\pi_{\theta}(s)$来表示。</p>
<img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Neural-network-as-Actor.png" class="" title="Neural-network-as-Actor">
<p>要训练这个神经网络模型，关键的问题是，它的训练目标是什么？强化学习的目标就是得到最多的奖励，所以应该用一个Actor所能得到的期望奖励作为评估其好坏的依据，可以通过蒙特卡洛法，多次实验用平均值作为期望值的矩估计。展开一个游戏过程来看，可以将游戏得分作为奖励，同一个Actor玩了很多局游戏，则用每局游戏奖励的平均值<strong>$\overline R_{\theta}$作为评估Actor $\pi_{\theta}(s)$好坏的依据</strong>。所以我们的只要将$\overline R_{\theta}$用$\theta$表示，最大化$\overline R_{\theta}$就可以。</p>
<img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Goodness-of-Actor.png" class="" title="Goodness-of-Actor">
<p>​    </p>
<h3 id="Policy-Gradient"><a href="#Policy-Gradient" class="headerlink" title="Policy Gradient"></a>Policy Gradient</h3><p>将上面的问题抽象一下，我们的目标就是找最优$\theta$，最大化$\overline R_{\theta}$。</p>
<img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Problem-statement.png" class="" title="Problem-statement">
<p><strong>梯度怎么求呢？</strong>这里$\overline R_{\theta}$是由episode的奖励$R(\tau)$和其概率$p(\tau|\theta)$决定，前者和Actor无关。这里对求导做了个变换，目的是为了把概率提出来，这样就可以表示成期望，然后利用蒙特卡洛来用均值代替。</p>
<img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/policy-gredient-define.png" class="" title="policy-gredient-define">
<p>注（<strong>重要采样</strong>）：这里通过乘$p(\tau|\theta)$后再除$p(\tau|\theta)$可以让该项可以表示成期望，但同时也将原来的$\nabla p(\tau|\theta)$变成了$\nabla log\ p(\tau|\theta)$，这会产生什么影响呢？$\nabla log\ p(\tau|\theta)$后面的求导中，还是会变成$1/p(\tau|\theta)$乘$\nabla p(\tau|\theta)$。原本更新$\theta$，按照$p(\tau|\theta)$的梯度反向传导，再用$R(\tau)$作为权重就可以了，前面这个$1/p(\tau|\theta)$是干啥的呢？是为了<strong>平衡样本偏差</strong>，本来应该完全按照$\tau$所带来的奖励决定其最终的概率，但现在样本是按照$p(\tau|\theta)$采样出来的，所以可能导致一个好的$\tau$在样本里极少出现，而未能对$\theta$的优化产生多大影响，所以$1/p(\tau|\theta)$就刚好是来解决这个问题。即：乘$p(\tau|\theta)$导致了偏差问题，再除$p(\tau|\theta)$有可以解决该问题。</p>
<p>$\nabla log\ p(\tau|\theta)$怎么求呢？将一局游戏(episode)拆开，可以表示为$\tau =\{s_1,a_1,r_1,s_2,a_2,r_2,···,s_T,a_T,r_T \}$，其似然为给定actor时产生该$\tau$的概率：$P(\tau|\theta)$。对其展开，整个游戏过程可以看做：给定了一个初始的状态$s_1$之后，Actor根据其观测到状态$s_t$和自身参数$\theta$产生一个动作$a_t$，环境根据状态$s_t$和Actore做出的动作$a_t$给出奖励$r_t$和下一各状态$s_{t+1}$。</p>
<p>$p(s_1)$表示初始状态是$s_1$的概率，由环境决定，不受Actor影响；$p(a_t|s_t,\theta)$表示其观测到状态$s_t$和自身参数$\theta$产生一个动作$a_t$的概率；$p(r_t,s_{t+1}|s_t,a_t)$表示Actor在$s_t$状态下做出动作$a_t$之后，得到的这局游戏中实际观测到的奖励$r_t$和下一个状态$s_{t+1}$的概率，这个概率也由环境决定，不受actor影响。所以<strong>似然中，只有$p(a_t|s_t,\theta)$与要优化的Actor相关</strong>。</p>

<p>将$p(\tau|\theta)$的计算公式代入$\nabla log\ p(\tau|\theta)$，其中只有$p(a_t|s_t,\theta)$与Actor相关。</p>
<img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/gradient-of-log-p-1627997683506.png" class="" title="gradient-of-log-p">
<p>代入$\nabla\overline R_{\theta}$中，得到更新公式</p>
<img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/gradient-of-R.png" class="" title="gradient-of-R">
<p>​    </p>
<h4 id="总结一下"><a href="#总结一下" class="headerlink" title="总结一下"></a>总结一下</h4><p>Actor的学习，就是要解决以下问题，我这里用期望代替均值，感觉表述更加准确。</p>
<script type="math/tex; mode=display">
\theta ^* = arg\ \underset{\theta}{max}\ E[R_{\theta}]</script><p>其中$E[R_{\theta}]$可以表示为</p>
<script type="math/tex; mode=display">
E[R_{\theta}] = \sum_{\tau} R(\tau) P(\tau|\theta)</script><p>其导数为</p>
<script type="math/tex; mode=display">
\begin{eqnarray*}

\nabla E[R_{\theta}] 
&=&  \nabla\sum_{\tau} R(\tau) P(\tau|\theta) 
\\
\\
&=& \sum_{\tau} R(\tau) \nabla P(\tau|\theta)
\tag{$R(\tau)$与$\theta$无关}
\\
\\
&=& \sum_{\tau} R(\tau)P(\tau|\theta) \frac{\nabla P(\tau|\theta)}{P(\tau|\theta)}
\tag{增加$P(\tau|\theta)$凑为期望}
\\
\\
&=& \sum_{\tau}P(\tau|\theta) R(\tau) \nabla log P(\tau|\theta)
\tag{换为log导数}
\\
\\
&\approx& \frac{1}{N}\sum^N_{n=1}R(\tau^n)\nabla log P(\tau^n|\theta)
\tag{期望用均值代替}
\end{eqnarray*}</script><p>其中$P(\tau|\theta)$可以展开其生成过程，并对其展开，并求解一下</p>
<script type="math/tex; mode=display">
\begin{eqnarray*}
&&P(\tau|\theta) = p(s_1)\prod ^T_{t=1}p(a_t|s_t,\theta)p(r_t,s_{t+1}|s_t,a_t)
\\
\\
&&log\ P(\tau|\theta) = log\ p(s_1) + \sum ^T_{t=1}\left [log\ p(a_t|s_t,\theta) + log \ p(r_t,s_{t+1}|s_t,a_t)\right ]
\\
\\
&&\nabla log P(\tau|\theta) = \sum ^T_{t=1}\nabla log p(a_t|s_t,\theta)
\end{eqnarray*}</script><p>因此</p>
<script type="math/tex; mode=display">
\begin{eqnarray*}
\color{red}
\nabla E[R_{\theta}] 


&\color{red}\approx& \frac{1}{N}\sum^N_{n=1}R(\tau^n)\nabla log P(\tau^n|\theta)
\\
\\

&=& \color{red}
\frac{1}{N}\sum^N_{n=1}\sum ^{T_n}_{t=1}R(\tau^n)\nabla log p(a^n_t|s^n_t,\theta)
\tag{仅$p(a_t|s_t,\theta)$与$\theta$相关}
\\
\\

\end{eqnarray*}</script><p>​    </p>
<h4 id="视作多分类"><a href="#视作多分类" class="headerlink" title="视作多分类"></a>视作多分类</h4><p>这其实和多分类问题相同。如果将学习Actor看做一个多分类问题，数据就是在每局游戏中Actor实际根据状态做出的动作。</p>
<img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/classify-data.png" class="" title="classify-data">
<p>其中每个$(s,a)$对中，$s$作为输入，$a$作为输出，对应episode的$R(\tau)$作为权重。</p>
<p>多分类交叉熵为</p>
<script type="math/tex; mode=display">
J_{CE}(\theta)  = -\frac1N \sum^{N}_{i=1}log(\hat y_{iy_i})</script><img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/classify-task.png" class="" title="classify-task">
<p>此处的$y_{i}$指的就是对应的$a$，$\hat y_{iy_i}$即$p(a^n_t|s^n_t,\theta)$，总样本数量为$N×T$个，权重为$R(\tau)$，则</p>
<script type="math/tex; mode=display">
J(\theta) =-\frac{1}{N×T}\sum^N_{n=1}\sum ^{T}_{t=1}R(\tau^n) log\ p(a^n_t|s^n_t,\theta)</script><p>这里全面的求平均是除$N×T$（实际上这样表示不准确，因为每个episode不一定都是T步），和上面Actor的公式有一点不一样，因为<strong>Actor是最大化</strong>的一次episode的<strong>联合概率</strong>，<strong>多分类是拆解成动作</strong>，最大化每个动作的概率。</p>
<p>​    </p>
<h4 id="Tips"><a href="#Tips" class="headerlink" title="Tips"></a>Tips</h4><h5 id="BaseLine"><a href="#BaseLine" class="headerlink" title="BaseLine"></a>BaseLine</h5><p>BaseLine就是指，将每个$(s,a)$对的权重由整个episode的奖励，改为这个<strong>奖励相对于平均奖励的差值（增益）</strong>，从而避免模型未能采样全带来的错误优化，一定程度上降低训练难度/对样本量要求：</p>
<script type="math/tex; mode=display">
\color{red}
\begin{eqnarray*}

\nabla E[R_{\theta}] 
&\approx&
\frac{1}{N}\sum^N_{n=1}\sum ^{T_n}_{t=1}
\left(
R(\tau^n) -b
\right)

\nabla log\ p(a^n_t|s^n_t,\theta)

\end{eqnarray*}</script><p>在上面的案例中，奖励$R(\tau)$永远是正的，在数据量非常大的时候这样也没什么问题，因为Actor一次做出各种动作的概率和为1，即使较差的动作对应的权重$R(\tau)$也是正的，但应该没有较好的动作权重高，因此最终差的动作概率还是会下降。但实际训练中，数据是采样得到的，有可能真正好的动作没有采样到（下图的a），这样就会导致优秀的动作a的概率下降，差一点的b,c概率上升。解决这个问题，可以给奖励，加一个baseline，低于base line奖励为负。</p>
<img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Add-a-Baseline.png" class="" title="Add-a-Baseline">
<p>实际操作时，可以不断将最新的$E[R(\tau)]$记录下来作为$b$。</p>
<p>​    </p>
<h5 id="Assign-Suitable-Credit"><a href="#Assign-Suitable-Credit" class="headerlink" title="Assign Suitable Credit"></a>Assign Suitable Credit</h5><p>Assign Suitable Credit就是将每个$(s,a)$对的权重由整个episode的奖励增益，改进为<strong>该action执行之后</strong>，<strong>时间衰减加权</strong>的累积奖励增益，从而使权重更加准确地表示action的影响：</p>
<script type="math/tex; mode=display">
\color{red}
\begin{eqnarray*}
\nabla E[R_{\theta}] 
&\approx&
\frac{1}{N}\sum^N_{n=1}\sum ^{T_n}_{t=1}
\left(
\sum^{T_n}_{t^{\prime}=t} \gamma^{t^{\prime}-t} r^n_{t^{\prime}} -b
\right)

\nabla log\ p(a^n_t|s^n_t,\theta)
\end{eqnarray*}</script><p>前面的方法，将episode整体的奖励$R(\tau)$，作为每个$(s,a)$对的权重，这点可以改进为<strong>$(s,a)$对执行之后得到的奖励作为其权重</strong>。比如下图中，$(s_b,a_2)$操作之后，可能只能带来减2分，但因为第1个动作的奖励比较高(5)，才导致整体的奖励是正向的。这个问题在数据量足够大时，也可以解决，模型总会看到第一个动作没那么高时，$(s_b,a_2)$操作带来的副作用（右图）。</p>
<img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Assign-Suitable-Credit1.png" class="" title="Assign-Suitable-Credit1">
<p>更进一步，$(s,a)$对的权重，由其之后的奖励<strong>加上时间衰减</strong>（$\gamma$）之后作为权重。</p>
<img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Assign-Suitable-Credit-r.png" class="" title="Assign-Suitable-Credit-r">
<p>从这个过程可以看到每个$(s,a)$对的权重其实大有文章，我们直接用$A^{\theta}(s_t,a_t)$来表示，计算的是在$s_t$采取$a_t$时，其权重有多大。其上标$\theta$表示用对应Actor和环境做互动后得到的轨迹，不同的Actor对于的平均水平不同，所以A必然和$\theta$相关。其含义为：在$s_t$采取$a_t$，<strong>相对于</strong>其他可能的action，有多好。通常$A^{\theta}(s_t,a_t)$可以直接由一个网络来学习。</p>
<img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Assign-Suitable-Credit-A.png" class="" title="Assign-Suitable-Credit-A">
<p>一个典型的A是$Q^{\pi}(s,a)-V^{\pi}(s)$，表示采取当前 $a$ 后的奖励比Actor自己操作的奖励之差。</p>
<p>​    </p>
<h3 id="Proximal-Policy-Optimization-PPO"><a href="#Proximal-Policy-Optimization-PPO" class="headerlink" title="Proximal Policy Optimization(PPO)"></a>Proximal Policy Optimization(PPO)</h3><p>PPO是OpenAI默认的强化学习算法，其地位可想而知。PPO是policy gradient的一个变形，所谓Proximal是近邻的意思，指off-policy中，和环境互动的policy要和更新的policy相近。</p>
<img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/PPO-prc.png" class="" title="PPO-prc">
<p>​    </p>
<h4 id="on-policy-→-off-policy"><a href="#on-policy-→-off-policy" class="headerlink" title="on-policy → off-policy"></a>on-policy → off-policy</h4><p>on-policy中，当我们更新一个Actor（$\pi_\theta$）时，会用这个Actor去和环境交互采样得到数据，利用下面的公式优化该Actor。Actor一旦更新，就需要用新的Actor再次采样数据，循环往复。</p>
<script type="math/tex; mode=display">
\begin{eqnarray*}

\nabla E[R_{\theta}] =E_{\tau \sim p_{\theta}(\tau)}

\left [
R(\tau)\ \nabla log\ p_\theta(\tau)

\right ]

\end{eqnarray*}</script><p>变成off-policy之后，我们就可以利用某个Actor（$\pi_{\theta^\prime }$）和环境交互的数据来优化当前的Actor（$\pi_\theta$），利用<a href="/2021/09/18/2021-09-18-%E9%87%8D%E8%A6%81%E9%87%87%E6%A0%B7/" title="重要采样">重要采样</a>：</p>
<script type="math/tex; mode=display">
\begin{eqnarray*}

\nabla E[R_{\theta}] =E_{\tau \sim p_{\theta^\prime}(\tau)}
\left [
\frac{p_\theta(\tau)}{p_{\theta^\prime}(\tau)}
R(\tau)\ \nabla log\ p_\theta(\tau)
\right ]

\end{eqnarray*}</script><p>使用Advantage Function：$A^{\theta}(s_t,a_t)$作为每步的权重时，将整个episode($\tau$)拆开为各个($s_t,a_t$)pair，则可表示为：</p>
<script type="math/tex; mode=display">
\begin{eqnarray*}
\nabla E[R_{\theta}]
&=&
E_{(s_t,a_t)\sim \pi_{\theta}}
\left [
A^{\theta}(s_t,a_t)
\nabla log\ p_{\theta}(a^n_t|s^n_t)
\right ]
\\
\\
&=&
E_{(s_t,a_t)\sim \pi_{\theta^\prime}}
\left [
\frac{P_{\theta}(s_t,a_t)}{P_{\theta^\prime}(s_t,a_t)}
A^{\theta^\prime}(s_t,a_t)
\nabla log\ p_{\theta}(a^n_t|s^n_t)
\right ]
\\
\\
&=&
E_{(s_t,a_t)\sim \pi_{\theta^\prime}}
\left [
\frac{P_{\theta}(a_t|s_t)}{P_{\theta^\prime}(a_t|s_t)}
\frac{P_{\theta}(s_t)}{P_{\theta^\prime}(s_t)}

A^{\theta^\prime}(s_t,a_t)
\nabla log\ p_{\theta}(a^n_t|s^n_t)
\right ]
\\
\\
&\approx& 
E_{(s_t,a_t)\sim \pi_{\theta^\prime}}
\left [
\frac{P_{\theta}(a_t|s_t)}{P_{\theta^\prime}(a_t|s_t)}
A^{\theta^\prime}(s_t,a_t)
\nabla log\ p_{\theta}(a^n_t|s^n_t)
\right ]


\end{eqnarray*}</script><p>这里第二步中，$A^{\theta}(s_t,a_t)$变成了$A^{\theta^\prime}(s_t,a_t)$，是因为这一项是从采样数据（由$\theta^\prime$决定）中算出来的，常用的做法是真实奖励与平均奖励的差，这个平均奖励是从采样数据中算出来的。</p>
<p>最后一步，约等于是因为将$\frac{P_{\theta}(s_t)}{P_{\theta^\prime}(s_t)}$假设为1了，即出现某个observation的概率和用哪个Actor无关，这样假设一方面是很多场景下确实成立，另一方面是由于observation出现的概率很难计算。上面这个式子是梯度计算公式，根据这个公式可以反推其对应的代价函数，根据公式：$\nabla f(x)=f(x)\nabla log\ f(x)$ 可知上式对应的<strong>代价函数</strong>为：</p>
<script type="math/tex; mode=display">
J^{\theta^\prime}(\theta) =E_{(s_t,a_t)\sim \pi_{\theta^\prime}}
\left [
\frac{P_{\theta}(a_t|s_t)}{P_{\theta^\prime}(a_t|s_t)}
A^{\theta^\prime}(s_t,a_t)
\right ]</script><p>​    </p>
<h4 id="Add-Constraint"><a href="#Add-Constraint" class="headerlink" title="Add Constraint"></a>Add Constraint</h4><p>在重要采样中提到过，当两个分布差异过大时，由重要采样得出的结果方差会很大。在off-policy中，和环境互动的Actor不能和我们要学习的Actor差异太多，<strong>即$\pi_{\theta^\prime}$和$\pi_{\theta}$不能差异太大</strong>。</p>
<p>PPO(Proximal Policy Optimization)将$\pi_{\theta}$和$\pi_{\theta^\prime}$的差异加到代价函数中，来保证$\pi_{\theta}$和$\pi_{\theta^\prime}$不会相差太大：</p>
<script type="math/tex; mode=display">
\begin{eqnarray*}

& J^{\theta^\prime}_{PPO}(\theta) = J^{\theta^\prime}(\theta)-\beta KL(\theta,\theta^\prime)
\\
\\
& J^{\theta^\prime}(\theta) = E_{(s_t,a_t)\sim \pi_{\theta^\prime}}
\left [
\frac{P_{\theta}(a_t|s_t)}{P_{\theta^\prime}(a_t|s_t)}
A^{\theta^\prime}(s_t,a_t)
\right ]

\end{eqnarray*}</script><p>TRPO (Trust Region Policy Optimization)则直接将$\pi_{\theta}$和$\pi_{\theta^\prime}$的差异小于设定阈值作为代价函数的不等式约束：</p>
<script type="math/tex; mode=display">
\begin{eqnarray*}

& J^{\theta^\prime}_{TRPO}(\theta) = E_{(s_t,a_t)\sim \pi_{\theta^\prime}}
\left [
\frac{P_{\theta}(a_t|s_t)}{P_{\theta^\prime}(a_t|s_t)}
A^{\theta^\prime}(s_t,a_t)
\right ]
\\
\\
& KL(\theta,\theta^\prime) < \delta

\end{eqnarray*}</script><p>​    </p>
<h4 id="PPO-algorithm"><a href="#PPO-algorithm" class="headerlink" title="PPO algorithm"></a>PPO algorithm</h4><p>按照刚才的思路，可以给出PPO算法的步骤。值得注意的是，① 每次iteration，$\theta$会在近邻约束下收敛，随后使用新的$\theta$重新采样得到新的数据用于训练；② 此处的$KL(\theta,\theta^k)$，指的是其对应行为之间的KL散度，并非真的是参数之间的KL散度，实际计算时，是用采样数据s输入两个Actor，得到对应action的分布，从而计算KL散度；③ 算法中加入了自适应的权重，适用过程很直观，KL项大于预设值就增大其权重，反之则减小。</p>
<img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/PPO-algorithm.png" class="" title="PPO-algorithm">
<p>​    </p>
<h4 id="PPO2-algorithm"><a href="#PPO2-algorithm" class="headerlink" title="PPO2 algorithm"></a>PPO2 algorithm</h4><p>PP2算法则直接将过大或者过小的权重截断，来限制重要采样结果的方差：</p>
<script type="math/tex; mode=display">
\begin{eqnarray*}

J^{\theta^\prime}_{PPO2}(\theta) &\approx& 

min \left (
\frac{P_{\theta}(a_t|s_t)}{P_{\theta^k}(a_t|s_t)}
A^{\theta^k}(s_t,a_t),
clip \left(
\frac{P_{\theta}(a_t|s_t)}{P_{\theta^k}(a_t|s_t)}
,
1-\epsilon
,1+\epsilon

\right )
A^{\theta^k}(s_t,a_t)

\right )


\end{eqnarray*}</script><p>截断的方式如下图所示</p>
<img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/PPO2-algorithm.png" class="" title="PPO2-algorithm">
<p>这一点其实很像倾向性得分中，为了减小方差，而采用的各种重叠区域的选取方法。2去</p>
<p>​    </p>
<h2 id="Value-based-Approach"><a href="#Value-based-Approach" class="headerlink" title="Value-based Approach"></a>Value-based Approach</h2><p>Value-based方法指学习一个Critic来判断Actor的好坏。Critic不决定Action，但可以从Critic得到一个最优的Actor。Critic分为两种：</p>
<ol>
<li>$V^{\pi}(s)$ ：在看见状态$s$后，使用Actor $\pi$产生的累积奖励的期望值</li>
<li>$Q^{\pi}(s,a)$：在看见状态$s$后，先采取Action $a$，然后使用Actor $\pi$产生的累积奖励的期望值</li>
</ol>
<blockquote>
<p>此处的$Q$指强制采取Action $a$，和因果推断中的干预是一个意思。</p>
</blockquote>
<p>比如说阿光在下棋的时候，旁边佐为会告诉他，现在的他能驾驭大马步飞了，下大马步飞是好棋，以前太弱不能这么下，即相同的State的情况下，不同的Actor的V是不一样的。</p>
<img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/sai2-1635782987756.png" class="" title="sai2">
<p>​    </p>
<h3 id="estimate-V-pi-s"><a href="#estimate-V-pi-s" class="headerlink" title="estimate $V^{\pi}(s)$"></a>estimate $V^{\pi}(s)$</h3><p>估计$V^{\pi}(s)$ 有两种方法：<strong>蒙特卡洛法</strong>和<strong>差分法</strong></p>
<p>​    </p>
<h4 id="Monte-Carlo-based-approach"><a href="#Monte-Carlo-based-approach" class="headerlink" title="Monte-Carlo based approach"></a>Monte-Carlo based approach</h4><p>蒙特卡洛法就是让Actor $\pi$玩游戏，产生episode，来训练critic。在看见状态$s_a$之后，直到游戏结束产生的累积奖励为$G_a$。这里的$G$和之前每个episode的总奖赏$R$不同，$G$表示转态$s_a$之后的累计奖励。</p>
<img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Monte-Carlo-based-approach.png" class="" title="Monte-Carlo-based-approach">
<p>​    </p>
<h4 id="Temporal-difference-approach"><a href="#Temporal-difference-approach" class="headerlink" title="Temporal-difference approach"></a>Temporal-difference approach</h4><p>差分法，策略$\pi$下产生的episode，相邻状态$s_t$和$s_{t+1}$下的$V$值相差$r_t$，可以以此为目标训练$V^{\pi}(s)$ 。</p>
<img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Temporal-difference-approach.png" class="" title="Temporal-difference-approach">
<p>​    </p>
<h4 id="MC-vs-TD"><a href="#MC-vs-TD" class="headerlink" title="MC vs TD"></a>MC vs TD</h4><p>第一点是目标值的方差和偏差的不同。MC法直接预测的是累积奖励$G_a$，方差会更大。而TD法，标签值是单步的$r$，方差相对小，但其用到了$V^{\pi}(s_{t+1})$，这个值一定准。</p>
<img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/MC-vs-TD-1.png" class="" title="MC-vs-TD-1">
<p>第二点是基于的假设不同。在下面的数据中，一共有8个episode，不管我们用哪个方法，很明显都能得到$V^{\pi}(s_{b})=3/4$，那么$V^{\pi}(s_{a})$应该等于多少呢。按照MC方法，$V^{\pi}(s_{a})=0$（只有一条数据）；按照TD方法，$V^{\pi}(s_{a})=V^{\pi}(s_{b})+0=3/4$。MC的假设下状态前后之间可能有相互影响。</p>
<img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/MC-vs-TD-2.png" class="" title="MC-vs-TD-2">
<p>第三点是TD方法不需要一整个episode结束（游戏结束）才能开始训练，只要知道其中一段就可以开始训练。</p>
<p>​    </p>
<p><strong>问题</strong>：</p>
<ol>
<li>看起来针对不同的Actor $\pi$有一个模型$V^{\pi}(s)$， Actor如果更新了，V要重新训练吗？</li>
<li>得到这个V之后<strong>如何使用</strong>？<ol>
<li>可以用于AC算法</li>
</ol>
</li>
<li>MC和TD之间的假设差异如何理解？</li>
</ol>
<p>​    </p>
<h3 id="Q-Learning"><a href="#Q-Learning" class="headerlink" title="Q-Learning"></a>Q-Learning</h3><p>Q-Learning学习的目标是$Q^{\pi}(s,a)$，一样可以用MC和TD方法。我们可以通过Q-Learning找到一个好的Actor。</p>
<img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/use-Q-learning-1632410014443.png" class="" title="use-Q-learning">
<p>当学习到某个Actor $\pi$ 对应的$Q^{\pi}(s,a)$之后，则根据这个$Q^{\pi}(s,a)$来决策作为新的Actor $\pi^\prime$ 会比$\pi$ 更好</p>
<img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Q-learning-Better-Actor.png" class="" title="Q-learning-Better-Actor">
<p>证明：先证$V^{\pi}(s) \leq Q^{\pi}(s,\pi ^\prime (s))$，然后逐项展开，递推得到$V^{\pi}(s) \leq V^{\pi^ \prime}(s)$的结论。</p>
<img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Q-learning-Better-Actor-proof-1632410694932.png" class="" title="Q-learning-Better-Actor-proof">
<p>​    </p>
<h4 id="TD-approach"><a href="#TD-approach" class="headerlink" title="TD approach"></a>TD approach</h4><p>通过TD的方式训练，此时会有一个问题，目标值：$r_t+Q^{\pi}(s_{t+1},\pi(s_{t+1}))$也会变化，会导致很难训练，因此可以将右边的网络（Target Network）<strong>固定</strong>，在左边网络训练多轮之后，更新一次右边的Target Network。</p>
<img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Target-Network.png" class="" title="Target-Network">
<p>基于已有的Q函数可以产出action，即$a= arg\ \underset{a}{max}\ Q(s,a)$，这本身就是一个policy。但这样不利于收集数据，给定s后，action是确定性的，不会<strong>探索</strong>其他的Action。</p>
<img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Never-explore.png" class="" title="Never-explore">
<p>有几种方法解决这个问题，第一种<strong>Epsilon Greedy</strong>：就是有$\varepsilon$的概率随机给出一个Action；第二种<strong>Boltzmann Exploration</strong>：就是根据每个动作的Q值，利用softmax给出每个动作出现的概率。</p>
<img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Exploration.png" class="" title="Exploration">
<p>除此之外还可以通过<strong>Replay Buffer</strong>，用off-policy的方式更新Q函数。让Buffer中存入一定熟练的交互数据，当buffer中的数据满了，去掉最老的数据。当学习Q函数时，从buffer中采样数据。</p>
<img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Replay-Buffer.png" class="" title="Replay-Buffer">
<p>Replay Buffer的方法是异策略，但此时产生数据的actor和学习的Q对应的$\pi$不同并不会导致问题。原因应该是此处是差分方法，且Q函数中的a本身就是强制设定的，和$\pi$无关。</p>
<p>疑问：</p>
<ol>
<li>TD方法，特别是Replay Buffer方法学习出来的Q对应的$\pi$到底是啥呢？</li>
</ol>
<p>​    </p>
<h4 id="Typical-Q-Learning-Algorithm"><a href="#Typical-Q-Learning-Algorithm" class="headerlink" title="Typical Q-Learning Algorithm"></a>Typical Q-Learning Algorithm</h4><ul>
<li>初始化Q-function，target Q-function（产生训练目标的Q） $\hat Q=Q$</li>
<li>每个episode<ul>
<li>每个时间步骤 $t$：<ul>
<li>给定状态$s_t$，根据$Q$（epsilon greedy），采取动作$a$</li>
<li>获得奖励$r_t$，和新的状态$s_{t+1}$</li>
<li>将$(s_t,a_t,r_t,s_{t+1})$存入buffer</li>
<li>从buffer中采样出样本$(s_i,a_i,r_i,s_{i+1})$（通常是采一个batch）</li>
<li>设定目标$y=r_i+ \underset{a}{max}\ \hat Q(s_{i+1},a)$</li>
<li>更新Q的参数，使$Q(s_i,a_i)$接近$y$</li>
<li>每隔C步，设置 $\hat Q=Q$</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>​    </p>
<h4 id="Tips-1"><a href="#Tips-1" class="headerlink" title="Tips"></a>Tips</h4><h5 id="Double-DQN"><a href="#Double-DQN" class="headerlink" title="Double DQN"></a>Double DQN</h5><p>Double DQN通过增加一个Q函数作为target Q-function，来解决DQN对Q值高估的问题。</p>
<p>基础的Q-Learning算法一般都会对Q值高估，这是因为Q-Learning中的Q对应的policy就是按Q-function（target function）选取最大Q值对应的a产生的。因此在每次学习中，会选取最高估的$Q(s_{t+1},a)$作为回归的目标，这就会导致Q值不断被高估。</p>
<img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/over-estimated.png" class="" title="over-estimated">
<p>解决这个问题的办法，可以用$Q$来决定动作a，但用另外一个$Q^{\prime}$作为target Q-function，此时$Q^{\prime}$是否高估与$Q$无关。</p>
<img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Q_prime.png" class="" title="Q_prime">
<p>效果如下：</p>
<img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/over-estimated-vs.png" class="" title="over-estimated-vs">
<p><a target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper/2010/file/091d584fced301b442654dd8c23b3fc9-Paper.pdf">Hado V. Hasselt, “Double Q-learning”, NIPS 2010</a></p>
<p>​    </p>
<h5 id="Dueling-DQN"><a href="#Dueling-DQN" class="headerlink" title="Dueling DQN"></a>Dueling DQN</h5><p>Dueling DQN通过将Q值拆为V值和$A(s,a)$（一般A会加和为1的正则化），使Q-function更具推理能力，提高学习效率。</p>
<img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Dueling-DQN-1635808866948.png" class="" title="Dueling-DQN">
<p>为什么这样做可以提高学习效率？当某个state下，值采样到一部分action下的结果时，如下图的4和0，由于A被加了正则化，此时模型更倾向于更新V来拟合4和0，这样一来，即使更新过程中没有采样到最后一个action的数据也能对其Q值有效更新。</p>
<img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/dueling-table.png" class="" title="dueling-table">
<p>要想达到刚才所讲的效果，必不可少的是对A做限制，比如说使其每列为0均值，这有点像layer normalization。</p>
<p><a target="_blank" rel="noopener" href="http://proceedings.mlr.press/v48/wangf16.pdf">Ziyu Wang, Tom Schaul, Matteo Hessel, Hado van Hasselt, Marc Lanctot, Nando de Freitas, “Dueling Network Architectures for Deep Reinforcement Learning”, arXiv preprint, 2015</a></p>
<p>​    </p>
<h5 id="Prioritized-Reply"><a href="#Prioritized-Reply" class="headerlink" title="Prioritized Reply"></a>Prioritized Reply</h5><p>Prioritized Reply通过将基础Q-Learning中在Experience Buffer中均匀采样改为对预测效果差的数据优先采样，来提升效果。此时参数的更新过程也会有所调整，不做展开。</p>
<img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Prioritized-Reply.png" class="" title="Prioritized-Reply">
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1511.05952.pdf">Schaul T, Quan J, Antonoglou I, et al. Prioritized experience replay[J]. arXiv preprint arXiv:1511.05952, 2015.</a></p>
<p>​    </p>
<h5 id="Multi-step"><a href="#Multi-step" class="headerlink" title="Multi-step"></a>Multi-step</h5><p>Multi-step通过将TD的拟合一步的$r_t$改进为拟合多步的$r_t,…,r_{t+N}$，使模型在TD和MC之间平衡，兼具两者的优点（和缺点），既减小了目标由训练模型定义导致的偏差，又使回归值$\sum r_{t^\prime}$的方差不至于太大。</p>
<img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Multi-step.png" class="" title="Multi-step">
<p>​    </p>
<h5 id="Noisy-Net"><a href="#Noisy-Net" class="headerlink" title="Noisy Net"></a>Noisy Net</h5><p>Noisy Net将Epsilon Greedy的有一定概率随机探索改进为对Q-function的参数加上噪音，使探索更有体系，试验中发现能够学习出更优的Actor。</p>
<img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Noisy-Net.png" class="" title="Noisy-Net">
<p>这种方式Q作为Actor，在给定一个state时，会采取相同的Action（称为State-dependent Exploration），比Epsilon Greedy更像一个真实的policy。</p>
<p>​    </p>
<h5 id="Distributional-Q-function"><a href="#Distributional-Q-function" class="headerlink" title="Distributional Q-function"></a>Distributional Q-function</h5><p>Distributional Q-function将基础的Q-function中求Reward的期望值改进预测Reward的分布，从而可以根据我们其他目标选择其他的Action，比如想要比较稳定的较好结果，就可以倾向于选Reward分布稳定一点Action。</p>
<img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Distributional-Q-function.png" class="" title="Distributional-Q-function">
<h5 id="Rainbow"><a href="#Rainbow" class="headerlink" title="Rainbow"></a>Rainbow</h5><p>将上述的六招都用上，每种方法用一种颜色表示，就是Rainbow。左图是每种方法各自的效果，右图是去掉其中一种方法之后的效果。</p>
<img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Rainbow-1636166579356.png" class="" title="Rainbow">
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1710.02298.pdf">Rainbow</a></p>
<h4 id="Continuous-Actions"><a href="#Continuous-Actions" class="headerlink" title="Continuous Actions"></a>Continuous Actions</h4><p>当Action是连续值，则根据已有的Q函数得到Action:$a= arg\ \underset{a}{max}\ Q(s,a)$不再能枚举所有action求解。常用的做法有（1）采用很多个Action，计算其对应的Q值，最大的作为结果；（2）使用梯度下降求解上面的优化问题；（3）通过网络的设计，让这个优化问题可以简单地求解。</p>
<p>第三种方法可以通过将Q值设定为 $a$ 的二次函数，比如：</p>
<img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/ContinuousActions.png" class="" title="ContinuousActions">
<h2 id="Actor-Critic"><a href="#Actor-Critic" class="headerlink" title="Actor-Critic"></a>Actor-Critic</h2><h3 id="Actor-Critic-1"><a href="#Actor-Critic-1" class="headerlink" title="Actor-Critic"></a>Actor-Critic</h3><p>在Policy Gradient中说过，增加了baseline和Assign Suitable Credit这两个Tips之后，Actor的更新公式变成：</p>
<script type="math/tex; mode=display">
\begin{eqnarray*}
\nabla E[R_{\theta}] 
&\approx&
\frac{1}{N}\sum^N_{n=1}\sum ^{T_n}_{t=1}
\left(
\color{red} \sum^{T_n}_{t^{\prime}=t} \gamma^{t^{\prime}-t} r^n_{t^{\prime}}
-b
\right)

\nabla log\ p(a^n_t|s^n_t,\theta)

\end{eqnarray*}</script><p>其中令 $G^n_t=\sum^{T_n}_{t^{\prime}=t} \gamma^{t^{\prime}-t} r^n_{t^{\prime}}$，是$a_t$之后时间衰减的累积奖励，这个值在实际与环境互动中得到的，很不稳定，如果G的方差比较大，尤其严重。因此我们使用G的期望值更为合理。</p>
<p>在Q-Learning中，我们所学习的Q-function就是用来估计Q值，也就是执行$a_t$之后的累积奖励，在很多Q-Learning中也同样会考虑时间衰减orT步之内的累积奖励，即：$\color{red}E [G^n_t]=Q^ {\pi_{\theta}}(s^n_t,a^n_t)$。而baseline：$b$想要表示的是平均水平的奖励，和V值的含义相同，因此令$\color{red}b=V ^{\pi_{\theta}}(s^n_t)$。因此我们可以借鉴Value-base的方法，用$\color{red}Q^ {\pi_{\theta}}(s^n_t,a^n_t) - V^{\pi_{\theta}}(s^n_t)$代替$\left( \sum^{T_n}_{t^{\prime}=t} \gamma^{t^{\prime}-t} r^n_{t^{\prime}}-b\right)$，这就是<strong>Actor-Critic</strong>，即：</p>
<script type="math/tex; mode=display">
\begin{eqnarray*}
\nabla E[R_{\theta}] 
&\approx&
\frac{1}{N}\sum^N_{n=1}\sum ^{T_n}_{t=1}
\left(
\color{red}Q^ {\pi_{\theta}}(s^n_t,a^n_t) - V^{\pi_{\theta}}(s^n_t)
\right)

\nabla log\ p(a^n_t|s^n_t,\theta)

\end{eqnarray*}</script><h3 id="Advantage-Actor-Critic"><a href="#Advantage-Actor-Critic" class="headerlink" title="Advantage Actor-Critic"></a>Advantage Actor-Critic</h3><p>上述的AC算法需要估计Q和V两个模型，通过<strong>将Q改为用V代替</strong>，可以转换为只需要V-function。Q可以转化为下一步的奖励和V值相加：</p>
<script type="math/tex; mode=display">
Q^ {\pi}(s^n_t,a^n_t) = E[r^n_t + V^{\pi}(s^n_{t+1})]</script><p>其中V用模型代替时就是期望值，再忽略$r^n_t$的期望运算，用真实值代替期望，则可以表示为：$Q^ {\pi}(s^n_t,a^n_t) = r^n_t + V^{\pi}(s^n_{t+1})$。此时：</p>
<script type="math/tex; mode=display">
\begin{eqnarray*}
\nabla E[R_{\theta}] 
&\approx&
\frac{1}{N}\sum^N_{n=1}\sum ^{T_n}_{t=1}
\left(
\color{red}r^n_t + V^{\pi}(s^n_{t+1}) - V^{\pi_{\theta}}(s^n_t)
\right)

\nabla log\ p(a^n_t|s^n_t,\theta)

\end{eqnarray*}</script><p>实际的训练流程就是：$\pi$和环境互动；然后用TD or MC方法学习$V^{\pi}(s)$；然后根据互动的数据和$V^{\pi}(s)$使用policy gradient（上面的公式）更新$\pi$。</p>
<img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/AdvantageActor-Critic.png" class="" title="AdvantageActor-Critic">
<p>​    </p>
<p>有两个Tip，第一个是$\pi(s)$和$V(s)$的输入都输入相同，他们的前几层<strong>参数是可以共享</strong>的；第二个是$\pi(s)$的输出可以加<strong>entropy不能太小的正则</strong>，以起到探索的作用。</p>
<img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/AAC-tips.png" class="" title="AAC-tips">
<p>​    </p>
<h3 id="Asynchronous-Advantage-Actor-Critic"><a href="#Asynchronous-Advantage-Actor-Critic" class="headerlink" title="Asynchronous Advantage Actor-Critic"></a>Asynchronous Advantage Actor-Critic</h3><p>增加一个分布式异步训练可以提高模型学习效率。分为4步：① 拷贝全局参数；② 和环境互动采样数据；③ 计算梯度；④ 更新全局参数。 </p>
<img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/A3C.png" class="" title="A3C">
<p><a target="_blank" rel="noopener" href="https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2#.68x6na7o9">Simple Reinforcement Learning with Tensorflow Part 8: Asynchronous Actor-Critic Agents (A3C)</a></p>
<p><a target="_blank" rel="noopener" href="http://proceedings.mlr.press/v48/mniha16.pdf">Mnih V, Badia A P, Mirza M, et al. Asynchronous methods for deep reinforcement learning[C]//International conference on machine learning. PMLR, 2016: 1928-1937.</a></p>
<p>​    </p>
<h3 id="Pathwise-Derivative-Policy-Gradient"><a href="#Pathwise-Derivative-Policy-Gradient" class="headerlink" title="Pathwise Derivative Policy Gradient"></a>Pathwise Derivative Policy Gradient</h3><p>这个方法在<strong>Actor后面接一个Q-Learning</strong>，让Actor学出Q看来最好的Action。</p>
<img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/PathwiseDerivativePolicyGradient.png" class="" title="PathwiseDerivativePolicyGradient">
<p>可以将其<strong>看做GAN</strong>，Actor产生action，Q评判这个Action是否好。也可以看做是Actor-Critic的加强，这里的Critic不仅可以评判Action是否好，还能告诉Actor什么样的Action是最好的。还可以看做Q-Learning的加强，此时产生Action不再是由Q直接产生，而是由Actor产生，从而可以产出连续值的Action。</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"># 机器学习</a>
              <a href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" rel="tag"># 学习笔记</a>
              <a href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" rel="tag"># 强化学习</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/07/29/2021-07-29-win10%E9%85%8D%E7%BD%AETensorFlow/" rel="prev" title="win10配置TensorFlow.md">
      <i class="fa fa-chevron-left"></i> win10配置TensorFlow.md
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/07/30/2021-07-30-win10%E9%85%8D%E7%BD%AEGPU%E7%89%88TensorFlow/" rel="next" title="win10配置GPU版TensorFlow.md">
      win10配置GPU版TensorFlow.md <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9C%BA%E6%99%AF%E6%8F%8F%E8%BF%B0"><span class="nav-number">1.</span> <span class="nav-text">场景描述</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%80%E4%BA%9B%E5%9F%BA%E6%9C%AC%E9%97%AE%E9%A2%98"><span class="nav-number">1.1.</span> <span class="nav-text">一些基本问题</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="nav-number">2.</span> <span class="nav-text">基本概念</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%86%E7%B1%BB"><span class="nav-number">3.</span> <span class="nav-text">分类</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Model-based-amp-Model-free"><span class="nav-number">3.1.</span> <span class="nav-text">Model-based &amp; Model-free</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#On-Policy-amp-Off-Policy"><span class="nav-number">3.2.</span> <span class="nav-text">On Policy &amp; Off Policy</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Policy-based-Approach"><span class="nav-number">4.</span> <span class="nav-text">Policy-based Approach</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%E4%BB%8B%E7%BB%8D"><span class="nav-number">4.1.</span> <span class="nav-text">问题介绍</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Policy-Gradient"><span class="nav-number">4.2.</span> <span class="nav-text">Policy Gradient</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%80%BB%E7%BB%93%E4%B8%80%E4%B8%8B"><span class="nav-number">4.2.1.</span> <span class="nav-text">总结一下</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%A7%86%E4%BD%9C%E5%A4%9A%E5%88%86%E7%B1%BB"><span class="nav-number">4.2.2.</span> <span class="nav-text">视作多分类</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Tips"><span class="nav-number">4.2.3.</span> <span class="nav-text">Tips</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#BaseLine"><span class="nav-number">4.2.3.1.</span> <span class="nav-text">BaseLine</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Assign-Suitable-Credit"><span class="nav-number">4.2.3.2.</span> <span class="nav-text">Assign Suitable Credit</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Proximal-Policy-Optimization-PPO"><span class="nav-number">4.3.</span> <span class="nav-text">Proximal Policy Optimization(PPO)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#on-policy-%E2%86%92-off-policy"><span class="nav-number">4.3.1.</span> <span class="nav-text">on-policy → off-policy</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Add-Constraint"><span class="nav-number">4.3.2.</span> <span class="nav-text">Add Constraint</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#PPO-algorithm"><span class="nav-number">4.3.3.</span> <span class="nav-text">PPO algorithm</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#PPO2-algorithm"><span class="nav-number">4.3.4.</span> <span class="nav-text">PPO2 algorithm</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Value-based-Approach"><span class="nav-number">5.</span> <span class="nav-text">Value-based Approach</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#estimate-V-pi-s"><span class="nav-number">5.1.</span> <span class="nav-text">estimate $V^{\pi}(s)$</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Monte-Carlo-based-approach"><span class="nav-number">5.1.1.</span> <span class="nav-text">Monte-Carlo based approach</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Temporal-difference-approach"><span class="nav-number">5.1.2.</span> <span class="nav-text">Temporal-difference approach</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#MC-vs-TD"><span class="nav-number">5.1.3.</span> <span class="nav-text">MC vs TD</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Q-Learning"><span class="nav-number">5.2.</span> <span class="nav-text">Q-Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#TD-approach"><span class="nav-number">5.2.1.</span> <span class="nav-text">TD approach</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Typical-Q-Learning-Algorithm"><span class="nav-number">5.2.2.</span> <span class="nav-text">Typical Q-Learning Algorithm</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Tips-1"><span class="nav-number">5.2.3.</span> <span class="nav-text">Tips</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Double-DQN"><span class="nav-number">5.2.3.1.</span> <span class="nav-text">Double DQN</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Dueling-DQN"><span class="nav-number">5.2.3.2.</span> <span class="nav-text">Dueling DQN</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Prioritized-Reply"><span class="nav-number">5.2.3.3.</span> <span class="nav-text">Prioritized Reply</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Multi-step"><span class="nav-number">5.2.3.4.</span> <span class="nav-text">Multi-step</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Noisy-Net"><span class="nav-number">5.2.3.5.</span> <span class="nav-text">Noisy Net</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Distributional-Q-function"><span class="nav-number">5.2.3.6.</span> <span class="nav-text">Distributional Q-function</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Rainbow"><span class="nav-number">5.2.3.7.</span> <span class="nav-text">Rainbow</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Continuous-Actions"><span class="nav-number">5.2.4.</span> <span class="nav-text">Continuous Actions</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Actor-Critic"><span class="nav-number">6.</span> <span class="nav-text">Actor-Critic</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Actor-Critic-1"><span class="nav-number">6.1.</span> <span class="nav-text">Actor-Critic</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Advantage-Actor-Critic"><span class="nav-number">6.2.</span> <span class="nav-text">Advantage Actor-Critic</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Asynchronous-Advantage-Actor-Critic"><span class="nav-number">6.3.</span> <span class="nav-text">Asynchronous Advantage Actor-Critic</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Pathwise-Derivative-Policy-Gradient"><span class="nav-number">6.4.</span> <span class="nav-text">Pathwise Derivative Policy Gradient</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Dongdong Wang</p>
  <div class="site-description" itemprop="description">从爪印判断，这是头雄狮</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">40</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">63</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="mailto:wangdongdong122@163.com" title="E-Mail → mailto:wangdongdong122@163.com" rel="noopener" target="_blank"><i class="envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



        <div class="wechat_OA">
            <span>欢迎加微信讨论</span>
            <br>
            <!-- 这里添加你的二维码图片 -->
            <img src ="/images/wechat.png" style="zoom:40%;" />
        </div>
      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Dongdong Wang</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>


        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  













<script>
if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js', () => {
    mermaid.initialize({
      theme    : 'forest',
      logLevel : 3,
      flowchart: { curve     : 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 }
    });
  }, window.mermaid);
}
</script>


  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
