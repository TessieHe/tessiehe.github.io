<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"wangdongdong122.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="本文介绍从attention到self-attention，再到transformer的原理。">
<meta property="og:type" content="article">
<meta property="og:title" content="Self-Attention">
<meta property="og:url" content="http://wangdongdong122.github.io/2021/06/24/2021-06-24-Self-Attention/index.html">
<meta property="og:site_name" content="凛冬将至">
<meta property="og:description" content="本文介绍从attention到self-attention，再到transformer的原理。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://wangdongdong122.github.io/2021/06/24/2021-06-24-Self-Attention/image-20210625130151133.png">
<meta property="og:image" content="http://wangdongdong122.github.io/2021/06/24/2021-06-24-Self-Attention/1624597133180_src">
<meta property="og:image" content="http://wangdongdong122.github.io/2021/06/24/2021-06-24-Self-Attention/image-20210708161738095.png">
<meta property="og:image" content="http://wangdongdong122.github.io/2021/06/24/2021-06-24-Self-Attention/image-20210708163214552.png">
<meta property="og:image" content="http://wangdongdong122.github.io/2021/06/24/2021-06-24-Self-Attention/image-20210708165933828.png">
<meta property="article:published_time" content="2021-06-24T05:02:38.000Z">
<meta property="article:modified_time" content="2021-08-29T06:28:16.701Z">
<meta property="article:author" content="Dongdong Wang">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="Attention">
<meta property="article:tag" content="transformer">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://wangdongdong122.github.io/2021/06/24/2021-06-24-Self-Attention/image-20210625130151133.png">

<link rel="canonical" href="http://wangdongdong122.github.io/2021/06/24/2021-06-24-Self-Attention/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Self-Attention | 凛冬将至</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">凛冬将至</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">从简单的例子开始</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="th fa-fw"></i>分类</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://wangdongdong122.github.io/2021/06/24/2021-06-24-Self-Attention/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Dongdong Wang">
      <meta itemprop="description" content="从爪印判断，这是头雄狮">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="凛冬将至">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Self-Attention
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-06-24 13:02:38" itemprop="dateCreated datePublished" datetime="2021-06-24T13:02:38+08:00">2021-06-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-08-29 14:28:16" itemprop="dateModified" datetime="2021-08-29T14:28:16+08:00">2021-08-29</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>本文介绍从attention到self-attention，再到transformer的原理。</p>
<span id="more"></span>
<h2 id="很久以前"><a href="#很久以前" class="headerlink" title="很久以前"></a>很久以前</h2><p>1964年由Naradaya-Watson提出过一个回归模型，非常有助于理解Attention。</p>
<p>给定有标签数据：$\{(\boldsymbol x_1,y_1),(\boldsymbol x_2,y_2),…,(\boldsymbol x_n,y_n)\}$，当给定一个新样本的特征$\boldsymbol x$，需要预测其对应$y$值。一个简单粗暴的方法就是拿<strong>所有样本的$y$值做加权平均</strong>，其权重就是各个样本和新样本的相似度。</p>
<script type="math/tex; mode=display">
\hat y = \sum^{n}_{i=1}a(\boldsymbol x,\boldsymbol x_i)y_i</script><p>其中$a(·)$是度量向量相似度的函数。这个方法简单，且估计值是一致性的。</p>
<p>​    </p>
<h2 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h2><p>Attention是由Bahdanau等在2015年提出的，其用于sequence-to-sequence任务中。在本文之前的大多数sequence-to-sequence模型采用RNN类的模型，先编码再解码，典型的结构如下。</p>
<img src="/2021/06/24/2021-06-24-Self-Attention/image-20210625130151133.png" class="" title="RNN-ed-coder">
<p>这样有个缺点，RNN是前后依赖的，一旦序列很长，就很难学习到之前的信息，$\boldsymbol h_\tau$会把很早之前的信息忘掉。因此作者提出了Attention的方法，让每一个decoder隐层$\boldsymbol s$都能从所有的encoder隐层$\boldsymbol h$获取信息。</p>
<img src="/2021/06/24/2021-06-24-Self-Attention/1624597133180_src" class="" title="img"> 
<p>在中间加一层$\boldsymbol c$，其由所有的encoder隐层$\boldsymbol h$加权平均得到，这样每个$\boldsymbol c$都包含了所有输入的信息，然后再将$\boldsymbol c$输入至其对应的解码器隐层$\boldsymbol s$中。那么关键的问题是：从$\boldsymbol h$得到$\boldsymbol c$时，加权平均的权重是什么？$\boldsymbol c_j$是用来预测$\boldsymbol y_j$的，而在能计算出的单元中，$\boldsymbol s_{j-1}$是离$\boldsymbol y_j$最近的结果了（可以认为这里有<strong>前后文相关假设</strong>），因此选择和$\boldsymbol s_{j-1}$和各个$\boldsymbol h$的相似度作为其权重。</p>
<p>通过这套操作可以大幅提升模型的表达能力，但模型参数量却没有量级变化。</p>
<p>这个模型在理解过程中有一些比较反直觉的地方，比如：① 相似性度量用的$\boldsymbol s_{j-1}$而不是$\boldsymbol s_{j}$；② 模型是逐个预测，这和RNN一样，因此其BP相对也更难理解一点；③ 如果忽略了逐个预测，直觉上会困惑与为什么算$\boldsymbol s$用到了权重，这个权重也是用$\boldsymbol s$计算出来的。</p>
<p>​    </p>
<h2 id="Generalized-Attention-Model"><a href="#Generalized-Attention-Model" class="headerlink" title="Generalized Attention Model"></a>Generalized Attention Model</h2><h3 id="定义一下"><a href="#定义一下" class="headerlink" title="定义一下"></a>定义一下</h3><p>给定一系列的key-value对$(\boldsymbol K, \boldsymbol V)$和query $\boldsymbol q$，一个广义的attention模型$A$可以定义如下：</p>
<script type="math/tex; mode=display">
A(\boldsymbol q,\boldsymbol K,\boldsymbol V)
= \sum_i p(a(\boldsymbol k_i,\boldsymbol q))*\boldsymbol v_i</script><p>其中$a(·)$表示相似性度量函数，$p(·)$表示归一化为概率分布。</p>
<p>如果以计算过程来图示：</p>
<script type="math/tex; mode=display">
\begin{eqnarray*}
Sequence\ of\ keys\ \boldsymbol K
\xrightarrow[]{Query \ \boldsymbol q}
&Attention\ distribution\ \boldsymbol \alpha&  \\       &×&       \\   
&Values\ \boldsymbol V&
\\
&↓& \\
&\boldsymbol o&

\end{eqnarray*}</script><ol>
<li>$\boldsymbol K$中每个向量和$\boldsymbol q$计算相似度，得到分布$\boldsymbol \alpha$</li>
<li>根据分布$\boldsymbol \alpha$，对$\boldsymbol V$做加权平均</li>
</ol>
<p>从这个计算过程可知</p>
<ul>
<li>key-value对$(\boldsymbol K, \boldsymbol V)$是一一对应的</li>
<li>一个query $\boldsymbol q$得到一个输出向量，其长度和$\boldsymbol V)$的单个向量长度相同</li>
</ul>
<p>​    </p>
<h3 id="KVq的含义"><a href="#KVq的含义" class="headerlink" title="KVq的含义"></a>KVq的含义</h3><p>这里需要把这三个东西是啥角色弄清楚，否则在self-attention中会更加迷惑。我们将最终输出向量定义为$\boldsymbol o$。</p>
<ul>
<li>query $\boldsymbol q$：问询，告诉Attention，输出的要求是什么，包含逾期的输出的信息。一个$\boldsymbol q$会得到一个输出向量$\boldsymbol o$。</li>
<li>Value $\boldsymbol V$：构成输出的值，加权平均得到最后的输出。由N个$\boldsymbol v$向量组成，$\boldsymbol v$的shape和输出$\boldsymbol o$的shape相同。虽然由哪些$\boldsymbol v$构成最终输出，要看各个$\boldsymbol v$对最终输出$\boldsymbol o$的增益，但$\boldsymbol v$自己却不会参与battle（比较相似度），而是把battle交由自己的发言人$\boldsymbol k$</li>
<li>Key $\boldsymbol K$：是$\boldsymbol V$输出的抓手！！其N个$\boldsymbol k$向量和$\boldsymbol v$一一对应，每个代表$\boldsymbol k$代表其对应的$\boldsymbol v$去和query $\boldsymbol q$匹配，来决定自己的$\boldsymbol v$对于输出有多大发言权。所以$\boldsymbol k$去和$\boldsymbol q$一般是同构的，这样比较相似度很方便。</li>
</ul>
<p>举个例子，在电影《师父》里，南方小拳种的高手要去天津扬名，开武馆，这时他得证明自己的实力（就是我们想要的输出$\boldsymbol o$）。最简单的就是他去跟各家武馆的老大（Value $\boldsymbol V$）都打一架，就知道自己有没有资格开武馆了，但是这样打完，赢了，天津武行脸上没面子，不会让你留下，输了更不行了，那怎么办呢？先教个徒弟（query $\boldsymbol q$）,让这个徒弟去和每个武馆最杰出的弟子（Key $\boldsymbol K$）打，这样打下来，根据徒弟打的成绩（相似度distribution $\boldsymbol \alpha$），就能知道师父的大概实力（输出$\boldsymbol o$）了。这就是Attention的计算过程！</p>
<p>​    </p>
<h3 id="回顾一下"><a href="#回顾一下" class="headerlink" title="回顾一下"></a>回顾一下</h3><p>了解什么是广义的Attention之后，我们再看下前面的两个例子如何在这个框架下理解。</p>
<p>很久之前的回归模型：</p>
<script type="math/tex; mode=display">
\begin{eqnarray*}
\boldsymbol X\ of\ training\ data\  (\boldsymbol K)
\xrightarrow[]{Instance\ \boldsymbol x \ (\boldsymbol q )}
&Attention\ distribution\ \boldsymbol \alpha&  \\       &×&       \\   
&\boldsymbol y \ of\ training\ data\ (\boldsymbol V)&
\\
&↓& \\
&\hat y&

\end{eqnarray*}</script><p>Bahdanau等在2015年提出的Attention，这里Key和Value是一样的，都是中间的隐层$\boldsymbol H$。</p>
<script type="math/tex; mode=display">
\begin{eqnarray*}
Encoder\ hidden\ states\ \boldsymbol H\ (\boldsymbol K)
\xrightarrow[]{Decoder\ hidden\ state\ \boldsymbol s_{i-1} \ (\boldsymbol q )}
&Attention\ distribution\ \boldsymbol \alpha&  \\       &×&       \\   
&Encoder\ hidden\ states\ \boldsymbol H\ (\boldsymbol V)&
\\
&↓& \\
&\boldsymbol s_i&
\end{eqnarray*}</script><p>​    </p>
<h2 id="self-attention"><a href="#self-attention" class="headerlink" title="self-attention"></a>self-attention</h2><h3 id="基本结构"><a href="#基本结构" class="headerlink" title="基本结构"></a>基本结构</h3><p>从不同角度可以将attention分成很多类，这里只讲self-attention。从其字面意义来说，self-attention中的权重度量的是输入序列之间的相似性，若输入序列为$(\boldsymbol w_1,…,\boldsymbol w_n)$，输出$\boldsymbol y_i=\sum_ja_{ij}·f(\boldsymbol w_j)$，其中的$a$度量的$(\boldsymbol w_1,…,\boldsymbol w_n)$之间的相似程度。</p>
<p>self-attention真正火起来是在transformer中，接下来我们重点介绍其使用的self-attention。其流程也很简单，通过三个线性变换（$\boldsymbol W_q,\boldsymbol W_k,\boldsymbol W_v$）将输入$\boldsymbol X$（$N$个词，每个词$D_x$维度，$\boldsymbol W_q$是$D_q×D_x$维度）变换为$\boldsymbol Q,\boldsymbol K,\boldsymbol V$。然后就是个attention，得到结果$\boldsymbol H$。</p>
<img src="/2021/06/24/2021-06-24-Self-Attention/image-20210708161738095.png" class="" title="image-20210708161738095">
<p>这个结构的好处是：（1）可以提取全局的信息；（2）可以并行。</p>
<p>​    </p>
<h3 id="Multi-Head-Self-Attention"><a href="#Multi-Head-Self-Attention" class="headerlink" title="Multi-Head Self-Attention"></a>Multi-Head Self-Attention</h3><p>类似于CNN，self-attention也可以多个并行，最后concate起来，称为Multi-Head Self-Attention，各个Head的计算也可以并行。在多个文献的探索中，会发现多个Head时，head之间会分工提取不同范围内的语义信息。</p>
<img src="/2021/06/24/2021-06-24-Self-Attention/image-20210708163214552.png" class="" title="image-20210708163214552">
<p>​    </p>
<h3 id="效果"><a href="#效果" class="headerlink" title="效果"></a>效果</h3><p><a target="_blank" rel="noopener" href="http://ai.googleblog.com/2017/08/transformer-novel-neural-network.html">transformer-novel-neural-network</a>给出过一个非常直观的效果，下面两个句子只有一词只差，但其导致了代词it的含义不同，而这个差异可以被self-attention中的相关性（图中的线条颜色深度）识别到。</p>
<img src="/2021/06/24/2021-06-24-Self-Attention/image-20210708165933828.png" class="" title="image-20210708165933828">
<p>​    </p>
<h2 id="网络结构合理性"><a href="#网络结构合理性" class="headerlink" title="网络结构合理性"></a>网络结构合理性</h2><p>理解Attention的时候，总是会有一个问题，模型的目标怎么让模型学到这些信息的？特别是Self-Attention,QKV都是随机初始化，为什么模型可以学到这样的上下文结果呢？</p>
<p>像word-embedding这种方法，我们可以从其数据结构的基本假设中知道，其训练过程必然可以试emdedding向量具有空间关系。</p>
<p>但Self-Attention，包括RNN、RNN，其逻辑是，“模型结构适合学习到这种信息，你看结果，确实学到了，所以很好很合理”。而不是“我的模型假设下，他必然会学到这样的信息，所以很强”。</p>
<p>​    </p>
<p><strong>从信息提取要求理解</strong></p>
<ol>
<li>表示层提取越多越准确的信息，任务学习得越好（表示学习基本的假设）</li>
<li>越能精确表示出各个词的含义，句子的含义表示的越准确</li>
<li>具备从其他词中获取当前词的含义时，模型会尽可能精准得利用</li>
</ol>
<p>​    </p>
<p><strong>从数据生成理论理解</strong></p>
<ol>
<li>模型越能知道数据生成过程，预测会越准确，所以模型具备表示出数据生成过程时，必然会越准确。</li>
<li>大部分NLP，CV任务都是以人的理解方式形成的任务，标签是人打的，可以认为从$X→y$的数据生成过程，就是人理解的方式，所以模型越能提供这种表达能力，就越好。特别是NLP，连$X$，也就是文本本身都是人类自己生成的。</li>
</ol>
<p>​    </p>
<p><strong>从熵的角度理解</strong></p>
<p>如果是在和人类大脑有着相似的容量约束情况下，最优的pattern就是容易被理解的，人类的理解方式就是最优的理解方式。</p>
<ol>
<li>模型要在有限的参数量，自发地学习pattern，必然需要信息压缩程度最高，最有规律的pattern，信息编码效率越高，越有规律，越能被人类理解。</li>
<li>进化过程中，人类理解东西的方式，也是在相同的约束下的成果，所以相互契合是可以理解的。</li>
</ol>
<p>​    </p>
<p>​    </p>
<p><strong>reference</strong></p>
<p>[1]. <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1904.02874.pdf">An Attentive Survey of Attention </a><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1904.02874.pdf">Models</a></p>
<p>[2].<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1409.0473">Neural Machine Translation by Jointly Learning to Align and Translate</a></p>
<p>[3].<a target="_blank" rel="noopener" href="https://jalammar.github.io/illustrated-transformer/">illustrated-transformer</a></p>
<p>[4].<a target="_blank" rel="noopener" href="http://ai.googleblog.com/2017/08/transformer-novel-neural-network.html">transformer-novel-neural-network</a></p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># 深度学习</a>
              <a href="/tags/Attention/" rel="tag"># Attention</a>
              <a href="/tags/transformer/" rel="tag"># transformer</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/06/23/2021-06-23-%E6%90%AD%E5%BB%BAhexo%E5%8D%9A%E5%AE%A2/" rel="prev" title="搭建hexo博客">
      <i class="fa fa-chevron-left"></i> 搭建hexo博客
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/06/26/2021-06-26-hexo%E5%9B%BE%E7%89%87%E8%B7%AF%E5%BE%84%E9%85%8D%E7%BD%AE/" rel="next" title="hexo图片路径配置">
      hexo图片路径配置 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BE%88%E4%B9%85%E4%BB%A5%E5%89%8D"><span class="nav-number">1.</span> <span class="nav-text">很久以前</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Attention"><span class="nav-number">2.</span> <span class="nav-text">Attention</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Generalized-Attention-Model"><span class="nav-number">3.</span> <span class="nav-text">Generalized Attention Model</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89%E4%B8%80%E4%B8%8B"><span class="nav-number">3.1.</span> <span class="nav-text">定义一下</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#KVq%E7%9A%84%E5%90%AB%E4%B9%89"><span class="nav-number">3.2.</span> <span class="nav-text">KVq的含义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%9E%E9%A1%BE%E4%B8%80%E4%B8%8B"><span class="nav-number">3.3.</span> <span class="nav-text">回顾一下</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#self-attention"><span class="nav-number">4.</span> <span class="nav-text">self-attention</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84"><span class="nav-number">4.1.</span> <span class="nav-text">基本结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Multi-Head-Self-Attention"><span class="nav-number">4.2.</span> <span class="nav-text">Multi-Head Self-Attention</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%88%E6%9E%9C"><span class="nav-number">4.3.</span> <span class="nav-text">效果</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E5%90%88%E7%90%86%E6%80%A7"><span class="nav-number">5.</span> <span class="nav-text">网络结构合理性</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Dongdong Wang</p>
  <div class="site-description" itemprop="description">从爪印判断，这是头雄狮</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">40</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">63</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="mailto:wangdongdong122@163.com" title="E-Mail → mailto:wangdongdong122@163.com" rel="noopener" target="_blank"><i class="envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



        <div class="wechat_OA">
            <span>欢迎加微信讨论</span>
            <br>
            <!-- 这里添加你的二维码图片 -->
            <img src ="/images/wechat.png" style="zoom:40%;" />
        </div>
      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Dongdong Wang</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>


        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  













<script>
if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js', () => {
    mermaid.initialize({
      theme    : 'forest',
      logLevel : 3,
      flowchart: { curve     : 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 }
    });
  }, window.mermaid);
}
</script>


  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
