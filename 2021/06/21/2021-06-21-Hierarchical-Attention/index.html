<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Hierarchical Attention Networks for Document Classification | 凛冬将至</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="Self-Attention谁先提出的，各文章里写的不一样吗，Attention Is All You Need中说是Jakob.2016年提出的，An Attentive Survey of Attention Models中说是Yang et al. 2016，本篇介绍后者。 Introduction核心思路：  分层（hierarchical structure）：先构建“词 → 句子”级的">
<meta property="og:type" content="article">
<meta property="og:title" content="Hierarchical Attention Networks for Document Classification">
<meta property="og:url" content="http://wangdongdong122.github.io/2021/06/21/2021-06-21-Hierarchical-Attention/index.html">
<meta property="og:site_name" content="凛冬将至">
<meta property="og:description" content="Self-Attention谁先提出的，各文章里写的不一样吗，Attention Is All You Need中说是Jakob.2016年提出的，An Attentive Survey of Attention Models中说是Yang et al. 2016，本篇介绍后者。 Introduction核心思路：  分层（hierarchical structure）：先构建“词 → 句子”级的">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://wangdongdong122.github.io/2021/06/21/images/Hierarchical-Attention.images/1624323245414_src-1624427147187">
<meta property="article:published_time" content="2021-06-21T01:26:17.000Z">
<meta property="article:modified_time" content="2021-06-23T09:48:53.500Z">
<meta property="article:author" content="Dongdong Wang">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="Attention">
<meta property="article:tag" content="Transformer">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="每日论文">
<meta property="article:tag" content="经典算法">
<meta property="article:tag" content="NLP">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://wangdongdong122.github.io/2021/06/21/images/Hierarchical-Attention.images/1624323245414_src-1624427147187">
  
    <link rel="alternate" href="/atom.xml" title="凛冬将至" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 5.4.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">凛冬将至</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">冬天的故事</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS 订阅"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="搜索"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://wangdongdong122.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-2021-06-21-Hierarchical-Attention" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/06/21/2021-06-21-Hierarchical-Attention/" class="article-date">
  <time class="dt-published" datetime="2021-06-21T01:26:17.000Z" itemprop="datePublished">2021-06-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      Hierarchical Attention Networks for Document Classification
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>Self-Attention谁先提出的，各文章里写的不一样吗，<a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">Attention Is All You Need</a>中说是<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1606.01933.pdf">Jakob.2016</a>年提出的，<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1904.02874.pdf">An Attentive Survey of Attention Models</a>中说是<a target="_blank" rel="noopener" href="https://www.aclweb.org/anthology/N16-1174.pdf">Yang et al. 2016</a>，本篇介绍后者。</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>核心思路：</p>
<ol>
<li>分层（hierarchical structure）：先构建“词 → 句子”级的表达，再聚合到文档级，即“句子 → 文档”</li>
<li>Attention：不同的词和句子包含的信息和重要程度都依赖于上下文，为了将其考虑进来，所以作者用两层的Attention</li>
</ol>
<p>作者没有提self-attention，应该是还没意识到这一点的牛逼之处。</p>
<h2 id="Hierarchical-Attention-Networks"><a href="#Hierarchical-Attention-Networks" class="headerlink" title="Hierarchical Attention Networks"></a>Hierarchical Attention Networks</h2><h3 id="encoder"><a href="#encoder" class="headerlink" title="encoder"></a>encoder</h3><p>encoder采用GRU产生，原理及结构省略</p>
<h3 id="Hierarchical-Attention"><a href="#Hierarchical-Attention" class="headerlink" title="Hierarchical Attention"></a>Hierarchical Attention</h3><p>数据表达</p>
<ul>
<li>sentences $\vec{s_i}$ ,$i=1,2,…L$</li>
<li>words represents: $w_{it}$, $t ∈ [1, T]$,   $\vec{s_i}$ contains $T$ words</li>
</ul>
<h4 id="Word-Encoder"><a href="#Word-Encoder" class="headerlink" title="Word Encoder"></a>Word Encoder</h4><p>先embedding，过双向GRU，将隐层concatenate起来</p>
<ol>
<li>word embedding: $W_e$, $x_{ij}=W_ew_{ij}$</li>
<li>forward GRU: $\overset{\rightarrow}{h_{it}}=\overset{\rightarrow}{GRU}(x_{it}),\ t ∈ [1, T]$</li>
<li>backward GRU:  $\overset{\leftarrow}{h_{it}}=\overset{\leftarrow}{GRU}(x_{it}),\ t ∈ [T, 1]$</li>
<li>concatenate:  $h_{it}=[\overset{\rightarrow}{h_{it}},\overset{\leftarrow}{h_{it}}]$</li>
</ol>
<h4 id="Word-Attention"><a href="#Word-Attention" class="headerlink" title="Word Attention"></a>Word Attention</h4><p>将对句子含义起重要作用的词提取出来，聚合成一个句子向量。先将所有的（$t ∈ [1, T]$）$h_{it}$过全连接得到Key: $u_{it}$；然后和随机变量的query: $u_w$求相似度分布: $\alpha$；最后将最开始的  $h_{it}$作为Value，加权得到sentence vector: $s_i$。所有信息都是从$h_{it}$中得到。</p>
<script type="math/tex; mode=display">
\begin{eqnarray*}
u_{it} &=& tanh(W_wh_{it}+b_w) \tag{FC layer}  \\                 
\\
\alpha_{it} &=& \frac{exp(u^{T}_{it}u_w)}{\sum_{t}{exp(u^{T}_{it}u_w)}}  
\tag{measure similarity & normalize}
\\
\\
s_i &=& \sum_{t}{\alpha_{it}h_{it}}
\tag{weighted sum}


\end{eqnarray*}</script><p>其中$ u_w$(word context vector)是随机初始化，然后在训练过程中学习的，可以当做是一个固定的query，用来表示这个句子中重要的信息。</p>
<p>维度信息：每个句子只产生一个向量$s_i$，其长度和单个词的BiGRU隐层concat之后的向量$h_{it}$长度相同（不一定等于词向量$w_{it}$的长度）。</p>
<h4 id="Sentence-Encoder"><a href="#Sentence-Encoder" class="headerlink" title="Sentence Encoder"></a>Sentence Encoder</h4><p>句子的encoder也和词的类似，先过bidirectional GRU然后concatenate。</p>
<ol>
<li>forward GRU：$\overset{\rightarrow}{h_{i}}=\overset{\rightarrow}{GRU}(s_{i}),\ i ∈ [1, L]$</li>
<li>backward GRU: $\overset{\leftarrow}{h_{i}}=\overset{\leftarrow}{GRU}(s_{i}),\ i ∈ [L, 1]$</li>
<li>concatenate: $h_{i}=[\overset{\rightarrow}{h_{i}},\overset{\leftarrow}{h_{i}}]$</li>
</ol>
<h4 id="Sentence-Attention"><a href="#Sentence-Attention" class="headerlink" title="Sentence Attention"></a>Sentence Attention</h4><p>这部分也和Word Attention部分一样，只是换了个层次</p>
<script type="math/tex; mode=display">
\begin{eqnarray*}u_{i} &=& tanh(W_sh_{i}+b_s) 
 \tag{FC layer}  
 \\                 
 \\
 \alpha_{i} &=& \frac{exp(u^{T}_{i}u_s)}{\sum_{i}{exp(u^{T}_{i}u_s)}}  
 \tag{measure similarity & normalize}
 \\
 \\
 v &=& \sum_{i}{\alpha_{i}h_{i}}
 \tag{weighted sum}

 \end{eqnarray*}</script><p>这里就将一个文档表示成一个向量$v$， 其长度和单个句子的BiGRU隐层concat之后的向量$h_{i}$长度相同。</p>
<h3 id="Document-Classification"><a href="#Document-Classification" class="headerlink" title="Document Classification"></a>Document Classification</h3><p>这部分很简单，文档向量$v$过softmax，然后用log loss训练。</p>
<script type="math/tex; mode=display">
\begin{eqnarray*}

p &=& softmax(W_cv+b_c) 
 \tag{softmax}  
 \\                 
 \\
L &=& -\sum_{d}{log\ p_{dj}}
 \tag{log loss}

 \end{eqnarray*}</script><p>其中，$j$是文档$d$的标签，只对正确标签计算loss。</p>
<h2 id="Results-and-analysis"><a href="#Results-and-analysis" class="headerlink" title="Results and analysis"></a>Results and analysis</h2><p>  Yelp 2013上的两个文档，左边是给出了5星好评的，右边是0星差评的。模型可以捕捉到那些词重要。<img src="../images/Hierarchical-Attention.images/1624323245414_src-1624427147187" alt="img" style="zoom: 50%;" /></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://wangdongdong122.github.io/2021/06/21/2021-06-21-Hierarchical-Attention/" data-id="ckq9m11380000qopp0o158znw" data-title="Hierarchical Attention Networks for Document Classification" class="article-share-link">分享</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Attention/" rel="tag">Attention</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/" rel="tag">NLP</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Transformer/" rel="tag">Transformer</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%AF%8F%E6%97%A5%E8%AE%BA%E6%96%87/" rel="tag">每日论文</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">深度学习</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95/" rel="tag">经典算法</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2021/06/22/2021-06-22-Decomposable-Attention/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">前一篇</strong>
      <div class="article-nav-title">
        
          A Decomposable Attention Model for Natural Language Inference
        
      </div>
    </a>
  
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Attention/" rel="tag">Attention</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NLP/" rel="tag">NLP</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Transformer/" rel="tag">Transformer</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/bug/" rel="tag">bug</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%AF%8F%E6%97%A5%E8%AE%BA%E6%96%87/" rel="tag">每日论文</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">深度学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95/" rel="tag">经典算法</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签云</h3>
    <div class="widget tagcloud">
      <a href="/tags/Attention/" style="font-size: 20px;">Attention</a> <a href="/tags/NLP/" style="font-size: 20px;">NLP</a> <a href="/tags/Transformer/" style="font-size: 20px;">Transformer</a> <a href="/tags/bug/" style="font-size: 10px;">bug</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 20px;">机器学习</a> <a href="/tags/%E6%AF%8F%E6%97%A5%E8%AE%BA%E6%96%87/" style="font-size: 20px;">每日论文</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="font-size: 20px;">深度学习</a> <a href="/tags/%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95/" style="font-size: 20px;">经典算法</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/06/">六月 2021</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2021/06/23/2021-06-23-%E6%90%AD%E5%BB%BAhexo%E5%8D%9A%E5%AE%A2/">搭建hexo博客</a>
          </li>
        
          <li>
            <a href="/2021/06/22/2021-06-22-Decomposable-Attention/">A Decomposable Attention Model for Natural Language Inference</a>
          </li>
        
          <li>
            <a href="/2021/06/21/2021-06-21-Hierarchical-Attention/">Hierarchical Attention Networks for Document Classification</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2021 Dongdong Wang<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>