<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>A Decomposable Attention Model for Natural Language Inference | 凛冬将至</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="Self-Attention谁先提出的，各文章里写的不一样吗，Attention Is All You Need中说是Jakob et al.2016年提出的，An Attentive Survey of Attention Models中说是Yang et al. 2016，本篇介绍前者。 Abstract：simpler and more lightweight，比之前的模型参数少一个数量级，">
<meta property="og:type" content="article">
<meta property="og:title" content="A Decomposable Attention Model for Natural Language Inference">
<meta property="og:url" content="http://wangdongdong122.github.io/2021/06/22/2021-06-22-Decomposable-Attention/index.html">
<meta property="og:site_name" content="凛冬将至">
<meta property="og:description" content="Self-Attention谁先提出的，各文章里写的不一样吗，Attention Is All You Need中说是Jakob et al.2016年提出的，An Attentive Survey of Attention Models中说是Yang et al. 2016，本篇介绍前者。 Abstract：simpler and more lightweight，比之前的模型参数少一个数量级，">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://wangdongdong122.github.io/2021/06/22/images/Decomposable-Attention.images/1624336820545_src">
<meta property="article:published_time" content="2021-06-22T01:29:17.000Z">
<meta property="article:modified_time" content="2021-06-24T23:04:39.770Z">
<meta property="article:author" content="Dongdong Wang">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="Attention">
<meta property="article:tag" content="Transformer">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="每日论文">
<meta property="article:tag" content="经典算法">
<meta property="article:tag" content="NLP">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://wangdongdong122.github.io/2021/06/22/images/Decomposable-Attention.images/1624336820545_src">
  
    <link rel="alternate" href="/atom.xml" title="凛冬将至" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 5.4.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">凛冬将至</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">冬天的故事</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS 订阅"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="搜索"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://wangdongdong122.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-2021-06-22-Decomposable-Attention" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/06/22/2021-06-22-Decomposable-Attention/" class="article-date">
  <time class="dt-published" datetime="2021-06-22T01:29:17.000Z" itemprop="datePublished">2021-06-22</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      A Decomposable Attention Model for Natural Language Inference
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>Self-Attention谁先提出的，各文章里写的不一样吗，<a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">Attention Is All You Need</a>中说是<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1606.01933.pdf">Jakob et al.2016</a>年提出的，<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1904.02874.pdf">An Attentive Survey of Attention Models</a>中说是<a target="_blank" rel="noopener" href="https://www.aclweb.org/anthology/N16-1174.pdf">Yang et al. 2016</a>，本篇介绍前者。</p>
<p>Abstract：simpler and more lightweight，比之前的模型参数少一个数量级，而且不依赖与任何词的顺序信息</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>近年来，Natural language inference(NLI)领域中提出的模型都数据量巨大，计算成本非常高。但实际上NLI中，往往是只需要少量的局域信息，然后将这些局域信息汇总起来进行预测即可。</p>
<h2 id="Approach"><a href="#Approach" class="headerlink" title="Approach"></a>Approach</h2><p><strong>模型结构</strong></p>
<p>  <img src="../images/Decomposable-Attention.images/1624336820545_src" alt="img" style="zoom: 67%;" /> </p>
<p><strong>Input representation</strong></p>
<p>输入是两个句子（不一定等长，$l_a$和$l_b$），句子是由每个词的embedding向量（长度$d$）组成的矩阵，label是多分类的one-hot标签（类别数为$C$）。</p>
<script type="math/tex; mode=display">
\begin{eqnarray*}
input\ sentence\ matrix1&:& \textbf{a}=(a_1,...,a_{l_a}) ,\ 
l_a:length
\\         
input\ sentence\ matrix2&:& \textbf{b}=(b_1,...,b_{l_b}) ,\ 
l_b:length     
\\
word\ embedding\ vector&:&a_i, b_j ∈ R^d
\\
indicator\ vector&:& \textbf{y}^{(n)}=(y^{(n)}_1,...,y^{(n)}_C),\ C:classes\ number 
\\

training\ data&:&\{ \textbf{a}^{(n)},\textbf{b}^{(n)},\textbf{y}^{(n)} \}^N_{n=1}
\\
test\ data&:&(\textbf{a},\textbf{b})

\end{eqnarray*}</script><p>$\textbf{a}$和$\textbf{b}$可以做一些变换之后再输入模型，标准版模型就输入$\textbf{a}$和$\textbf{b}$了。</p>
<h3 id="Attend"><a href="#Attend" class="headerlink" title="Attend"></a>Attend</h3><p>这一步是用attention，让两个句子$(\textbf{a},\textbf{b})$相互表示对方，得到$(\boldsymbol\beta,\boldsymbol\alpha)$,$\boldsymbol\beta$是$\boldsymbol b$表示出来的$\boldsymbol a$。</p>
<p>首先要计算相关性权重</p>
<script type="math/tex; mode=display">
e_{ij}:=F^{'}{(a_i,b_j)}

:=F(a_i)^TF(b_j)</script><p>这里有一个计算简化，如果按照$F^{‘}{(a_i,b_j)}$计算，则需要计算$l_a×l_b$次$F^{‘}{(·)}$，但按照后者计算则只用计算$l_a+l_b$次$F{(·)}$。</p>
<p>然后对weight标准化，并根据标准化的权重加权得到新的词表达：$\beta_i,\alpha_j$。需要注意的是，此处命名有点反直觉，$\beta_i$的计算中，query是$a_i$，Key和Value都是$\textbf{b}$。$\alpha_j$则相反，query是$b_i$，Key和Value都是$\textbf{a}$。</p>
<script type="math/tex; mode=display">
\begin{eqnarray*}
\beta_i:=\sum^{l_b}_{j=1}
\frac{exp(e_{ij})}{\sum^{l_b}_{k=1}exp(e_{ik})}b_j
\\
\\
\alpha_j:=\sum^{l_a}_{i=1}
\frac{exp(e_{ij})}{\sum^{l_a}_{k=1}exp(e_{kj})}a_i
\end{eqnarray*}</script><h3 id="Compare"><a href="#Compare" class="headerlink" title="Compare"></a>Compare</h3><p>接下来，分别比较$\{(a_i,\beta_i)\}^{l_a}_{i=1}$和$\{(b_j,\alpha_j)\}^{l_b}_{j=1}$中每个pair，也就是看看用$\textbf{b}$表示出来的$a_i$，即$\beta_i$和真正的$a_i$有多像，如果很像（反映在$\textbf{v}_{1,i}$中），则证明句子$\textbf{b}$中有$a_i$的信息。</p>
<script type="math/tex; mode=display">
\begin{eqnarray*}
\textbf{v}_{1,i}:=G([a_i,\beta_i])
\ \ \forall i\in [1,...,l_a]
\\
\\
\textbf{v}_{2,j}:=G([b_j,\alpha_j])
\ \ \forall j\in [1,...,l_b]
\end{eqnarray*}</script><p>其中$[·，·]$表示concatenation，$G$在论文中是全连接。因为这个计算次数是线性的，所以不用像前面一样将$a_i,\beta_i$拆开计算了。</p>
<h3 id="Aggregate"><a href="#Aggregate" class="headerlink" title="Aggregate"></a>Aggregate</h3><p>将每个词的比较向量聚合成句子的比较向量</p>
<script type="math/tex; mode=display">
\textbf{v}_1 = \sum^{l_a}_{i=1}\textbf{v}_{1,i}\ \ \ ,\ \ \ 
\textbf{v}_2 = \sum^{l_b}_{j=1}\textbf{v}_{2,j}</script><p>然后concat起来过分类器，得到预测结果</p>
<script type="math/tex; mode=display">
{\widehat{\textbf y}} = H([\textbf v_1,\textbf v_2])
,\ \ \ \ 
{\widehat{\textbf y}} \in R^C</script><h3 id="Intra-Sentence-Attention"><a href="#Intra-Sentence-Attention" class="headerlink" title="Intra-Sentence Attention"></a>Intra-Sentence Attention</h3><p>重点<strong>self-attention</strong>来了，前面的模型里，输入是简单的word embedding。这里提出一种增强输入表达的方法：intra-sentence attention，将句子中每个词之间的关系表示出来。</p>
<script type="math/tex; mode=display">
f_{ij}:=F_{intra}(a_i)^TF_{intra}(a_j),
\\
\\
a^{'}_i=\sum^{l_a}_{j=1}\frac{exp(f_{ij}+d_{i-j})}{exp(f_{ik}+d_{i-k})}a_j
\\\\</script><p>其中，$F_{intra}$是一个全连接，$f_{ij}$就表示$a_i$和$a_j$的相似程度。$d_{i-j}$是距离敏感度偏置项，作用是不让某个词的权重过小。</p>
<p><strong>学点巴洛克风格的词</strong></p>
<p>vanilla version：The “<strong>vanilla</strong> <strong>version</strong>“ is generally the version that has no customisation applied - it is the “regular”,  “ordinary” or “plain old” version. For a lot of consumer based software - this would be the only version. You would not build custom versions for every user.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://wangdongdong122.github.io/2021/06/22/2021-06-22-Decomposable-Attention/" data-id="ckqd2qfru000fg4pp87ie0uno" data-title="A Decomposable Attention Model for Natural Language Inference" class="article-share-link">分享</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Attention/" rel="tag">Attention</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/" rel="tag">NLP</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Transformer/" rel="tag">Transformer</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%AF%8F%E6%97%A5%E8%AE%BA%E6%96%87/" rel="tag">每日论文</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">深度学习</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95/" rel="tag">经典算法</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2021/06/23/2021-06-23-%E6%90%AD%E5%BB%BAhexo%E5%8D%9A%E5%AE%A2/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">前一篇</strong>
      <div class="article-nav-title">
        
          搭建hexo博客
        
      </div>
    </a>
  
  
    <a href="/2021/06/21/2021-06-21-Hierarchical-Attention/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">后一篇</strong>
      <div class="article-nav-title">Hierarchical Attention Networks for Document Classification</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Attention/" rel="tag">Attention</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NLP/" rel="tag">NLP</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Transformer/" rel="tag">Transformer</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/bug/" rel="tag">bug</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%AF%8F%E6%97%A5%E8%AE%BA%E6%96%87/" rel="tag">每日论文</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">深度学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95/" rel="tag">经典算法</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签云</h3>
    <div class="widget tagcloud">
      <a href="/tags/Attention/" style="font-size: 20px;">Attention</a> <a href="/tags/NLP/" style="font-size: 20px;">NLP</a> <a href="/tags/Transformer/" style="font-size: 20px;">Transformer</a> <a href="/tags/bug/" style="font-size: 10px;">bug</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 20px;">机器学习</a> <a href="/tags/%E6%AF%8F%E6%97%A5%E8%AE%BA%E6%96%87/" style="font-size: 20px;">每日论文</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="font-size: 20px;">深度学习</a> <a href="/tags/%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95/" style="font-size: 20px;">经典算法</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/06/">六月 2021</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2021/06/23/2021-06-23-%E6%90%AD%E5%BB%BAhexo%E5%8D%9A%E5%AE%A2/">搭建hexo博客</a>
          </li>
        
          <li>
            <a href="/2021/06/22/2021-06-22-Decomposable-Attention/">A Decomposable Attention Model for Natural Language Inference</a>
          </li>
        
          <li>
            <a href="/2021/06/21/2021-06-21-Hierarchical-Attention/">Hierarchical Attention Networks for Document Classification</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2021 Dongdong Wang<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</body>
</html>