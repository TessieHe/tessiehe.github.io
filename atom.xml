<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>凛冬将至</title>
  
  <subtitle>冬天的故事</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://wangdongdong122.github.io/"/>
  <updated>2021-06-23T23:03:33.818Z</updated>
  <id>http://wangdongdong122.github.io/</id>
  
  <author>
    <name>Dongdong Wang</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>搭建hexo博客</title>
    <link href="http://wangdongdong122.github.io/2021/06/23/2021-06-23-%E6%90%AD%E5%BB%BAhexo%E5%8D%9A%E5%AE%A2/"/>
    <id>http://wangdongdong122.github.io/2021/06/23/2021-06-23-搭建hexo博客/</id>
    <published>2021-06-23T05:02:38.000Z</published>
    <updated>2021-06-23T23:03:33.818Z</updated>
    
    <content type="html"><![CDATA[<h2 id="安装hexo"><a href="#安装hexo" class="headerlink" title="安装hexo"></a>安装hexo</h2><p>参考内容：<a href="https://www.jianshu.com/p/9bbae1d105be">https://www.jianshu.com/p/9bbae1d105be</a></p><h2 id="配置hexo与更换Next主题"><a href="#配置hexo与更换Next主题" class="headerlink" title="配置hexo与更换Next主题"></a>配置hexo与更换Next主题</h2><p><strong>下载主题</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/iissnan/hexo-theme-next themes/next</span><br></pre></td></tr></table></figure><p>由于github被墙，使用代理科学上网后，clone经常报<code>OpenSSL SSL_connect</code>的错，因此我直接在网页上下载的zip包，解压到<code>themes/next</code>文件夹。</p><p><strong>页面报错</strong></p><p>跟换主题后重新<code>hexo s</code>，页面无法显示内容，提示以下信息：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;% extends &#x27;_layout.swig&#x27; %&#125; &#123;% import &#x27;_macro/post.swig&#x27; as post_template %&#125; &#123;% import &#x27;_macro/sidebar.swig&#x27; as sidebar_template %&#125; &#123;% block title %&#125;&#123;&#123; page.title &#125;&#125; | &#123;&#123; config.title &#125;&#125;&#123;% endblock %&#125; &#123;% block page_class %&#125;page-post-detail&#123;% endblock %&#125; &#123;% block content %&#125;</span><br><span class="line">&#123;&#123; post_template.render(page) &#125;&#125;</span><br><span class="line">&#123;% if theme.jiathis %&#125; &#123;% include &#x27;_partials/share/jiathis.swig&#x27; %&#125; &#123;% elseif theme.baidushare %&#125; &#123;% include &#x27;_partials/share/baidushare.swig&#x27; %&#125; &#123;% elseif theme.add_this_id %&#125; &#123;% include &#x27;_partials/share/add-this.swig&#x27; %&#125; &#123;% elseif theme.duoshuo_shortname and theme.duoshuo_share %&#125; &#123;% include &#x27;_partials/share/duoshuo_share.swig&#x27; %&#125; &#123;% endif %&#125;</span><br><span class="line">&#123;% endblock %&#125; &#123;% block sidebar %&#125; &#123;&#123; sidebar_template.render(true) &#125;&#125; &#123;% endblock %&#125; &#123;% block script_extra %&#125; &#123;% include &#x27;_scripts/pages/post-details.swig&#x27; %&#125; &#123;% endblock %&#125;</span><br></pre></td></tr></table></figure><p>根据<a href="https://blog.csdn.net/qq_39898645/article/details/109181736">博客</a>介绍，原因是hexo在5.0之后把swig给删除了需要自己手动安装，安装后解决了该问题。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm i hexo-renderer-swig</span><br></pre></td></tr></table></figure><p><strong>显示公式</strong></p><p>由于我的博客中有大量公式，页面中无法显示。参考<a href="https://www.jianshu.com/p/9b9c241146bc">博客</a>解决了该问题，问题的核心是配置好两个因素：mathjax和kramed。</p><p>添加mathjax</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm uninstall hexo-math --save</span><br><span class="line">npm install hexo-renderer-mathjax --save</span><br></pre></td></tr></table></figure><p>换渲染引擎</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm uninstall hexo-renderer-marked --save</span><br><span class="line">npm install hexo-renderer-kramed --save</span><br></pre></td></tr></table></figure><p>由于我之前瞎试，装过pandoc，也要一起卸掉：<code>npm uninstall hexo-renderer-pandoc --save</code></p><p>修改渲染引擎的bug：到博客根目录下，找到node_modules\kramed\lib\rules\inline.js，第11行的 escape 变量：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">//escape: /^\\([\\`*&#123;&#125;\[\]()#$+\-.!_&gt;])/,</span><br><span class="line">escape: /^\\([`*\[\]()#$+\-.!_&gt;])/,</span><br></pre></td></tr></table></figure><p>第20行的em变量：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">//em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,</span><br><span class="line">em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,</span><br></pre></td></tr></table></figure><p>配置.\themes\next\_config.yml</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># MathJax Support</span><br><span class="line">mathjax:</span><br><span class="line">  enable: true</span><br><span class="line">  per_page: true</span><br><span class="line">  engine: mathjax</span><br><span class="line">  cdn: //cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML</span><br></pre></td></tr></table></figure><p>在文章的Front-matter里打开mathjax开关</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">title: Hierarchical Attention Networks for Document Classification</span><br><span class="line">date: 2021-06-23 09:26:17</span><br><span class="line">tags:</span><br><span class="line">    - 深度学习</span><br><span class="line">    - Attention</span><br><span class="line">    - Transformer</span><br><span class="line">    - 机器学习</span><br><span class="line">    - 每日论文</span><br><span class="line">    - 经典算法</span><br><span class="line">    - NLP</span><br><span class="line">mathjax: true</span><br></pre></td></tr></table></figure><p>重启以下</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hexo clean</span><br><span class="line">hexo generate</span><br><span class="line">hexo s</span><br></pre></td></tr></table></figure><p>到这里我的公式一部分能显示，一部分不能显示。继续查了查，发现别人也有这个情况，比如多行公式的时候显示不了，是因为不能出现连续的大括号<code>&#123;&#123;</code>​。我怀疑我的也是类似的问题，于是试了一下在我的<code>:公式</code>的结构中，把冒号删了，居然所有公式都正常显示了，然后我又把冒号加回去，还是都可以显示！！！不知道上面那堆设置需要时间起作用还是我的修改触发了什么，总之问题解决了。</p><p>操作完这套后，<a href="http://localhost:4000/可以正常显示公式，但github.io上却不行（有可能是我clean&amp;generate没有放在最后一步），我又参考[博客](https://www.cnblogs.com/zhyantao/p/10424874.html)增加了一些操作。">http://localhost:4000/可以正常显示公式，但github.io上却不行（有可能是我clean&amp;generate没有放在最后一步），我又参考[博客](https://www.cnblogs.com/zhyantao/p/10424874.html)增加了一些操作。</a></p><p>打开<code>node_modules/hexo-renderer-mathjax/mathjax.html</code>，将 <code>&lt;script&gt;</code> 改为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;script src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot;&gt;&lt;/script&gt;</span><br></pre></td></tr></table></figure><p>打开<code>node_modules/hexo-renderer-kramed/lib/renderer.js</code>，将</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">// Change inline math rule</span><br><span class="line">function formatText(text) &#123;</span><br><span class="line">  // Fit kramed&#x27;s rule: $$ + \1 + $$</span><br><span class="line">  return text.replace(/`\$(.*?)\$`/g, &#x27;$$$$$1$$$$&#x27;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>改为</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// Change inline math rule</span><br><span class="line">function formatText(text) &#123;</span><br><span class="line">    return text;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>打开<code>node_modules/hexo-renderer-mathjax/mathjax.html</code>，将 <code>&lt;script&gt;</code> 改为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;&lt;/script&gt;</span><br></pre></td></tr></table></figure><p>最后再重启一次</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hexo clean</span><br><span class="line">hexo generate</span><br><span class="line">hexo d</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;安装hexo&quot;&gt;&lt;a href=&quot;#安装hexo&quot; class=&quot;headerlink&quot; title=&quot;安装hexo&quot;&gt;&lt;/a&gt;安装hexo&lt;/h2&gt;&lt;p&gt;参考内容：&lt;a href=&quot;https://www.jianshu.com/p/9bbae1d105be&quot;&gt;
      
    
    </summary>
    
    
      <category term="bug" scheme="http://wangdongdong122.github.io/tags/bug/"/>
    
  </entry>
  
  <entry>
    <title>A Decomposable Attention Model for Natural Language Inference</title>
    <link href="http://wangdongdong122.github.io/2021/06/22/2021-06-22-Decomposable-Attention/"/>
    <id>http://wangdongdong122.github.io/2021/06/22/2021-06-22-Decomposable-Attention/</id>
    <published>2021-06-22T01:29:17.000Z</published>
    <updated>2021-06-23T23:12:58.194Z</updated>
    
    <content type="html"><![CDATA[<p>Self-Attention谁先提出的，各文章里写的不一样吗，<a href="https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">Attention Is All You Need</a>中说是<a href="https://arxiv.org/pdf/1606.01933.pdf">Jakob et al.2016</a>年提出的，<a href="https://arxiv.org/pdf/1904.02874.pdf">An Attentive Survey of Attention Models</a>中说是<a href="https://www.aclweb.org/anthology/N16-1174.pdf">Yang et al. 2016</a>，本篇介绍前者。</p><p>Abstract：simpler and more lightweight，比之前的模型参数少一个数量级，而且不依赖与任何词的顺序信息</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>近年来，Natural language inference(NLI)领域中提出的模型都数据量巨大，计算成本非常高。但实际上NLI中，往往是只需要少量的局域信息，然后将这些局域信息汇总起来进行预测即可。</p><h2 id="Approach"><a href="#Approach" class="headerlink" title="Approach"></a>Approach</h2><p><strong>模型结构</strong></p><p>  <img src="../images/Decomposable-Attention.images/1624336820545_src" alt="img" style="zoom: 67%;" /> </p><p><strong>Input representation</strong></p><p>输入是两个句子（不一定等长，$l_a$和$l_b$），句子是由每个词的embedding向量（长度$d$）组成的矩阵，label是多分类的one-hot标签（类别数为$C$）。</p><script type="math/tex; mode=display">\begin{eqnarray*}input\ sentence\ matrix1&:& \textbf{a}=(a_1,...,a_{l_a}) ,\ l_a:length\\         input\ sentence\ matrix2&:& \textbf{b}=(b_1,...,b_{l_b}) ,\ l_b:length     \\word\ embedding\ vector&:&a_i, b_j ∈ R^d\\indicator\ vector&:& \textbf{y}^{(n)}=(y^{(n)}_1,...,y^{(n)}_C),\ C:classes\ number \\training\ data&:&\{ \textbf{a}^{(n)},\textbf{b}^{(n)},\textbf{y}^{(n)} \}^N_{n=1}\\test\ data&:&(\textbf{a},\textbf{b})\end{eqnarray*}</script><p>$\textbf{a}$和$\textbf{b}$可以做一些变换之后再输入模型，标准版模型就输入$\textbf{a}$和$\textbf{b}$了。</p><h3 id="Attend"><a href="#Attend" class="headerlink" title="Attend"></a>Attend</h3><p>这一步是用attention，让两个句子$(\textbf{a},\textbf{b})$相互表示对方，得到$(\boldsymbol\beta,\boldsymbol\alpha)$,$\boldsymbol\beta$是$\boldsymbol b$表示出来的$\boldsymbol a$。</p><p>首先要计算相关性权重</p><script type="math/tex; mode=display">e_{ij}:=F^{'}{(a_i,b_j)}:=F(a_i)^TF(b_j)</script><p>这里有一个计算简化，如果按照$F^{‘}{(a_i,b_j)}$计算，则需要计算$l_a×l_b$次$F^{‘}{(·)}$，但按照后者计算则只用计算$l_a+l_b$次$F{(·)}$。</p><p>然后对weight标准化，并根据标准化的权重加权得到新的词表达：$\beta_i,\alpha_j$。需要注意的是，此处命名有点反直觉，$\beta_i$的计算中，query是$a_i$，Key和Value都是$\textbf{b}$。$\alpha_j$则相反，query是$b_i$，Key和Value都是$\textbf{a}$。</p><script type="math/tex; mode=display">\begin{eqnarray*}\beta_i:=\sum^{l_b}_{j=1}\frac{exp(e_{ij})}{\sum^{l_b}_{k=1}exp(e_{ik})}b_j\\\\\alpha_j:=\sum^{l_a}_{i=1}\frac{exp(e_{ij})}{\sum^{l_a}_{k=1}exp(e_{kj})}a_i\end{eqnarray*}</script><h3 id="Compare"><a href="#Compare" class="headerlink" title="Compare"></a>Compare</h3><p>接下来，分别比较${(a<em>i,\beta_i)}^{l_a}</em>{i=1}$和${(b<em>j,\alpha_j)}^{l_b}</em>{j=1}$中每个pair，也就是看看用$\textbf{b}$表示出来的$a<em>i$，即$\beta_i$和真正的$a_i$有多像，如果很像（反映在$\textbf{v}</em>{1,i}$中），则证明句子$\textbf{b}$中有$a_i$的信息。</p><script type="math/tex; mode=display">\begin{eqnarray*}\textbf{v}_{1,i}:=G([a_i,\beta_i])\ \ \forall i\in [1,...,l_a]\\\\\textbf{v}_{2,j}:=G([b_j,\alpha_j])\ \ \forall j\in [1,...,l_b]\end{eqnarray*}</script><p>其中$[·，·]$表示concatenation，$G$在论文中是全连接。因为这个计算次数是线性的，所以不用像前面一样将$a_i,\beta_i$拆开计算了。</p><h3 id="Aggregate"><a href="#Aggregate" class="headerlink" title="Aggregate"></a>Aggregate</h3><p>将每个词的比较向量聚合成句子的比较向量</p><script type="math/tex; mode=display">\textbf{v}_1 = \sum^{l_a}_{i=1}\textbf{v}_{1,i}\ \ \ ,\ \ \ \textbf{v}_2 = \sum^{l_b}_{j=1}\textbf{v}_{2,j}</script><p>然后concat起来过分类器，得到预测结果</p><script type="math/tex; mode=display">{\widehat{\textbf y}} = H([\textbf v_1,\textbf v_2]),\ \ \ \ {\widehat{\textbf y}} \in R^C</script><h3 id="Intra-Sentence-Attention"><a href="#Intra-Sentence-Attention" class="headerlink" title="Intra-Sentence Attention"></a>Intra-Sentence Attention</h3><p>重点<strong>self-attention</strong>来了，前面的模型里，输入是简单的word embedding。这里提出一种增强输入表达的方法：intra-sentence attention，将句子中每个词之间的关系表示出来。</p><script type="math/tex; mode=display">f_{ij}:=F_{intra}(a_i)^TF_{intra}(a_j),\\\\a^{'}_i=\sum^{l_a}_{j=1}\frac{exp(f_{ij}+d_{i-j})}{exp(f_{ik}+d_{i-k})}a_j\\\\</script><p>其中，$F<em>{intra}$是一个全连接，$f</em>{ij}$就表示$a<em>i$和$a_j$的相似程度。$d</em>{i-j}$是距离敏感度偏置项，作用是不让某个词的权重过小。</p><p><strong>学点巴洛克风格的词</strong></p><p>vanilla version：The “<strong>vanilla</strong> <strong>version</strong>“ is generally the version that has no customisation applied - it is the “regular”,  “ordinary” or “plain old” version. For a lot of consumer based software - this would be the only version. You would not build custom versions for every user.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Self-Attention谁先提出的，各文章里写的不一样吗，&lt;a href=&quot;https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf&quot;&gt;Attention Is A
      
    
    </summary>
    
    
      <category term="深度学习" scheme="http://wangdongdong122.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Attention" scheme="http://wangdongdong122.github.io/tags/Attention/"/>
    
      <category term="Transformer" scheme="http://wangdongdong122.github.io/tags/Transformer/"/>
    
      <category term="机器学习" scheme="http://wangdongdong122.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="每日论文" scheme="http://wangdongdong122.github.io/tags/%E6%AF%8F%E6%97%A5%E8%AE%BA%E6%96%87/"/>
    
      <category term="经典算法" scheme="http://wangdongdong122.github.io/tags/%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95/"/>
    
      <category term="NLP" scheme="http://wangdongdong122.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Hierarchical Attention Networks for Document Classification</title>
    <link href="http://wangdongdong122.github.io/2021/06/21/2021-06-21-Hierarchical-Attention/"/>
    <id>http://wangdongdong122.github.io/2021/06/21/2021-06-21-Hierarchical-Attention/</id>
    <published>2021-06-21T01:26:17.000Z</published>
    <updated>2021-06-23T09:48:53.500Z</updated>
    
    <content type="html"><![CDATA[<p>Self-Attention谁先提出的，各文章里写的不一样吗，<a href="https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">Attention Is All You Need</a>中说是<a href="https://arxiv.org/pdf/1606.01933.pdf">Jakob.2016</a>年提出的，<a href="https://arxiv.org/pdf/1904.02874.pdf">An Attentive Survey of Attention Models</a>中说是<a href="https://www.aclweb.org/anthology/N16-1174.pdf">Yang et al. 2016</a>，本篇介绍后者。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>核心思路：</p><ol><li>分层（hierarchical structure）：先构建“词 → 句子”级的表达，再聚合到文档级，即“句子 → 文档”</li><li>Attention：不同的词和句子包含的信息和重要程度都依赖于上下文，为了将其考虑进来，所以作者用两层的Attention</li></ol><p>作者没有提self-attention，应该是还没意识到这一点的牛逼之处。</p><h2 id="Hierarchical-Attention-Networks"><a href="#Hierarchical-Attention-Networks" class="headerlink" title="Hierarchical Attention Networks"></a>Hierarchical Attention Networks</h2><h3 id="encoder"><a href="#encoder" class="headerlink" title="encoder"></a>encoder</h3><p>encoder采用GRU产生，原理及结构省略</p><h3 id="Hierarchical-Attention"><a href="#Hierarchical-Attention" class="headerlink" title="Hierarchical Attention"></a>Hierarchical Attention</h3><p>数据表达</p><ul><li>sentences $\vec{s_i}$ ,$i=1,2,…L$</li><li>words represents: $w_{it}$, $t ∈ [1, T]$,   $\vec{s_i}$ contains $T$ words</li></ul><h4 id="Word-Encoder"><a href="#Word-Encoder" class="headerlink" title="Word Encoder"></a>Word Encoder</h4><p>先embedding，过双向GRU，将隐层concatenate起来</p><ol><li>word embedding: $W<em>e$, $x</em>{ij}=W<em>ew</em>{ij}$</li><li>forward GRU: $\overset{\rightarrow}{h<em>{it}}=\overset{\rightarrow}{GRU}(x</em>{it}),\ t ∈ [1, T]$</li><li>backward GRU:  $\overset{\leftarrow}{h<em>{it}}=\overset{\leftarrow}{GRU}(x</em>{it}),\ t ∈ [T, 1]$</li><li>concatenate:  $h<em>{it}=[\overset{\rightarrow}{h</em>{it}},\overset{\leftarrow}{h_{it}}]$</li></ol><h4 id="Word-Attention"><a href="#Word-Attention" class="headerlink" title="Word Attention"></a>Word Attention</h4><p>将对句子含义起重要作用的词提取出来，聚合成一个句子向量。先将所有的（$t ∈ [1, T]$）$h<em>{it}$过全连接得到Key: $u</em>{it}$；然后和随机变量的query: $u<em>w$求相似度分布: $\alpha$；最后将最开始的  $h</em>{it}$作为Value，加权得到sentence vector: $s<em>i$。所有信息都是从$h</em>{it}$中得到。</p><script type="math/tex; mode=display">\begin{eqnarray*}u_{it} &=& tanh(W_wh_{it}+b_w) \tag{FC layer}  \\                 \\\alpha_{it} &=& \frac{exp(u^{T}_{it}u_w)}{\sum_{t}{exp(u^{T}_{it}u_w)}}  \tag{measure similarity & normalize}\\\\s_i &=& \sum_{t}{\alpha_{it}h_{it}}\tag{weighted sum}\end{eqnarray*}</script><p>其中$ u_w$(word context vector)是随机初始化，然后在训练过程中学习的，可以当做是一个固定的query，用来表示这个句子中重要的信息。</p><p>维度信息：每个句子只产生一个向量$s<em>i$，其长度和单个词的BiGRU隐层concat之后的向量$h</em>{it}$长度相同（不一定等于词向量$w_{it}$的长度）。</p><h4 id="Sentence-Encoder"><a href="#Sentence-Encoder" class="headerlink" title="Sentence Encoder"></a>Sentence Encoder</h4><p>句子的encoder也和词的类似，先过bidirectional GRU然后concatenate。</p><ol><li>forward GRU：$\overset{\rightarrow}{h<em>{i}}=\overset{\rightarrow}{GRU}(s</em>{i}),\ i ∈ [1, L]$</li><li>backward GRU: $\overset{\leftarrow}{h<em>{i}}=\overset{\leftarrow}{GRU}(s</em>{i}),\ i ∈ [L, 1]$</li><li>concatenate: $h<em>{i}=[\overset{\rightarrow}{h</em>{i}},\overset{\leftarrow}{h_{i}}]$</li></ol><h4 id="Sentence-Attention"><a href="#Sentence-Attention" class="headerlink" title="Sentence Attention"></a>Sentence Attention</h4><p>这部分也和Word Attention部分一样，只是换了个层次</p><script type="math/tex; mode=display">\begin{eqnarray*}u_{i} &=& tanh(W_sh_{i}+b_s)  \tag{FC layer}   \\                  \\ \alpha_{i} &=& \frac{exp(u^{T}_{i}u_s)}{\sum_{i}{exp(u^{T}_{i}u_s)}}   \tag{measure similarity & normalize} \\ \\ v &=& \sum_{i}{\alpha_{i}h_{i}} \tag{weighted sum} \end{eqnarray*}</script><p>这里就将一个文档表示成一个向量$v$， 其长度和单个句子的BiGRU隐层concat之后的向量$h_{i}$长度相同。</p><h3 id="Document-Classification"><a href="#Document-Classification" class="headerlink" title="Document Classification"></a>Document Classification</h3><p>这部分很简单，文档向量$v$过softmax，然后用log loss训练。</p><script type="math/tex; mode=display">\begin{eqnarray*}p &=& softmax(W_cv+b_c)  \tag{softmax}   \\                  \\L &=& -\sum_{d}{log\ p_{dj}} \tag{log loss} \end{eqnarray*}</script><p>其中，$j$是文档$d$的标签，只对正确标签计算loss。</p><h2 id="Results-and-analysis"><a href="#Results-and-analysis" class="headerlink" title="Results and analysis"></a>Results and analysis</h2><p>  Yelp 2013上的两个文档，左边是给出了5星好评的，右边是0星差评的。模型可以捕捉到那些词重要。<img src="../images/Hierarchical-Attention.images/1624323245414_src-1624427147187" alt="img" style="zoom: 50%;" /></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Self-Attention谁先提出的，各文章里写的不一样吗，&lt;a href=&quot;https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf&quot;&gt;Attention Is A
      
    
    </summary>
    
    
      <category term="深度学习" scheme="http://wangdongdong122.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Attention" scheme="http://wangdongdong122.github.io/tags/Attention/"/>
    
      <category term="Transformer" scheme="http://wangdongdong122.github.io/tags/Transformer/"/>
    
      <category term="机器学习" scheme="http://wangdongdong122.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="每日论文" scheme="http://wangdongdong122.github.io/tags/%E6%AF%8F%E6%97%A5%E8%AE%BA%E6%96%87/"/>
    
      <category term="经典算法" scheme="http://wangdongdong122.github.io/tags/%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95/"/>
    
      <category term="NLP" scheme="http://wangdongdong122.github.io/tags/NLP/"/>
    
  </entry>
  
</feed>
