<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>凛冬将至</title>
  
  <subtitle>从简单的例子开始</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://wangdongdong122.github.io/"/>
  <updated>2022-01-16T06:00:56.000Z</updated>
  <id>http://wangdongdong122.github.io/</id>
  
  <author>
    <name>Dongdong Wang</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>线性回归</title>
    <link href="http://wangdongdong122.github.io/2022/01/16/2022-01-16-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    <id>http://wangdongdong122.github.io/2022/01/16/2022-01-16-线性回归/</id>
    <published>2022-01-16T02:16:03.000Z</published>
    <updated>2022-01-16T06:00:56.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文总结线性回归知识。</p><span id="more"></span><h2 id="古典线性回归的基本假定"><a href="#古典线性回归的基本假定" class="headerlink" title="古典线性回归的基本假定"></a>古典线性回归的基本假定</h2><h3 id="线性假定"><a href="#线性假定" class="headerlink" title="线性假定"></a>线性假定</h3><script type="math/tex; mode=display">y_i=\beta_i x_{i,1} + \beta_2x_{i,2}+···+ \beta_kx_{i,k}+</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文总结线性回归知识。&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://wangdongdong122.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://wangdongdong122.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="线性回归" scheme="http://wangdongdong122.github.io/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    
      <category term="算法总结" scheme="http://wangdongdong122.github.io/tags/%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/"/>
    
  </entry>
  
  <entry>
    <title>婚礼出行安排</title>
    <link href="http://wangdongdong122.github.io/2022/01/03/2022-01-03-%E5%A9%9A%E7%A4%BC%E5%87%BA%E8%A1%8C%E5%AE%89%E6%8E%92/"/>
    <id>http://wangdongdong122.github.io/2022/01/03/2022-01-03-婚礼出行安排/</id>
    <published>2022-01-03T12:42:43.000Z</published>
    <updated>2022-01-04T23:19:04.000Z</updated>
    
    <content type="html"><![CDATA[<p>黑盒优化问题介绍。</p><p>天琪家婚礼疫情政策</p><span id="more"></span><p>​    </p><h2 id="出行政策"><a href="#出行政策" class="headerlink" title="出行政策"></a>出行政策</h2><h3 id="抵达北京市提醒"><a href="#抵达北京市提醒" class="headerlink" title="抵达北京市提醒"></a>抵达北京市提醒</h3><p>根据2021年11月17日、11月13日、10月24日北京市人民政府官网消息：</p><p>1、自2021年11月17日零时起，进（返）京人员须持==48小时内核酸检测阴性证明和“北京健康宝”绿码==，==14日内有1例以上（含1例）本土新冠病毒感染者所在县（市、区、旗）旅居史人员严格限制进（返）京==，民航将加强远端登机进京查验。<a href="http://www.beijing.gov.cn/fuwu/bmfw/bmzt/yqzt/jfj/202110/t20211029_2523905.html">链接</a></p><p>2、环京地区通勤人员，在该措施执行后首次进(返)京须持48小时内核酸检测阴性证明，此后每次进(返)京持14日内核酸检测阴性证明即可。</p><p>3、==14日内有陆路边境口岸所在县(市、区、旗)旅居史人员非必须不进(返)京==。<a href="http://www.beijing.gov.cn/ywdt/zwzt/yqfk/jfj/202111/t20211118_2539890.html">链接</a></p><p>4、对其他口岸入境进京人员，须在入境口岸所在地隔离观察满21天。</p><p>更多详细信息，请务必登录北京市人民政府官网，查看《核酸检测阴性证明起始时间、环京地区通勤人员身份如何认定，官方解读》、《11月17日零时起进返京须持48小时内核酸阴性证明+绿码》、《北京对进京人员实施管控》，按照官方通知为准。</p><p>（来源：2021年11月17日、11月13日、10月24日北京市人民政府官网）</p><p><strong>问题</strong>：</p><ol><li><p>“14日内”如何定义？</p><p>出入京时间算起，14天。</p></li><li><p>“14日内有1例以上（含1例）本土新冠病毒感染者所在县（市、区、旗）旅居史人员严格限制进（返）京”，如果是省会城市，是不是只要对应区没有本土病例就行？</p><p>县级市直接看市，地级市指区（未确认）。</p></li><li><p>“14日内有陆路边境口岸所在县(市、区、旗)旅居史人员非必须不进(返)京”，怎么才算必须，回京需要检查什么？</p><p>登记的有7个省66个边境口岸，其中云南省19个县，其中有腾冲市，需要执行暂缓进京政策，就是没有办法进京（由北京疾控中心确认）。师兄说，会直接无法查看健康码。</p><p>附：对7个省（自治区）51个陆路边境口岸所在县（市、区、旗）执行限制进（返）京管控措施，具体是：</p><ul><li>吉林省（1县）：珲春市；</li><li>黑龙江省（10县）：密山市、虎林市、绥芬河市、东宁市、饶河县、同江市、抚远市、萝北县、爱辉区、逊克县；</li><li>内蒙古自治区（9县）：满洲里市、额尔古纳市、新巴尔虎左旗、二连浩特市、东乌珠穆沁旗、乌拉特中旗、额济纳旗、达尔罕茂明安联合旗、阿尔山市；</li><li>新疆自治区（12县）：巴里坤哈萨克自治县、奇台县、青河县、福海县、阿拉山口市、霍尔果斯市、察布查尔锡伯自治县、塔城市、吉木乃县、乌恰县、阿合奇县、塔什库尔干塔吉克自治县；</li><li>西藏自治区（3县）：吉隆县、聂拉木县、普兰县；</li><li>云南省（10县）：江城哈尼族彝族自治县、勐腊县、勐海县、耿马县、瑞丽市、==腾冲市==、文山市、马关县、河口瑶族自治县、金平县；</li><li>广西自治区（6县）：东兴市、凭祥市、宁明县、龙州县、靖西市、那坡县。</li></ul></li></ol><p>​    </p><h3 id="抵达腾冲市提醒"><a href="#抵达腾冲市提醒" class="headerlink" title="抵达腾冲市提醒"></a>抵达腾冲市提醒</h3><p>根据2021年7月6日腾冲疾控动态微信公众号最新消息：</p><p>全市居民非必要不前往瑞丽市等边境县市及国内疫情中、高风险地区，确须前往的，需经属地指挥部批准报备，返腾时提供72小时内核酸检测阴性证明。</p><p>更多详细信息，请务必登录腾冲疾控动态微信公众号，查看《腾冲市应对新冠肺炎疫情工作领导小组指挥部关于进一步强化疫情防控工作的通告》，按照官方通知为准。</p><p>（来源：2021年7月6日腾冲疾控动态微信公众号）</p><p>​    </p><h3 id="腾冲出行提醒"><a href="#腾冲出行提醒" class="headerlink" title="腾冲出行提醒"></a>腾冲出行提醒</h3><p>根据2021年12月24日腾冲机场消息：</p><p>1、从腾冲机场乘机出行需要提供==48小时内核酸检测阴性证明==（电子和纸质都可以）、==检验行程码和健康码双码正常才能出行==。另外：如果是腾冲本地人员还需提供通行证明。</p><p>2、请在出行前务必联系腾冲机场确认当前疫情管控政策。</p><p>​    </p><h3 id="抵达云南省提醒"><a href="#抵达云南省提醒" class="headerlink" title="抵达云南省提醒"></a>抵达云南省提醒</h3><p>一、根据云南省卫生服务热线防控要求：</p><p>1、近14天内有本土疫情的城市旅居史人员来（返）云南需要持48小时内核酸检测阴性报告。</p><p>2、请在出行前务必联系云南省卫生服务热线确认当前核酸检测报告的时限要求以及落地后疫情管控政策。</p><p>二、根据2021年7月30日云南疾控微信公众号消息：</p><p>1、近14天内有国内高风险地区旅居史及云南健康码为红码的人员，请立即向居住地社区或防疫部门报告，并自觉配合做好14天（时间以离开当地开始计算）集中隔离医学观察，期间开展4次（第1、4、7、14天）核酸检测。</p><p>2、近14天内有国内中风险地区旅居史及云南健康码为黄码的人员，请立即向居住地社区或防疫部门报告，自觉进行7天居家健康监测，期间开展3次（每次至少间隔24小时）核酸检测。</p><p>更多详细信息，请务必联系云南省卫生服务热线或登录云南疾控微信公众号，查看《关注云南省疾控中心发布疫情防控紧急提醒》，按照官方通知为准。</p><p>（来源：云南省卫生服务热线，2021年7月30日云南疾控微信公众号）</p><p>​    </p><h3 id="昆明出行提醒"><a href="#昆明出行提醒" class="headerlink" title="昆明出行提醒"></a>昆明出行提醒</h3><p>根据2021年10月25日昆明市疾病预防控制中心微信公众号消息：</p><p>所有离昆人员，在离昆前==48小时内，主动完成一次核酸检测==。</p><p>更多详细信息，请务必登录昆明市疾病预防控制中心微信公众号，查看《昆明市疾控中心温馨提示》，按照官方通知为准。</p><p>（来源：2021年10月25日昆明市疾病预防控制中心微信公众号）</p><p>​    </p><h3 id="抵达安庆市提醒"><a href="#抵达安庆市提醒" class="headerlink" title="抵达安庆市提醒"></a>抵达安庆市提醒</h3><p>根据2021年12月9日、7月29日安庆市人民政府发布微信公众号消息：</p><p>1、省外来(返)宜人员需提供==48小时内核酸检测阴性证明==，并主动向所在社区(村)、单位或入住酒店报告，抵宜后至少开展1次核酸检测。</p><p>2、14天内有高风险地区或已实行封控封闭管理区域旅居史的，请配合做好14天的集中隔离医学观察措施；==有中风险地区旅居史的，请配合做好14天的居家隔离医学观察==，无居家隔离条件的实行集中隔离医学观察(从离开中高风险地区算起)。</p><p>更多详细信息，请务必登录安庆市人民政府发布微信公众号，查看《安庆疾控中心发布疫情防控提醒 非必要不出省 科学防护不松懈》、《紧急扩散安庆发布最新通告：这些人需14天集中隔离医学观察》，按照官方通知为准。</p><p>（来源：2021年12月9日、7月29日安庆市人民政府发布微信公众号）</p><h3 id="抵达黄山市提醒"><a href="#抵达黄山市提醒" class="headerlink" title="抵达黄山市提醒"></a>抵达黄山市提醒</h3><p>根据2021年12月28日黄山发布微信公众号消息：</p><p>1、对近14天以来有中高风险地区所在地级市其他县旅居史人员，3日内有本土疫情发生、但未调整风险等级的地级市其他县（市、区）旅居史人员,非必要不来黄,确需来（返）黄的，查验登机前48小时内核酸检测阴性证明，在抵黄后立即开展1次核酸检测，并进行14天自我健康监测。</p><p>2、对自安徽省外来（返）黄人员：建议携带登机==前48小时内核酸检测阴性证明，或抵黄后就地开展一次核酸检测==。</p><p>3、对近14天以来有中高风险地区旅居史人员，限制来（返）黄；已经抵黄的，实施14天集中隔离医学观察。</p><p>4、对近14天以来有中高风险地区所在县（市、区）及3日内有本土疫情发生、但未调整风险等级的县（市、区）旅居史人员，限制来（返）黄。已经抵黄的，按照“三天两检”（间隔24小时以上）落实核酸检测，在第二次核酸检测阴性结果未出前，落实居家健康监测；解除居家健康监测后，进行11天自我健康监测。</p><p>5、对入境来（返）黄人员继续实施“入境地14天集中隔离+7天目的地集中隔离+7天居家健康监测+4周健康随访”管理措施（对14天内有中国澳门旅居史的来（返）黄人员查验来黄登机前48小时内核酸检测阴性证明）。</p><p>更多详细信息，请务必登录黄山发布微信公众号，查看《市疫防办重要提醒》，按照官方通知为准。</p><p>（来源：2021年12月28日黄山发布微信公众号）</p><h3 id="抵达上海市提醒"><a href="#抵达上海市提醒" class="headerlink" title="抵达上海市提醒"></a>抵达上海市提醒</h3><p>根据2021年10月29日上海发布微信公众号消息：</p><p>1、对所有来自或途经国内疫情高风险地区的来沪返沪人员，一律实施14天集中隔离健康观察，实行4次新冠病毒核酸检测。</p><p>2、对所有来自或途经国内疫情中风险地区的来沪返沪人员，一律实施14天严格的社区健康管理，实行2次新冠病毒核酸检测。</p><p>更多详细信息，请务必登录上海发布微信公众号，查看《上海进一步加强对国内疫情中高风险地区来沪返沪人员健康管理》，按照官方通知为准。</p><p>（来源：2021年10月29日上海发布微信公众号）</p><p>​    </p><h3 id="抵达安徽省提醒"><a href="#抵达安徽省提醒" class="headerlink" title="抵达安徽省提醒"></a>抵达安徽省提醒</h3><p>根据2021年11月12日安徽疾控微信公众号消息：</p><p>1、近14天内发生本土疫情的城市来（返）皖人员，须提供48小时内核酸检测阴性证明。</p><p>更多详细信息，请务必登录安徽疾控微信公众号，查看《新冠肺炎疫情防控重要健康提示》按照官方通知为准。</p><p>（来源：2021年11月12日安徽疾控微信公众号）</p><h2 id="各地风险水平"><a href="#各地风险水平" class="headerlink" title="各地风险水平"></a>各地风险水平</h2><h3 id="昆明：有风险"><a href="#昆明：有风险" class="headerlink" title="昆明：有风险"></a><a href="https://voice.baidu.com/act/newpneumonia/newpneumonia/?from=osari_aladin_banner&amp;city=%E4%BA%91%E5%8D%97-%E6%98%86%E6%98%8E">昆明</a>：有风险</h3><img src="/2022/01/03/2022-01-03-%E5%A9%9A%E7%A4%BC%E5%87%BA%E8%A1%8C%E5%AE%89%E6%8E%92/image-20220103153045552.png" class="" title="image-20220103153045552"><p>==2021-12-27本土新增1人==。</p><p>​    </p><h3 id="大理：安全"><a href="#大理：安全" class="headerlink" title="大理：安全"></a><a href="https://voice.baidu.com/act/newpneumonia/newpneumonia/?from=osari_aladin_banner&amp;city=%E4%BA%91%E5%8D%97-%E5%A4%A7%E7%90%86%E5%B7%9E">大理</a>：安全</h3><img src="/2022/01/03/2022-01-03-%E5%A9%9A%E7%A4%BC%E5%87%BA%E8%A1%8C%E5%AE%89%E6%8E%92/image-20220103153300291.png" class="" title="image-20220103153300291">：​    ### [保山](https://voice.baidu.com/act/newpneumonia/newpneumonia/?from=osari_aladin_banner&city=%E4%BA%91%E5%8D%97-%E4%BF%9D%E5%B1%B1)：陆路边境口岸<img src="/2022/01/03/2022-01-03-%E5%A9%9A%E7%A4%BC%E5%87%BA%E8%A1%8C%E5%AE%89%E6%8E%92/image-20220103153454324.png" class="" title="image-20220103153454324"><p><strong>云南省地图</strong></p><img src="/2022/01/03/2022-01-03-%E5%A9%9A%E7%A4%BC%E5%87%BA%E8%A1%8C%E5%AE%89%E6%8E%92/2020020907000930.jpg" class="" title="img"><img src="/2022/01/03/2022-01-03-%E5%A9%9A%E7%A4%BC%E5%87%BA%E8%A1%8C%E5%AE%89%E6%8E%92/p1.ssl.cdn.btime.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=jpeg" class="" title="img"><p>保山，属于边界市，==腾冲属于陆路边境口岸所在县==。</p><p>​    </p><h3 id="北京：有风险"><a href="#北京：有风险" class="headerlink" title="北京：有风险"></a><a href="https://voice.baidu.com/act/newpneumonia/newpneumonia/?from=osari_aladin_banner&amp;city=%E5%8C%97%E4%BA%AC-%E5%8C%97%E4%BA%AC">北京</a>：有风险</h3><img src="/2022/01/03/2022-01-03-%E5%A9%9A%E7%A4%BC%E5%87%BA%E8%A1%8C%E5%AE%89%E6%8E%92/image-20220103153758345.png" class="" title="image-20220103153758345"><h3 id="安庆：安全"><a href="#安庆：安全" class="headerlink" title="安庆：安全"></a><a href="https://voice.baidu.com/act/newpneumonia/newpneumonia/?from=osari_aladin_banner&amp;city=%E5%AE%89%E5%BE%BD-%E5%AE%89%E5%BA%86">安庆</a>：安全</h3><img src="/2022/01/03/2022-01-03-%E5%A9%9A%E7%A4%BC%E5%87%BA%E8%A1%8C%E5%AE%89%E6%8E%92/image-20220103162400758.png" class="" title="image-20220103162400758"><h2 id="各方案行程"><a href="#各方案行程" class="headerlink" title="各方案行程"></a>各方案行程</h2><p>2022-01-13（周四）出发，2022-01-16（周日）婚礼，2022-01-17（周一）返京，请3天假。</p><h3 id="昆明中转"><a href="#昆明中转" class="headerlink" title="昆明中转"></a>昆明中转</h3><h4 id="去程"><a href="#去程" class="headerlink" title="去程"></a>去程</h4><h4 id="返程"><a href="#返程" class="headerlink" title="返程"></a>返程</h4><p>​    </p><h2 id="情况预案"><a href="#情况预案" class="headerlink" title="情况预案"></a>情况预案</h2><p>Tips：</p><ul><li>出入北京都需要向社区报备，需要记下来航班和座位号</li></ul><p>​    </p><h3 id="情况1-怀宁本土确诊"><a href="#情况1-怀宁本土确诊" class="headerlink" title="情况1:怀宁本土确诊"></a>情况1:怀宁本土确诊</h3><p>怀宁出现本土确诊，则需要==去无确诊地区隔离14天后，可进京==。</p><p>​    </p><h3 id="相关咨询电话"><a href="#相关咨询电话" class="headerlink" title="相关咨询电话"></a>相关咨询电话</h3><p>北京疾控中心咨询热线：010-12320</p><p>北京市民服务热线：12345</p><p>北京大兴国际机场机场服务/投诉受理热线：010-96158。（北京大兴机场没有单独的防疫电话）</p><p>延静寺社区报备电话：65928548。微信：延静寺社区</p><p>​    </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;黑盒优化问题介绍。&lt;/p&gt;
&lt;p&gt;天琪家婚礼疫情政策&lt;/p&gt;
    
    </summary>
    
    
      <category term="日常事务" scheme="http://wangdongdong122.github.io/tags/%E6%97%A5%E5%B8%B8%E4%BA%8B%E5%8A%A1/"/>
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://wangdongdong122.github.io/2022/01/02/2022-01-02-Pr%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/"/>
    <id>http://wangdongdong122.github.io/2022/01/02/2022-01-02-Pr使用教程/</id>
    <published>2022-01-02T15:11:12.000Z</published>
    <updated>2022-01-02T15:11:12.000Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://wangdongdong122.github.io/2021/12/18/2021-12-18-%E5%87%B8%E4%BC%98%E5%8C%96part1/"/>
    <id>http://wangdongdong122.github.io/2021/12/18/2021-12-18-凸优化part1/</id>
    <published>2021-12-18T09:54:36.000Z</published>
    <updated>2021-12-18T09:54:36.000Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>黑盒优化问题</title>
    <link href="http://wangdongdong122.github.io/2021/11/24/2021-11-24-%E9%BB%91%E7%9B%92%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98/"/>
    <id>http://wangdongdong122.github.io/2021/11/24/2021-11-24-黑盒优化问题/</id>
    <published>2021-11-24T07:30:43.000Z</published>
    <updated>2022-01-02T15:11:34.000Z</updated>
    
    <content type="html"><![CDATA[<p>黑盒优化问题介绍。</p><span id="more"></span><p>​    </p><h2 id="黑盒优化是什么"><a href="#黑盒优化是什么" class="headerlink" title="黑盒优化是什么"></a>黑盒优化是什么</h2><p>黑盒函数指计算函数的解析表达式和工作方式未知，只能输入数据得到输出值。黑盒优化问题指优化目标表达形式未知的优化问题。在强化学习的model-free类的方法中，可以将环境看做一个黑盒，Actor通过与环境互动，来解决黑和优化问题。</p><p>​    </p><h2 id="有哪些方法"><a href="#有哪些方法" class="headerlink" title="有哪些方法"></a>有哪些方法</h2><h3 id="网格搜索法"><a href="#网格搜索法" class="headerlink" title="网格搜索法"></a>网格搜索法</h3><p>网格搜索法（grid search）对每个需要优化的参数取一个有限值集，然后将每个参数的值集做<strong>笛卡尔积</strong>，构成网格，对每个笛卡尔积中每组参数取值评估优化目标取值，搜索其中的最优解。</p><p>N个参数，每个参数取M个有限值集，则需要评估$M^N$组参数的效果。因此需要评估的次数随参数的数量（N）指数上升，网格太密（M大）也会需要巨大的计算量。</p><p>​    </p><h3 id="随机搜索法"><a href="#随机搜索法" class="headerlink" title="随机搜索法"></a>随机搜索法</h3><p>随机搜索(random search)法在参数可能的值域中<strong>随机取值评估效果</strong>，搜索其中的最优解。此处的随机取值，指的是整个参数空间中随机取，即<strong>在高维空间中随机取点</strong>，这一点不同于网格搜索中各个参数上取值集再组合成一组参数。</p><p><strong>网格搜索法与随机搜索法对比</strong></p><img src="/2021/11/24/2021-11-24-%E9%BB%91%E7%9B%92%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98/raw.githubusercontent.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=jpeg" class="" title="网格搜索与随机搜索"><p>当我们对N个参数总共进行B次效果评价时，网格搜索只能对每个参数只能评价其在$B^{\frac 1N}$个位置上的效果，而随机搜索则能对每个参数都评估其取个B不同值的效果。</p><p>​    </p><h3 id="其他优化算法"><a href="#其他优化算法" class="headerlink" title="其他优化算法"></a>其他优化算法</h3><p>其他方法还有：<strong>贝叶斯优化，遗传算法，进化算法</strong>等。</p><p>​    </p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="https://zhuanlan.zhihu.com/p/66312442">知乎：黑盒优化简介</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;黑盒优化问题介绍。&lt;/p&gt;
    
    </summary>
    
    
      <category term="黑盒优化" scheme="http://wangdongdong122.github.io/tags/%E9%BB%91%E7%9B%92%E4%BC%98%E5%8C%96/"/>
    
      <category term="优化算法" scheme="http://wangdongdong122.github.io/tags/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>PU-Learning介绍</title>
    <link href="http://wangdongdong122.github.io/2021/11/21/2021-11-21-PU-Learning/"/>
    <id>http://wangdongdong122.github.io/2021/11/21/2021-11-21-PU-Learning/</id>
    <published>2021-11-21T09:59:45.000Z</published>
    <updated>2021-12-05T07:40:05.855Z</updated>
    
    <content type="html"><![CDATA[<p>Positive-unlabeled learning是半监督学习的一个研究方向，指在只有正类和大量无标注数据的情况下，学习模型，本文主要内容的参考是20年的一篇<a href="https://link.springer.com/content/pdf/10.1007/s10994-020-05877-5.pdf">survey</a>。</p><span id="more"></span><h2 id="PU-Learning介绍"><a href="#PU-Learning介绍" class="headerlink" title="PU-Learning介绍"></a>PU-Learning介绍</h2><p>PU-Learning指训练数据是由正样本和无标签样本构成的学习算法。PU-Learning的概念在2000年之后才开始被出现，可以视作半监督算法的一个方向。</p><p>​    </p><h3 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h3><p>对于小概率事件，数据往往只记录其发生的情况（信息量大），并且无法全部记载，因此产生PU场景。在一些存在数据选择偏差的场景中也可以视作PU-Learning的问题。</p><p><strong>医药领域</strong>：医学的记录一般只记载查出来有病的，没检查的病或者查了没得的病是不会记录的</p><p><strong>商业应用</strong></p><ul><li>推荐系统中，点击用户是正样本，但不点击未必是负样本，可能是因为展示位置偏等因素导致用户未点击</li><li>反欺诈中，欺诈用户可以通过一些手段打标为坏人，却难以得到无偏的好人</li><li>知识图谱中，训练数据一般是有连接的节点对</li><li>拒绝推断中，被拒绝用户可以看做未打标用户</li></ul><p><strong>安全与信号处理</strong>：卫星图片的分类，只要少量的数据是被标注过类别的，其他大量的图片标签未知</p><p>​    </p><h3 id="关键问题"><a href="#关键问题" class="headerlink" title="关键问题"></a>关键问题</h3><p>PU-Learning中研究的关键问题，可以总结为以下几点：</p><ol><li>怎么将从PU data中学习的问题抽象为数学形式？</li><li>为了设计学习算法，我们应该对PU data做哪些假设？</li><li>怎么从PU data中预估类别的先验概率($P(y=1)$)，估出来为什么有用？</li><li>怎么从PU data中学习模型？</li><li>在PU-Learning场景中，怎么评估模型的效果？</li><li>现实应用场景中什么时候会产生，以及为什么会产生PU data？</li><li>PU-Learning和其他机器学习领域有什么关系？</li></ol><p>​    </p><h2 id="准备知识"><a href="#准备知识" class="headerlink" title="准备知识"></a>准备知识</h2><h3 id="PU-learning"><a href="#PU-learning" class="headerlink" title="PU  learning"></a>PU  learning</h3><p>本文所使用的符号</p><img src="/2021/11/21/2021-11-21-PU-Learning/pu-nt.png" class="" title="pu-nt"><p>​    </p><h3 id="打标机制"><a href="#打标机制" class="headerlink" title="打标机制"></a>打标机制</h3><p>样本是通过某个概率的机制决定是否打上标签的，这个机制可以通过倾向性得分 $e(x)$ 来表示。在PU-Learning中可以表示为：</p><script type="math/tex; mode=display">e(x)=P(s=1|y=1,x)</script><p>这里的倾向性得分的定义和其他场景中有一点差异，由于PU场景中只会给正样本打标，所以也只看正样本的倾向性。</p><p>​    </p><h3 id="class-prior和label-frequency的关系"><a href="#class-prior和label-frequency的关系" class="headerlink" title="class prior和label frequency的关系"></a>class prior和label frequency的关系</h3><p>class prior $\alpha$ 和 label frequency $c$之间有紧密的关系，给定一个PU数据集，则其中一个已知就可推出另一个：</p><script type="math/tex; mode=display">\begin{eqnarray*}c &=& P(s=1|y=1)\\\\ &=& \frac {P(s=1,y=1)}{P(y=1)}\\\\ &=& \frac {P(s=1)}{\alpha}\end{eqnarray*}</script><p>​    </p><h2 id="相关假设"><a href="#相关假设" class="headerlink" title="相关假设"></a>相关假设</h2><p>PU-Learning一般需要对打标机制(labeling mechanism)和类别分布(class distributions in the data)中至少一个做出假设。</p><p>​    </p><h3 id="打标机制-1"><a href="#打标机制-1" class="headerlink" title="打标机制"></a>打标机制</h3><h4 id="Selected-Completely-At-Random-SCAR"><a href="#Selected-Completely-At-Random-SCAR" class="headerlink" title="Selected Completely At Random (SCAR)"></a>Selected Completely At Random (SCAR)</h4><p>SCAR定义：<strong>打标样本是从正样本中完全随机选择的</strong>，和样本本身的属性无关。此时任意一个正样本被打标的概率都相等，即propensity score满足：</p><script type="math/tex; mode=display">e(x)= P(s = 1|x, y = 1)= P(s = 1|y = 1)= c</script><p>这个假设下，可以通过倾向性得分来得到正确的预测概率，任意一个样本被打标的概率就等于其为正例的概率乘标签频率：</p><script type="math/tex; mode=display">P(s=1|x)=P(s = 1|x, y = 1)·P(y=1|x) = c·P(y=1|x)</script><p>​    </p><h4 id="Selected-At-Random-SAR"><a href="#Selected-At-Random-SAR" class="headerlink" title="Selected At Random (SAR)"></a>Selected At Random (SAR)</h4><p>SAR定义：<strong>选择正样本打标的概率依赖于其属性值</strong>。此时，打标的正样本相对于所有正样本是有偏的，但这是<strong>更为现实</strong>的情况，比如病人是否去医院检查病情和其经济情况和病情相关，因此在所有的患病人群（正例）中，是否被医生确诊和病人自身的属性相关。此时倾向性得分：</p><script type="math/tex; mode=display">e(x)=P(s=1|x,y=1)</script><p>​    </p><h4 id="Probabilistic-gap-PU-PGPU"><a href="#Probabilistic-gap-PU-PGPU" class="headerlink" title="Probabilistic gap PU (PGPU)"></a>Probabilistic gap PU (PGPU)</h4><p>需要先定义什么是Probabilistic gap：$𝛥P(x)=P(y=1|x)−P(y=0|x)$，即一个样本属于正类和负类的概率之差。</p><p>PGPU定义：<strong>Probabilistic gap越大的正样本被打标的概率越大</strong>。即：</p><script type="math/tex; mode=display">e(x)=f(𝛥P(x))=f(P(y=1|x)−P(y=0|x)),\ \frac{d}{dt}f(t)>0</script><p>定义The  observed  probabilistic  gap为$𝛥\widetilde{P}(x)=P(s=1|x)−P(s=0|x)$，则有以下性质：</p><script type="math/tex; mode=display">\begin{eqnarray*}𝛥\widetilde{P}(x)&=&e(x)(𝛥P(x)+1)-1\\\\𝛥\widetilde{P}(x)&\leq& 𝛥P(x)\tag{上式去掉$e$}\\\\𝛥\widetilde{P}(x_1)= 𝛥\widetilde{P}(x_2) &\Leftrightarrow& 𝛥P(x_1)= 𝛥P(x_2) \\𝛥\widetilde{P}(x_1)< 𝛥\widetilde{P}(x_2) &\Leftrightarrow& 𝛥P(x_1)< 𝛥P(x_2)\end{eqnarray*}</script><p>​    </p><h3 id="数据假设"><a href="#数据假设" class="headerlink" title="数据假设"></a>数据假设</h3><h4 id="Negativity"><a href="#Negativity" class="headerlink" title="Negativity"></a>Negativity</h4><p>定义：简单粗暴地假设没打标的都属于负样本。虽然假设很难成立，但实际做的时候经常是这么做的。</p><p>​    </p><h4 id="Separability"><a href="#Separability" class="headerlink" title="Separability"></a>Separability</h4><p>定义：正负样本是分离的。这个假设下，理论上是存在分类器将正负样本完全分开的。</p><p>​    </p><h4 id="Smoothness"><a href="#Smoothness" class="headerlink" title="Smoothness"></a>Smoothness</h4><p>定义：越相似的样本，属于同一类别的概率越高。在这个假设下，可以将离正例远的样本当做可靠的负样本。</p><p>​    </p><h3 id="Identifiable-class-prior"><a href="#Identifiable-class-prior" class="headerlink" title="Identifiable class prior"></a>Identifiable class prior</h3><p>class prior$P(y=1)$一般都有用，但由于SCAR假设一般都不满足，所以无法求出class prior，为了其可求，需要做额外的假设，常见的假设如下（从强到弱）：</p><ol><li>Separable Classes/Non-overlapping distributions：正负样本的分布是没有重叠的</li><li>Positive subdomain/anchor set：</li><li>Positive function/separability</li><li>Irreducibility</li></ol><p>​    </p><h2 id="效果度量"><a href="#效果度量" class="headerlink" title="效果度量"></a>效果度量</h2><h3 id="Metrics-for-PU-data"><a href="#Metrics-for-PU-data" class="headerlink" title="Metrics for PU data"></a>Metrics for PU data</h3><p>最常用的评估指标是基于$F_1$score（$F_1(\hat {\boldsymbol y}) = \frac{2pr}{p+r}$）的，其中的召回率$r$在<strong>SCAR假设</strong>下可以算出来，但准确率$p$却算不出来。PU<br>Learning中可以用下面这个指标</p><script type="math/tex; mode=display">\begin{eqnarray*}\frac{p·r}{P(\boldsymbol y=1)} &=& \frac{p·r^2}{r·P(\boldsymbol y=1)}\\\\&=& \frac{P(\boldsymbol y=1|\hat {\boldsymbol y}=1)·r^2}{P(\hat {\boldsymbol y}=1|\boldsymbol y=1)·P(\boldsymbol y=1)}\\\\&=& \frac{P(\boldsymbol y=1|\hat {\boldsymbol y}=1)·r^2}{P(\hat {\boldsymbol y}=1,\boldsymbol y=1)}\\\\&=& \frac{r^2}{P(\hat {\boldsymbol y}=1)}\end{eqnarray*}</script><p>这个指标也是召回和准确率越高越好。其他的还有假设检验等方法也可以度量效果，就不再展开了。</p><p>​    </p><h2 id="常用算法"><a href="#常用算法" class="headerlink" title="常用算法"></a>常用算法</h2><h3 id="Two-step-techniques"><a href="#Two-step-techniques" class="headerlink" title="Two-step techniques"></a>Two-step techniques</h3><p>一个古老的方法，这个方法基于的假设有：<strong>separability &amp; smoothness</strong>，具体分为两步：</p><p>step1：先从未标注的样本找出可靠的负样本</p><p>step2：用传统的分类器做分类</p><p>​    </p><p><strong>step1的典型方法</strong>有：</p><p>spy：先拿出一部分打标的小样本作为间谍；拿其余打标样本作为正样本，未打标样本作为负样本用来训练一个朴素贝叶斯分类器；对间谍样本和无标样本打分，将<strong>无标样本中评分低于所有间谍样本</strong>的作为可信赖的负样本。这个方法需要足够多的打标样本，要不间谍样本就少了。</p><p>1-DNF：先用一些学习算法找到在正例中出现频率 高于无标数据的特征，作为强正向的特征；没有足够强正向特征的就作为可信赖的负样本。这个方法对“强正向特征”的要求条件太低了，可能会找出很多，这就会导致找到的可信负样本太少。为解决这个问题，又有研究人员提出通过增加出现频率的阈值作为条件，筛除一批强正向特征（1-DNFII）。</p><p><strong>step2的典型方法</strong>有：这里其实任何一个有监督的二分类方法都可以，典型的有SVM、LR、Naive Bayes (NB)等。</p><p>​    </p><h3 id="biased-learning"><a href="#biased-learning" class="headerlink" title="biased learning"></a>biased learning</h3><p>这个方法将<strong>无标数据看做是负样本和类别标签噪声</strong>，因此会将无标签样本当做负样本，同时对噪声问题带来的bias做一些纠正（对错误分类增加更高的惩罚，使loss更小）。这个噪声被当做一个常数，因此<strong>SCAR假设</strong>成立。这个方法在分类、聚类、矩阵补全等场景都可以用，这里只介绍分类场景的方法。</p><p><strong>biased SVM</strong>：在标准SVM的基础上，给正负样本的误判加不同的惩罚。比如无标签样本越像是负样本就给越大的权重，实际做法可以将其<strong>距离任意一个正样本的最小距离</strong>作为其误判的惩罚权重。</p><p><strong>Bagging SVM</strong>：训练很多个biased SVM，每个biased SVM用所有的正例和<strong>一部分无标样本</strong>当做负样本（占比K为重要超参）训练，然后将所有的biased SVM求均值。 这个方法可推广为PU-Bagging，现在应用比较广。 </p><p>​    </p><h3 id="class-prior-incorporation"><a href="#class-prior-incorporation" class="headerlink" title="class prior incorporation"></a>class prior incorporation</h3><p>在SCAR假设下，利用class prior来辅助分类。有三种类型的方法：① 后处理：先直接将无标数据当做负样本，然后利用class prior做调整；② 预处理：先用class prior调整数据集，再训练模型；③ 算法调整：结合class prior，调整算法。</p><p>​    </p><h4 id="后处理方法"><a href="#后处理方法" class="headerlink" title="后处理方法"></a>后处理方法</h4><p>在SCAR假设中给出过公式，样本被打标的概率与其为正例的概率成正比，系数就是label frequency $c$。</p><script type="math/tex; mode=display">P(s=1|x)=c·P(y=1|x)</script><p>于是可以得到</p><script type="math/tex; mode=display">P(y=1|x) = \frac{p(s=1|x)}{c}</script><p>根据这个公式，可以给出算法(<a href="https://www.researchgate.net/profile/Charles-Elkan/publication/221654469_Learning_classifiers_from_only_positive_and_unlabeled_data/links/02bfe513e19296c331000000/Learning-classifiers-from-only-positive-and-unlabeled-data.pdf">Elkan and Noto 2008</a>)：</p><p>step①：训练一个分类器，预测样本被标记的概率，$P(s=1|x)$</p><p>step②：使用①中训练的分类器预测正例被标记的概率，$P(s=1|y=1)$，得到均值$c$</p><p>step③：对于任意一个样本k，使用①中分类器预测其打标概率，$P(s=1|k)$</p><p>step④：计算样本k是正的概率$P(s=1|k)/c$</p><p>或者，也可以通过<strong>修改分类的阈值</strong>来矫正bias，如果正常情况下阈值为$\tau$，则在PU数据中，阈值应该改为：$\tau^{pu}=c\tau$。</p><p>​    </p><h4 id="预处理方法"><a href="#预处理方法" class="headerlink" title="预处理方法"></a>预处理方法</h4><p>这个方法通过创造一个新的数据集来解决问题。刚才讲了可以通过改变判断类别的阈值来矫正bias，也可以通过<strong>改变样本的权重</strong>做到类似的效果。这里详细介绍一下<strong>Empirical-Risk-Minimization Based Methods</strong>。首先回顾一下两个背景知识：</p><p><strong>二分类样本的数据分布</strong>可以表示为：</p><script type="math/tex; mode=display">\boldsymbol x \sim p(x) \sim \alpha p_+(x)+(1-\alpha)p_-(x)\tag{1}</script><p>其中，class prior $\alpha=P(Y=1)$，$p,p_+,p_-$分别表示真实分布、正样本分布、负样本分布，（可以理解为$p(y=1)p(x|y=1)+p(y=0)p(x|y=0)$）。</p><p>PU data中<strong>打标数据的分布</strong>可以表示为：</p><script type="math/tex; mode=display">\begin{eqnarray*}p_l(x) &=& p(x|s=1)\\\\&=& p(x|s=1,y=1)\tag{PU definition}\\\\&=& \frac{p(s=1|x,y=1)}{p(s=1|y=1)} p(x|y=1)\tag{Bayes' rule}\\\\&=&\frac{e(x)}{c}p_+(x)\tag{2}\end{eqnarray*}</script><p>在经验风险最小方法中，我们可以将loss函数表示为<strong>正负样本各自的风险期望加权求和</strong>，到样本级别，其实就是每个样本的loss和：</p><script type="math/tex; mode=display">R(g) = \alpha E_{p+}   \left [    L^+(g(x))  \right ]+(1- \alpha) E_{p_-}  \left [    L^-(g(x))  \right ]</script><p>在PU场景中，我们可以根据前面的推导，将这个loss做一个转化：</p><script type="math/tex; mode=display">\begin{eqnarray*}R(g) &=& \alpha E_{p+}  \left [  L^+(g(x)) \right ]+(1- \alpha) E_{p_-} \left [  L^-(g(x))\right ]\\\\ &=& \alpha E_{p+}  \left [  L^+(g(x)) \right ]+ E_p \left [  L^-(g(x))\right ]- \alpha E_{p_+} \left [  L^-(g(x))\right ] \tag{公式(1)}\\\\ &=& \alpha E_{p+}  \left [  L^+(g(x)) -   L^-(g(x)) \right ]+ E_p \left [  L^-(g(x))\right ]\tag{合并}\\\\ &=& \alpha E_{l}  \left [\frac{c}{e(x)} \left( L^+(g(x)) -   L^-(g(x)) \right ) \right ]+ E_p \left [  L^-(g(x))\right ]\tag{公式(2)}\end{eqnarray*}</script><p>按这个新的计算方法，只需要：打标的数据分布、倾向性得分就可以计算了。</p><p>​    </p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="https://zhuanlan.zhihu.com/p/82556263">知乎：学习笔记3-PU learning</a></li><li><a href="https://www.researchgate.net/profile/Kristen-Jaskie/publication/337503578_Positive_And_Unlabeled_Learning_Algorithms_And_Applications_A_Survey/links/609b0124458515d31513c2e9/Positive-And-Unlabeled-Learning-Algorithms-And-Applications-A-Survey.pdf">Jaskie K, Spanias A. Positive and unlabeled learning algorithms and applications: A survey[C]//2019 10th International Conference on Information, Intelligence, Systems and Applications (IISA). IEEE, 2019: 1-8.</a></li><li><a href="https://link.springer.com/content/pdf/10.1007/s10994-020-05877-5.pdf">Bekker J, Davis J. Learning from positive and unlabeled data: A survey[J]. Machine Learning, 2020, 109(4): 719-760.</a></li><li><a href="https://proceedings.neurips.cc/paper/2016/file/be3159ad04564bfb90db9e32851ebf9c-Paper.pdf">Niu, Gang, et al. “Theoretical Comparisons of Positive-Unlabeled Learning against Positive-Negative Learning.” NIPS’16 Proceedings of the 30th International Conference on Neural Information Processing Systems, vol. 29, 2016, pp. 1207–1215.</a></li><li><a href="https://epubs.siam.org/doi/pdf/10.1137/1.9781611972795.23">Li X L, Yu P S, Liu B, et al. Positive unlabeled learning for data stream classification[C]//Proceedings of the 2009 SIAM international conference on data mining. Society for Industrial and Applied Mathematics, 2009: 259-270.</a></li><li><a href="http://www.scielo.org.mx/pdf/cys/v18n3/v18n3a5.pdf">Li H, Chen Z, Liu B, et al. Spotting fake reviews via collective positive-unlabeled learning[C]//2014 IEEE international conference on data mining. IEEE, 2014: 899-904.</a></li><li><a href="http://proceedings.mlr.press/v37/hsiehb15.pdf">Hsieh C J, Natarajan N, Dhillon I. PU learning for matrix completion[C]//International conference on machine learning. PMLR, 2015: 2445-2453.</a></li><li><a href="https://riunet.upv.es/bitstream/handle/10251/64736/IPM%20Autor.pdf?sequence=4">Fusilier D H, Montes-y-Gómez M, Rosso P, et al. Detecting positive and negative deceptive opinions using PU-learning[J]. Information processing &amp; management, 2015, 51(4): 433-443.</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Positive-unlabeled learning是半监督学习的一个研究方向，指在只有正类和大量无标注数据的情况下，学习模型，本文主要内容的参考是20年的一篇&lt;a href=&quot;https://link.springer.com/content/pdf/10.1007/s10994-020-05877-5.pdf&quot;&gt;survey&lt;/a&gt;。&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://wangdongdong122.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="半监督学习" scheme="http://wangdongdong122.github.io/tags/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="PU-Learning" scheme="http://wangdongdong122.github.io/tags/PU-Learning/"/>
    
  </entry>
  
  <entry>
    <title>常用概率统计定理</title>
    <link href="http://wangdongdong122.github.io/2021/11/21/2021-11-21-%E5%B8%B8%E7%94%A8%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E5%AE%9A%E7%90%86/"/>
    <id>http://wangdongdong122.github.io/2021/11/21/2021-11-21-常用概率统计定理/</id>
    <published>2021-11-21T02:00:31.000Z</published>
    <updated>2021-11-21T08:10:46.065Z</updated>
    
    <content type="html"><![CDATA[<p>总结一些常用的数理统计定理。</p><span id="more"></span><h2 id="The-rule-of-Lazy-Statistician"><a href="#The-rule-of-Lazy-Statistician" class="headerlink" title="The rule of Lazy Statistician"></a>The rule of Lazy Statistician</h2><h3 id="意义"><a href="#意义" class="headerlink" title="意义"></a>意义</h3><p>在我们求$E(Y)$时，不必算出$Y$的分布率或密度函数，而只要利用$X$的分布律或密度函数以及$Y$和$X$之间的关系即可。</p><p>​    </p><h3 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h3><p>设$Y$是随机变量$X$的函数，$Y=f(X)$($f$是连续函数) ，那么</p><p>(1) $X$是离散型随机变量，它的分布律为 $P(X=x_k)=p_k, k=1,2,…$。若$\sum_{k=1}^{\infty}f(x_k)p(x_k)$绝对收敛，则有：</p><script type="math/tex; mode=display">E(Y)=E[f(X)]=\sum_{k=1}^{\infty} f(x_k)p(x_k)</script><p>(2) $X$是连续型随机变量，它的概率密度为 $p(x)$,若$\int_{-\infty}^{\infty} f(x) p(x) dx$绝对收敛，则有:</p><script type="math/tex; mode=display">E(Y)=E[f(X)]=\int_{-\infty}^{\infty} f(x)p(x)dx</script><p>​    </p><h3 id="推广"><a href="#推广" class="headerlink" title="推广"></a>推广</h3><p>该定理可推广至两个及以上随机变量的函数。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;总结一些常用的数理统计定理。&lt;/p&gt;
    
    </summary>
    
      <category term="数理统计" scheme="http://wangdongdong122.github.io/categories/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/"/>
    
    
      <category term="基础知识" scheme="http://wangdongdong122.github.io/tags/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
  </entry>
  
  <entry>
    <title>常用不等式定理</title>
    <link href="http://wangdongdong122.github.io/2021/11/21/2021-11-19-%E5%B8%B8%E7%94%A8%E4%B8%8D%E7%AD%89%E5%BC%8F%E5%AE%9A%E7%90%86/"/>
    <id>http://wangdongdong122.github.io/2021/11/21/2021-11-19-常用不等式定理/</id>
    <published>2021-11-21T02:00:31.000Z</published>
    <updated>2021-12-05T08:03:42.999Z</updated>
    
    <content type="html"><![CDATA[<p>总结一些常用的不等式定理。</p><span id="more"></span><h2 id="jensen不等式"><a href="#jensen不等式" class="headerlink" title="jensen不等式"></a>jensen不等式</h2><p>设 $f$ 是定义域为实数的函数，如果对于所有的实数 $x$,$f^{‘’}(x)\geq 0$，那么 $f$ 是凸函数。当 $x$ 是向量时，如果其hessian矩阵H是半正定的$(H\geq 0)$，那么f是凸函数。如果$f^{‘’}(x)\geq 0$或者$(H\geq 0)$那么称f是严格凸函数。</p><p>如果f是凸函数，$X$是随机变量，那么</p><script type="math/tex; mode=display">E[f(X)] \geq f(EX)</script><p>特别地，如果$f$是严格凸函数，那么$E[f(X)]= f(EX)$当且仅当$P(X=E(X))=1$,也就是说$X$是常量。</p><p>当$f$是(严格) 凹函数,当且仅当$-f$是(严格) 凸函数。Jensen不等式应用于凹函数时，不等号方向反向，也就是$E[f(X)]  \leq f(EX)$</p><p>​    </p><h2 id="四种平均数"><a href="#四种平均数" class="headerlink" title="四种平均数"></a>四种平均数</h2><p>四种平均数之间的关系</p><script type="math/tex; mode=display">H_n ≤ G_n ≤ A_n ≤ Q_n</script><p>​    </p><h3 id="调和平均数"><a href="#调和平均数" class="headerlink" title="调和平均数"></a>调和平均数</h3><script type="math/tex; mode=display">H_n=\frac{n}{  \frac{1}{a_1}+ \frac{1}{a_2}+ ...+ \frac{1}{a_n}}</script><p>​    </p><h3 id="几何平均数"><a href="#几何平均数" class="headerlink" title="几何平均数"></a>几何平均数</h3><script type="math/tex; mode=display">G_n=(a_1a_2...a_n)^{\frac 1n}</script><p>​    </p><h3 id="算术平均数"><a href="#算术平均数" class="headerlink" title="算术平均数"></a>算术平均数</h3><script type="math/tex; mode=display">A_n=\frac{a_1+a_2+...+a_n}n</script><p>​    </p><h3 id="平方平均数"><a href="#平方平均数" class="headerlink" title="平方平均数"></a>平方平均数</h3><script type="math/tex; mode=display">Q_n=\sqrt{\frac{a_1^2+a_2^2+...+a_n^2}{n}}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;总结一些常用的不等式定理。&lt;/p&gt;
    
    </summary>
    
      <category term="基础知识" scheme="http://wangdongdong122.github.io/categories/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
    
      <category term="基础知识" scheme="http://wangdongdong122.github.io/tags/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
      <category term="常见定理" scheme="http://wangdongdong122.github.io/tags/%E5%B8%B8%E8%A7%81%E5%AE%9A%E7%90%86/"/>
    
      <category term="不等式定理" scheme="http://wangdongdong122.github.io/tags/%E4%B8%8D%E7%AD%89%E5%BC%8F%E5%AE%9A%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>EM算法</title>
    <link href="http://wangdongdong122.github.io/2021/11/18/2021-11-18-EM%E0%B8%AB%E0%B9%83%E0%B8%97%E0%B8%88/"/>
    <id>http://wangdongdong122.github.io/2021/11/18/2021-11-18-EMหใทจ/</id>
    <published>2021-11-18T15:09:16.000Z</published>
    <updated>2021-11-21T09:58:42.000Z</updated>
    
    <content type="html"><![CDATA[<p>EM(Expectation-Maximization)算法常用于估计隐变量。</p><span id="more"></span><h2 id="基本思想"><a href="#基本思想" class="headerlink" title="基本思想"></a>基本思想</h2><p>当模型中含有隐变量时，直接求解所有参数是比较困难的。比如给定的训练样本是$\{ x^{(1)},…x^{(m)}\}$，样例间独立，我们想找到每个样例隐含的类别 $z$，能使得$p(x,z)$最大。$p(x,z)$的最大似然估计如下：</p><script type="math/tex; mode=display">\begin{eqnarray*}l(\theta) &=& \sum_{i=1}^{m}log\ p(x|\theta) \tag{原始的最大似然}\\\\&=& \sum_{i=1}^{m}log\ \sum_z p(x,z|\theta) \tag{引入隐变量$z$}\end{eqnarray*}</script><p>最大似然中有一个是未知的，所以很难直接求，因此EM算法采用的方法是，先根据$x$和$\theta$的值，推断出$z$（E步）；然后在根据$x$和$z$的值，用最大似然求$\theta$（M步）。即</p><ol><li><p>假设参数 $\theta$ 已知，根据训练数据推断出最优隐变量 $z$ 的值（E步）</p></li><li><p>假设隐变量 $z$ 已知，对参数 $\theta$ 做最大似然估计（M步）</p></li></ol><p>​    </p><h2 id="以优化下界的思路理解"><a href="#以优化下界的思路理解" class="headerlink" title="以优化下界的思路理解"></a>以优化下界的思路理解</h2><p>EM是一种<strong>解决存在隐含变量优化</strong>问题的有效方法。既然不能直接最大化 $l(\theta)$，我们可以不断地建立 $l(\theta)$的下界(E步) ，然后优化下界(M步) 。这句话比较抽象，看下面的。</p><p>对于每一个样例 $i$，让 $Q_i$表示该样例隐含<strong>变量z的某种分布</strong>（每个$i$对应一个分布）， $Q_i$满足的条件是 $\sum_z Q_i(z)=1,Q_i(z)\geq 0$。(如果z是连续性的，那么 $Q_i$是概率密度函数，需要将求和符号换做积分符号) 。比如要将班上学生聚类，假设隐藏变量$z$是身高，那么就是连续的高斯分布。如果按照隐藏变量是男女，那么就是伯努利分布了。</p><p>可以由前面阐述的内容得到下面的公式：</p><script type="math/tex; mode=display">\begin{eqnarray*}\sum_i log\ p(x^{(i)}|\theta)   &=& \sum_ilog \sum_{z^{(i)}} p(x^{(i)}, z^{(i)} |\theta) \tag{1，引入隐变量}\\\\&=& \sum_ilog \sum_{z^{(i)}} Q_i (z^{(i)})   \frac {p(x^{(i)}, z^{(i)} |\theta)} {Q_i (z^{(i)})} \tag{2，引入分布Q}\\\\&=& \sum_i log \ E_{Z_i \sim Q_i}\    \left [     \frac {p(x^{(i)}, z^{(i)} |\theta)} {Q_i (z^{(i)})}   \right ] \tag{3，期望形式}  \\\\&\geq&  \sum_i E_{Z_i \sim Q_i}\    \left [     log(\frac {p(x^{(i)}, z^{(i)} |\theta)} {Q_i (z^{(i)})})   \right ]\tag{4，jensen不等式}\\\\&=&      \sum_i \sum_{z^{(i)}} Q_i (z^{(i)}) log  \frac {p(x^{(i)}, z^{(i)} |\theta)} {Q_i (z^{(i)})} \end{eqnarray*}</script><p>这个过程可以看作是对 $l(\theta)$ 求了下界。对于 $Q_i$ 的选择，有多种可能，那种更好的？假设已经 $\theta$ 给定，那么的 $l(\theta)$ 值就决定于 $Q_i (z^{(i)})$ 和 $p(x^{(i)}, z^{(i)})$ 了。我们可以通过调整这两个概率使下界不断上升，以逼近 $l(\theta)$ 的真实值（这里是$l$是似然函数，所以越大越好）。那么什么时候算是调整好了呢？当不等式变成等式时，说明我们调整后的概率能够等价于 $l(\theta)$ 了。按照这个思路，我们要找到等式成立的条件。根据Jensen不等式，要想让等式成立，需要让随机变量变成常数值，这里得到：</p><script type="math/tex; mode=display">\frac {p(x^{(i)}, z^{(i)} |\theta)} {Q_i (z^{(i)})} =C</script><p> C为常数，不依赖于 $z^{(i)}$ 。对此式子做进一步推导，我们知道 $\sum_z Q_i (z^{(i)})=1$ ，那么也就有</p><script type="math/tex; mode=display">\sum_z p (x^{(i)},z^{(i)}|\theta)=\sum_z C \cdot Q_i (z^{(i)})=C</script><p> 那么有下式：</p><script type="math/tex; mode=display">\begin{eqnarray*}{Q_i (z^{(i)})} &=& \frac                     {p(x^{(i)}, z^{(i)} |\theta)}                     {C}\\\\&=& \frac                     {p(x^{(i)}, z^{(i)} |\theta)}                     {\sum_z p(x^{(i)}, z^{(i)} |\theta)}\\\\&=& \frac     {p(x^{(i)}, z^{(i)} |\theta)}     {p(x^{(i)} |\theta)}\\\\&=& p(z^{(i)} |x^{(i)}, \theta)\end{eqnarray*}</script><p>至此，我们推出了在固定其他参数 $\theta$ 后，$Q_i (z^{(i)})$ 的计算公式就是后验概率，解决了$Q_i (z^{(i)})$ 如何选择的问题。这一步就是E步，建立的 $l(\theta)$ 下界。</p><p>接下来的M步，就是在给定$Q_i (z^{(i)})$ 后，调整 $\theta$，去极大化 $l(\theta)$ 的下界(在固定 $Q_i (z^{(i)})$ 后，下界还可以调整的更大) 。</p><p>那么一般的EM算法的步骤如下：    </p><hr><p>循环重复直到收敛:</p><p>​      (E步) 对于每一个$i$，计算</p><script type="math/tex; mode=display">Q_i (z^{(i)}) := p(z^{(i)} |x^{(i)}, \theta)</script><p>​      (M步) 计算</p><script type="math/tex; mode=display">\theta:= arg\ \max_{\theta} \sum_i \sum_{z^{(i)}} Q_i (z^{(i)}) log  \frac {p(x^{(i)}, z^{(i)} |\theta)} {Q_i (z^{(i)})}</script><hr><p>​    </p><h2 id="收敛证明"><a href="#收敛证明" class="headerlink" title="收敛证明"></a>收敛证明</h2><p>那么怎么确保EM收敛？假定 $\theta ^{(t)}$和 $\theta ^{(t+1)}$是EM第t次和t+1次迭代后的结果。如果我们证明了 $l(\theta ^{(t)}) \le l(\theta ^{(t+1)})  $，也就是说极大似然估计单调增加，那么最终我们会到达最大似然估计的最大值。下面来证明，选定后，我们得到E步</p><script type="math/tex; mode=display">Q_i (z^{(i)}) := p(z^{(i)} |x^{(i)}, \theta)</script><p> 这一步保证了在给定 $\theta ^{(t)}$时，Jensen不等式中的等式成立，也就是</p><script type="math/tex; mode=display">l(\theta^{(t)})=\sum_i \sum_{z^{(i)}} Q_i (z^{(i)}) log  \frac {p(x^{(i)}, z^{(i)} |\theta^{(t)})} {Q_i (z^{(i)})}</script><p>然后进行M步，固定 $Q_i (z^{(i)})$，并将 $\theta ^{(t)}$ 视作变量，对上面的 $l(\theta^{(t)})$ 求导后，得到 $\theta ^{(t+1)}$，这样经过一些推导会有以下式子成立：</p><script type="math/tex; mode=display">\begin{eqnarray*}l(\theta^{(t+1)}) &=& \sum_i log\sum_{z^{(i)}} Q_i (z^{(i)})                       \frac {p(x^{(i)}, z^{(i)} |\theta^{(t+1)})} {Q_i (z^{(i)})}                      \tag{4,似然函数定义}\\\\&\ge& \sum_i \sum_{z^{(i)}} Q_i (z^{(i)}) log      \frac {p(x^{(i)}, z^{(i)} |\theta^{(t+1)})} {Q_i (z^{(i)})}      \tag{5,下界}\\\\&\ge& \sum_i \sum_{z^{(i)}} Q_i (z^{(i)}) log      \frac {p(x^{(i)}, z^{(i)} |\theta^{(t)})} {Q_i (z^{(i)})}         \tag{6,t+1步就是最大化此式}\\\\&=& l(\theta^{(t)})    \tag{7,E步保证}\end{eqnarray*}</script><p>解释第(5) 步，得到 $\theta ^{(t+1)}$ 时，只是最大化，也就是 $l(\theta^{(t+1)})$的下界，而没有使等式成立，等式成立只有是在固定 $\theta$，并按E步得到 $Q_i$时才能成立。</p><p>况且根据我们前面得到的下式，对于所有的 $Q_i$和都 $\theta$成立</p><script type="math/tex; mode=display">l(\theta) \ge \sum_i \sum_{z^{(i)}} Q_i (z^{(i)}) log  \frac {p(x^{(i)}, z^{(i)} |\theta)} {Q_i (z^{(i)})}</script><p>第(6) 步利用了M步的定义，M步就是将 $\theta ^{(t)}$  调整到 $\theta ^{(t+1)}$ ，使得下界最大化。因此(6) 成立，(7) 是之前的等式结果。这样就证明了 $l(\theta)$会单调增加。</p><p>再次解释一下(5) 、(6) 、(7) 。首先(5) 对所有的参数都满足，而其等式成立条件只是在固定 $\theta$，并调整好Q时成立，而第(5) 步只是固定Q，调整 $\theta$，不能保证等式一定成立。(5) 到(6) 就是M步的定义，(6) 到(7) 是前面E步所保证等式成立条件。也就是说E步会将下界拉到与 $l(\theta)$ 一个特定值(这里是 $\theta ^{(t)}$ ) 一样的高度，而此时发现下界仍然可以上升，因此经过M步后，下界又被拉升，但达不到与 $l(\theta)$ 另外一个特定值一样的高度，之后E步又将下界拉到与这个特定值一样的高度，重复下去，直到最大值。</p><p>如果我们定义</p><script type="math/tex; mode=display">J(Q,\theta)=\sum_i \sum_{z^{(i)}} Q_i (z^{(i)}) log  \frac {p(x^{(i)}, z^{(i)} |\theta)} {Q_i (z^{(i)})}</script><p>从前面的推导可知 $l(\theta)\ge J(Q,\theta)$，EM可以看作是J的坐标上升法，E步固定 $\theta$，优化 $Q$，M步固定 $Q$优化$\theta$。 </p><p>​    </p><p><strong>参考内容</strong></p><ol><li><a href="https://www.cnblogs.com/jerrylead/archive/2011/04/06/2006936.html">JerryLead-EM算法</a></li><li>《机器学习》（西瓜书），P162</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;EM(Expectation-Maximization)算法常用于估计隐变量。&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://wangdongdong122.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="基础知识" scheme="http://wangdongdong122.github.io/tags/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
      <category term="EM" scheme="http://wangdongdong122.github.io/tags/EM/"/>
    
  </entry>
  
  <entry>
    <title>多目标学习</title>
    <link href="http://wangdongdong122.github.io/2021/11/16/2021-11-15-%E5%A4%9A%E7%9B%AE%E6%A0%87%E5%AD%A6%E4%B9%A0/"/>
    <id>http://wangdongdong122.github.io/2021/11/16/2021-11-15-多目标学习/</id>
    <published>2021-11-16T01:06:25.000Z</published>
    <updated>2021-11-30T03:16:07.843Z</updated>
    
    <content type="html"><![CDATA[<p>多目标学习在搜广推领域很火，家家都在用。</p><span id="more"></span><h2 id="ESMM"><a href="#ESMM" class="headerlink" title="ESMM"></a>ESMM</h2><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>用户行为链路：<strong>曝光 → 点击 → 转化</strong></p><p>各个概率定义如下：</p><script type="math/tex; mode=display">\begin{eqnarray*}&&pCTR = p(click|impression)\\\\&&pCVR = p(conversion|impression,click)\\\\&&pCTCVR = p(click,conversion|impression)\end{eqnarray*}</script><p>直接用点击用户构建CVR模型的问题有两点</p><ol><li>sample selection bias (SSB)：CVR模型在点击样本上训练，却需要再整个impression样本上应用。</li><li>data sparsity (DS)：点击样本比较少。</li></ol><p>​    </p><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p>模型结构如下，有2个子网络，完成3个任务，左右两边的网络<strong>共享Embedding层</strong>。</p><img src="/2021/11/16/2021-11-15-%E5%A4%9A%E7%9B%AE%E6%A0%87%E5%AD%A6%E4%B9%A0/Architecture-overview-of-ESMM.png" class="" title="Architecture-overview-of-ESMM"><p>其多个任务之间存在相互关系，假设用$\boldsymbol x$表示feature(本文只讨论曝光样本，所以给定x即指曝光样本的x)，$y$表示点击，$z$表示转化，那么根据$pCTCVR = pCTR · pCVR$，可以得到：</p><script type="math/tex; mode=display">\underbrace{p(y=1,z=1|\boldsymbol x)} _ {pCTCVR}\ =\ \underbrace{p(y=1|\boldsymbol x)} _ {pCTR}\  ×\  \underbrace{p(z=1|y=1,\boldsymbol x)} _ {pCVR}</script><p>因此可以通过右边子网络的两个结果计算出想要的$pCVR$，这样做的好处是，右边的子网络是在全量样本集上训练的，因此可以解决数据有偏的问题。</p><script type="math/tex; mode=display">\underbrace{p(z=1|y=1,\boldsymbol x)} _ {pCVR}\ =\ \frac{\overbrace{p(y=1,z=1|\boldsymbol x)} ^ {pCTCVR}}{\underbrace{p(y=1|\boldsymbol x)} _ {pCTR}}</script><p>模型的loss由CTR和CTCVR两个任务的loss构成，而不包含CVR的loss。</p><script type="math/tex; mode=display">L(\theta_{cvr},\theta_{ctr})=\sum^N_{i=1}l(y_i,f(\boldsymbol x_i; \theta_{ctr}))+\sum^N_{i=1} l(y_i\&z_i,f(\boldsymbol x_i;\theta_{ctr})×f(\boldsymbol x_i;\theta_{cvr}))</script><p>​    </p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ol><li>ESMM是否能解决<strong>样本有偏</strong>的问题？<ul><li>是否有解决需要根据这个样本有偏的定义来讨论。文中说由于CVR模型是用在整体曝光样本上，但训练时只用了点击的样本。这实际上是说模型正在预测的是：如果一个用户是点击用户，则其转换的概率是多少（$p(z=1|y=1,\boldsymbol x)$）？而模型应用时，直接用在曝光样本上，所以其实际上想要预测的是一个曝光样本有多大概率转化（$p(z=1|\boldsymbol x)$）。也就是，实际应用的时候就是需要CTCVR，而ESMM所预测出来的CVR还是CVR，此时ESMM没有解决样本有偏问题。</li><li>另一种情况是，所谓应用在全量样本上，指的是如果让某个用户点击了，其转化的概率有多大$p(z=1|do(y=1),\boldsymbol x)$。从其论文公式就知道其求解的还是$p(z=1|y=1,\boldsymbol x)$，此时，ESMM同样没有解决这个问题，因此所以我认为<strong>ESMM没有解决这个问题</strong>。</li></ul></li><li><strong>参数共享的作用</strong>是什么？什么情况下，ESMM相对于直接预测单任务的效果有提升？<ul><li>共享参数的作用类似于迁移学习，<strong>一个任务学习到的参数有助于与其相似的任务</strong>。因此在更大样本下学习到的pCTR的参数，很可能有助于pCVR的任务，业务上可以理解为用户点击的概率越高，其转化的概率可能也越高。</li><li>如前所述，多任务学习应该是在<strong>子任务之间有相关性</strong>时，有效果提升。</li></ul></li></ol><p>​    </p><h2 id="MMOE"><a href="#MMOE" class="headerlink" title="MMOE"></a>MMOE</h2><h3 id="背景-1"><a href="#背景-1" class="headerlink" title="背景"></a>背景</h3><p>Multi-gate Mixture-of-Experts（MMOE）由Google在18年发布于KDD，应用于Google的推荐系统。</p><p>以前的多任务学习存在两个问题：</p><ol><li>任务之间的相关性大小会影响多任务学习的效果，手动根据任务之间的关系来设置模型参数费时费力。</li></ol><p>​    </p><h3 id="方法-1"><a href="#方法-1" class="headerlink" title="方法"></a>方法</h3><p>多任务学习主要的框架如下：</p><img src="/2021/11/16/2021-11-15-%E5%A4%9A%E7%9B%AE%E6%A0%87%E5%AD%A6%E4%B9%A0/MMOE.png" class="" title="MMOE"><p><strong>Shared-bottom Multi-task Model</strong>：最常见的多任务学习模型，若共有$K$个任务，每个任务共享底层的表示层（Shared-bottom），表示为 $f$；有$K$个塔层，表示为 $h^{k},k=1,2,…,K$，分别对应$K$个任务。则本方法可以表示为：</p><script type="math/tex; mode=display">y_k = h^k(f(\boldsymbol x))</script><p>​    </p><p><strong>The Original Mixture-of-Experts (MoE) Model</strong>：MoE模型（91年Robert A Jacobs等人提出）可以表示为：</p><script type="math/tex; mode=display">y=\sum^n_{i=1} g(x)_i f_i(x)</script><p>其中$g(x)$是softmax函数，称为<strong>门网络</strong>，$g(x)_i$表示的是其第$i$个输出，对于第$i$个专家网络的权重，$\sum^n_{i=1}g(x)_i=1$；$f_i(x)$表示的是第一个<strong>专家网络</strong>的输出。</p><p>其作用的机制和self-attention很像，但这里面并没有key，直接根据query（输入特征x）算出的权重，而是直接根据任务的需要训练出权重。</p><p>​    </p><p><strong>Multi-gate Mixture-of-Experts（MMoE）</strong>：由本文提出，用于自动捕捉任务之间的差异，而不需要手动设置参数。MMoE中给每个子任务单独设置一个门网络$g^k(x)$（对应第k个任务，最后一层为softmax），用于决定每个专家网络$f_i(x)$的权重，加权后得到每个任务的表示层结果$f^k(x)$，然后过各个任务的塔层$h^k(·)$得到预测结果$y_k$，模型可以表示为：</p><script type="math/tex; mode=display">\begin{eqnarray*}y_k &=&  h^k(f^k(x)),\\\\where\ f^k(x) &=& \sum^n_{i=1} g^k(x)_i f_i(x)\end{eqnarray*}</script><p>这样每个任务都可以在给定输入$x$的条件下选择所需的expert网络的组合。</p><p>​    </p><h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><ol><li><p>融合漏斗链路类的多任务</p><p>MMoE可以和ESMM一样，把一个长流程链路上的多节点的任务之间计算关系放在模型中。</p></li></ol><ol><li><p>和transformer一样弄成多层</p><p>本模型的结构和self-attention类似，所以也可以和transformer一样弄成多层的结构，强化表达能力。</p></li></ol><p>​    </p><p>​    </p><h2 id="参考内容"><a href="#参考内容" class="headerlink" title="参考内容"></a>参考内容</h2><p><a href="https://zhuanlan.zhihu.com/p/78762586">知乎：Multi-task多任务模型在推荐算法中应用总结</a></p><p><a href="https://zhuanlan.zhihu.com/p/145288000">知乎：多任务学习之MMOE模型</a></p><p><a href="https://arxiv.org/pdf/1804.07931.pdf">Ma, Xiao, et al. “Entire Space Multi-Task Model: An Effective Approach for Estimating Post-Click Conversion Rate.” The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval, 2018, pp. 1137–1140.</a></p><p><a href="https://dl.acm.org/doi/pdf/10.1145/3219819.3220007">Ma J ,  Zhao Z ,  Yi X , et al. Modeling Task Relationships in  Multi-task Learning with Multi-gate Mixture-of-Experts. ACM, 2018.</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;多目标学习在搜广推领域很火，家家都在用。&lt;/p&gt;
    
    </summary>
    
    
      <category term="学习笔记" scheme="http://wangdongdong122.github.io/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
      <category term="多目标学习" scheme="http://wangdongdong122.github.io/tags/%E5%A4%9A%E7%9B%AE%E6%A0%87%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>concordance index</title>
    <link href="http://wangdongdong122.github.io/2021/11/08/2021-11-08-concordance-Index/"/>
    <id>http://wangdongdong122.github.io/2021/11/08/2021-11-08-concordance-Index/</id>
    <published>2021-11-08T11:55:40.000Z</published>
    <updated>2021-11-15T06:39:43.156Z</updated>
    
    <content type="html"><![CDATA[<p>排序性指标concordance index介绍</p><span id="more"></span><p>​    </p><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>C-index，英文名全称concordance index，中文里有人翻译成一致性指数，最早是由范德堡大学（Vanderbilt  University）生物统计教教授Frank E Harrell Jr  1996年提出，主要用于计算生存分析中的COX模型预测值与真实之间的区分度（discrimination），和大家熟悉的AUC其实是差不多的。</p><p>​    </p><h2 id="含义"><a href="#含义" class="headerlink" title="含义"></a>含义</h2><p>用来评估两个变量之间排序性的一致性，即$X_1&gt;X_2$时，$Y_1&gt;Y_2$的概率。</p><p>​    </p><h2 id="计算"><a href="#计算" class="headerlink" title="计算"></a>计算</h2><p><strong>暴力求解</strong>（以生存分析为例）</p><ol><li>所有样本互相配对，共有N*(N-1)/2对,其中N为样本数。</li><li>去除配对中两个病人都没有达到事件终点（比如死亡），或者其中的一个病人A的生存时间短于另一个病人B，然而病人A还没有到达事件终点（死亡）~ps：这种配对无法判断出谁先死的。此时剩下的配对数记为：M。</li><li>计算剩下的配对中,预测结果和实际相一致的配对数记为K，即（两个病人如果生存时间较长的一位其预测生存时间长于另一位,或预测的生存概率高的一位的生存时间长于另一位,则称之为预测结果与实际结果相符，称之为一致）。</li><li>计算C-index=K/M。</li></ol><p>​    </p><p><strong>Bootstrap方法</strong></p><ol><li>采用重抽样技术从原始样本中抽取一定数量的样本,此过程允许重复抽样。</li><li>根据抽出的样本计算给定的统计量T。</li><li>重复上述N次(一般大于1000),得到N个统计量T。</li><li>计算上述N个统计量T的样木方差,得到统计量的方差。</li></ol><p>​    </p><h2 id="性质"><a href="#性质" class="headerlink" title="性质"></a>性质</h2><p><strong>取值范围</strong>：和AUC的取值范围一样，理论取值范围在[0,1]之间，实际应用时一般在[0.5,1]。0.5代表完全随机，1表示排序性完全一致。</p><p><strong>和AUC关系</strong>：当$Y \in \{0,1\}$时，和AUC等价。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;排序性指标concordance index介绍&lt;/p&gt;
    
    </summary>
    
    
      <category term="评估指标" scheme="http://wangdongdong122.github.io/tags/%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87/"/>
    
      <category term="排序指标" scheme="http://wangdongdong122.github.io/tags/%E6%8E%92%E5%BA%8F%E6%8C%87%E6%A0%87/"/>
    
      <category term="concordance index" scheme="http://wangdongdong122.github.io/tags/concordance-index/"/>
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://wangdongdong122.github.io/2021/10/10/2021-10-10-%E8%8B%B1%E8%AF%AD%E9%9F%B3%E6%A0%87/"/>
    <id>http://wangdongdong122.github.io/2021/10/10/2021-10-10-英语音标/</id>
    <published>2021-10-10T06:16:29.418Z</published>
    <updated>2021-10-19T13:49:56.312Z</updated>
    
    <content type="html"><![CDATA[<h2 id="The-Story-of-Phonetic-symbol"><a href="#The-Story-of-Phonetic-symbol" class="headerlink" title="The Story of Phonetic symbol"></a>The Story of Phonetic symbol</h2><h3 id="K-K-音标"><a href="#K-K-音标" class="headerlink" title="K.K.音标"></a>K.K.音标</h3><p>《美式英语发音辞典》（A Pronouncing Dictionary of American English）所使用的音标。1944年首次出版，由于两位作者John Samuel Kenyon及Thomas A. Knott的姓皆以K为开头，所以此辞典俗称为<strong>K.K.</strong>（Kenyon and Knott）</p><h2 id="元⾳"><a href="#元⾳" class="headerlink" title="元⾳"></a>元⾳</h2><h3 id="Front-Vowels-前元音"><a href="#Front-Vowels-前元音" class="headerlink" title="Front Vowels 前元音"></a>Front Vowels 前元音</h3><ol><li><p>[i:] 舌抵下齿，双唇扁平作微笑状，发出从[ɪ]到[ j ]的过程。 是字母ea、ee、ey、ie、或ei在单词中的发音，此音是长元音，一定注意把音发足，发出过程感。</p></li><li><p>[ɪ] 舌抵下齿，双唇扁平分开，牙床近于全舌，发短促之“一”音。 是字母i 或y在单词中的发音，发此音要短促而轻快。也标为[i]</p></li><li><p>[æ] 双唇扁平，舌前微升，舌尖抵住下龈，牙床开，软鄂升起，唇自然开放。是字母a在闭音节或重读闭音节中的发音。</p></li><li><p>[e] 舌近硬腭，舌尖顶下齿，牙床半开半合，作微笑状。 是字母e或ea在单词中的发音。</p></li></ol><h4 id="Practice"><a href="#Practice" class="headerlink" title="Practice"></a>Practice</h4><div class="table-container"><table><thead><tr><th>[i:] [ɪ]→[j]</th><th>[ɪ]</th><th>[æ]</th><th>[e]</th></tr></thead><tbody><tr><td>eat the meat [iːt ðə miːt]</td><td>him [hɪm]</td><td>a mad man [ˈmædmən]</td><td>get ready [ɡet ˈredi]</td></tr><tr><td>the Chinese people [ˌtʃaɪˈniːz ˈpiːpl]</td><td>a small fish [smɔːl  fɪʃ]</td><td>hand in hand [hænd ɪn hænd]</td><td>best friend [best frend]</td></tr><tr><td>keep the seat[kiːp ðə siːt]</td><td>fit as a fiddle [fɪt æz ə ˈfɪdl]</td><td>a jazz band [dʒæz bænd]</td><td>wet with sweat [wet wɪð swet]</td></tr><tr><td>a piece of cheese[piːs ɒv tʃiːz]</td><td>make a wish[meɪk ə wɪʃ]</td><td>a black bag [blæk bæɡ]</td><td>ten men [ten men]</td></tr><tr><td>three trees [θriː triːz]</td><td>a bit chilly [ə bɪt ˈtʃɪli]</td><td>A plastic bag [ˈplæstɪk bæɡ]</td><td>best seller [best ‘selə(r)]</td></tr><tr><td>Peter’s secret [ˈpiːtə(r)s ˈsiːkrət]</td><td>sit down [ˈsɪt daʊn]</td><td>a happy marriage [ˈhæpi ˈmærɪdʒ]</td><td>very well [ˈveri wel]</td></tr><tr><td>a friend in need [ə frend ɪn niːd]</td><td>bit by bit [bɪt baɪ bɪt]</td><td></td><td>Teddy Bear [ˈtedi beə(r)]</td></tr></tbody></table></div><p>​    </p><h3 id="Central-Vowels中元音"><a href="#Central-Vowels中元音" class="headerlink" title="Central Vowels中元音"></a>Central Vowels中元音</h3>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;The-Story-of-Phonetic-symbol&quot;&gt;&lt;a href=&quot;#The-Story-of-Phonetic-symbol&quot; class=&quot;headerlink&quot; title=&quot;The Story of Phonetic symbol&quot;&gt;&lt;/a&gt;Th
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>重要采样</title>
    <link href="http://wangdongdong122.github.io/2021/09/18/2021-09-18-%E9%87%8D%E8%A6%81%E9%87%87%E6%A0%B7/"/>
    <id>http://wangdongdong122.github.io/2021/09/18/2021-09-18-重要采样/</id>
    <published>2021-09-18T01:30:35.000Z</published>
    <updated>2021-12-05T08:01:06.248Z</updated>
    
    <content type="html"><![CDATA[<p>重要采样和样本选择偏差和因果推断中的倾向性得分加权、强化学习off-policy等内容高度相关，是一个基本工具。</p><span id="more"></span><h2 id="重要采样"><a href="#重要采样" class="headerlink" title="重要采样"></a>重要采样</h2><p>一般情况下，我们先得到$x$符合$p(x)$分布时，$f(x)$的期望$E_{x\sim p}[f(x)]$时，只要从$p(x)$中采样得到样本$x$，然后代入函数中得到$f(x_i)$，求其均值即可，当样本量趋向于无穷大时，均值会趋近于期望值。</p><script type="math/tex; mode=display">E_{x\sim p}[f(x)]\approx \frac{1}{N}\sum_{i=1}^N f(x_i),\ \ where\ x_i\ is\ sampled\ from\ p(x)</script><p>但当我们只有从另一个分布$q(x)$中采样的$x_i$时，可以通过以下转换来计算$E_{x\sim p}[f(x)]$：</p><script type="math/tex; mode=display">\begin{eqnarray*}E_{x\sim p}[f(x)] &=& \int f(x)p(x)dx\\\\&=& \int f(x)\frac{p(x)}{q(x)}q(x)dx\\\\&=& E_{x\sim q}[f(x)\frac{p(x)}{q(x)}]\end{eqnarray*}</script><p>上式中，$\frac{p(x)}{q(x)}$就是<strong>Importance weight</strong></p><p>​    </p><h2 id="应用的条件"><a href="#应用的条件" class="headerlink" title="应用的条件"></a>应用的条件</h2><ul><li>当$p(x)$不等于0时，$q(x)$也不能接近于0</li></ul><p>实际上 $p$ 和 $q$ 不能差太多，否则他们的方差后差距很大。重要采样只是得到的<strong>均值相同</strong>，但<strong>方差不同</strong>。</p><script type="math/tex; mode=display">\begin{eqnarray*}E_{x\sim p}[f(x)] &=& E_{x\sim q}[f(x)\frac{p(x)}{q(x)}]\\\\Var_{x\sim p}[f(x)] &=& E_{x\sim p}[f(x)^2]- \left (E_{x\sim p}[f(x)]\right )^2\\\\Var_{x\sim q}[f(x)\frac{p(x)}{q(x)}] &=&E_{x\sim q}\left[  \left (    f(x)\frac{p(x)}{q(x)}  \right )^2\right] -\left(E_{x\sim q}  \left [    f(x)\frac{p(x)}{q(x)}  \right ]\right)^2\\\\&=&E_{x\sim p}\left[    f(x)^2\frac{p(x)}{q(x)}\right] -\left(E_{x\sim p}[f(x)]\right)^2\\\\&\neq&E_{x\sim p}[f(x)^2]- \left (E_{x\sim p}[f(x)]\right )^2=Var_{x\sim p}[f(x)]\end{eqnarray*}</script><p>$p$ 和 $q$ 的分布差异越大，方差也会越大。比如下图，$E_{x\sim p}[f(x)]$应该是negative的，但是<strong>由于$q(x)$在负轴上的概率很小，很有可能采样不到，导致根据重要采样得到的结果是positive的</strong>。当然如果采样了足够多次，则从q中采样到x为负值之后，其weight会非常高，从而平衡正值上的结果。</p><img src="/2021/09/18/2021-09-18-%E9%87%8D%E8%A6%81%E9%87%87%E6%A0%B7/Issue-of-Importance-Sampling.png" class="" title="Issue-of-Importance-Sampling"><p>如果仔细看重要采样之后的方差和真实方差之间的差异，会观察到其形式和相对熵很类似。将两者作差会得到：</p><script type="math/tex; mode=display">\begin{eqnarray*}&&E_{x\sim p}\left[    f(x)^2\frac{p(x)}{q(x)}\right]- E_{x\sim p}\left[    f(x)^2\right]\\\\&=&E_{x\sim p}\left[    f(x)^2\frac{p(x)}{q(x)} - f(x)^2\right]\\\\&=&E_{x\sim p}\left[    \frac {f(x)^2}{q(x)}    \left (      p(x) - q(x)    \right )\right]\end{eqnarray*}</script><p>这个结果<strong>一定是大于0的？</strong>即重要采样之后的方差一定是大于真实的方差的吗？这个证明和交叉熵的非负性证明看起来很相似。</p><p><strong>疑问</strong></p><script type="math/tex; mode=display">\begin{eqnarray*}&&\int f(x)g(x)dx \leq \int f(x)^2dx\\&& st.\ \int f(x)dx=\int g(x)dx=1\end{eqnarray*}</script><p>在KL散度的非负性的证明中可以知道下式是成立的</p><script type="math/tex; mode=display">\begin{eqnarray*}&& \int f(x)\ log\ f(x)dx > \int f(x)\ log\ g(x)dx\\&& st.\ \int f(x)dx=\int g(x)dx=1\end{eqnarray*}</script><p>证明过程：</p><script type="math/tex; mode=display">\begin{eqnarray*}&& \int f(x)\ log\ f(x)dx - \int f(x)\ log\ g(x)dx\\\\&=&\int f(x)\ log\ \frac{f(x)}{g(x)}dx\\\end{eqnarray*}</script><script type="math/tex; mode=display">\int \frac {p(x)^2}{q(x)}dx \geq 1</script><script type="math/tex; mode=display">\int f(x)^2dx=1\\\int g(x)^2dx=1</script><p>​    </p><h2 id="物理含义"><a href="#物理含义" class="headerlink" title="物理含义"></a>物理含义</h2><p>利用一个人群的表现，估计另一个人群的表现。比如拒绝推断；一个药物的效果（$f(X)$）只和身高、体重、血型、星座（$x$）相关，这个药物只有中国人（$q(x)$）用过，现在先知道美国人（$p$）用的效果。</p><p>​    </p><h2 id="常见问题"><a href="#常见问题" class="headerlink" title="常见问题"></a>常见问题</h2><p><strong>什么情况下会不能从$p(x)$采样数据？</strong></p><p>比如说前面说的某个药物只在某个人群上使用过、只有申请通过的用户上才会有表现。在强化学习Off Policy中，不想Actor有一点更新就要重新和环境做互动，互动可以得到奖励，对应$f(x)$，给定一个Actor，我们可以知道其产生各种动作的概率，对应$p(x)$，老版的Actor或其他策略产生各动作的概率也已知，对应$q(x)$。</p><p>​    </p><h2 id="异策略评估方法"><a href="#异策略评估方法" class="headerlink" title="异策略评估方法"></a>异策略评估方法</h2><p>在异策略场景中，我们想要知道在目标域的 $s$ 分布下，按照策略 $\pi$ 决策动作 $a$ 时，得到的奖励 $r$ 的期望值。其中，目标域的 $s$ 的分布可以表示为 $p_t(s)$；$\pi$是根据 $s$ 产出 $a$，可以表示为 $p_{\pi}(a|s)$ ；$r$ 是由 $s$ 和 $a$ 决定，表示为 $r(s,a)$；我们将 $s$ 和 $a$ 的联合分布表示为 $p(s,a)=p_t(s)·p_{\pi}(a|s)$。</p><p>如果我们能拿到的是源域的 $s$ 分布下，按照另一个策略 $\pi^{\prime}$ 决策出动作的，也能观测到这些数据得到的奖励 $r$ 。其中，源域的 $s$ 分布表示为 $q_s(s)$；给定 $s$ 后 $a$ 的分布表示为 $q_{\pi^{\prime}}(a|s)$ ；此时 $s$ 和 $a$ 的联合分布表示为 $q(s,a)=q_s(s)·q_{\pi^{\prime}}(a|s)$。</p><p>套用上文中重要采样的公式：</p><script type="math/tex; mode=display">\begin{eqnarray*}E_{x\sim p}[f(x)] &=& E_{s,a\sim p}[r(s,a)]\\\\&=& \int r(s,a)\ p(s,a)\ d(s,a)\tag{概率定义}\\\\&=& \int r(s,a)\frac{p(s,a)}{q(s,a)}\ q(s,a)\ d(s,a)\tag{重要采样}\\\\&=& \int \frac{p(s,a)}{q(s,a)}\ r(s,a)\ q(s,a)\ d(s,a)\tag{换个顺序}\\\\&=& \int \frac{p_t(s)·p_{\pi}(a|s)}{q_s(s)·q_{\pi^{\prime}}(a|s)}\ r(s,a)\ q(s,a)\ d(s,a)\tag{拆为条件概率}\\\\&=& \int \frac{p_t(s)}{q_s(s)}  \ \frac{p_{\pi}(a|s)}{q_{\pi^{\prime}}(a|s)}\ r(s,a)\ q(s,a)\ d(s,a)\tag{分为两项}\\\\&=& E_{s,a\sim q}\left [{\color{red}\frac{p_t(s)}{q_s(s)}  \ \frac{p_{\pi}(a|s)}{q_{\pi^{\prime}}(a|s)}}\ r(s,a)\right ]\tag{加权期望形式}\end{eqnarray*}</script><p>可以看到每个样本的权重为$\frac{p_t(s)}{q_s(s)}$ 和 $\frac{p_{\pi}(a|s)}{q_{\pi^{\prime}}(a|s)}$ 的乘积。前一项表示的是 $s$ 分布本身的差异，后一项表示的是策略之间的差异，和前文的<strong>应用条件</strong>一样，这里当$p$不等于0时，$q$ 也不能等于或接近于0。简单说，就是<strong>可能在目标域中出现的样本，也需要可能在源域中出现；待评估策略 $\pi$ 可能给的Action，实际应用的策略 $\pi^{\prime}$ 也必须可能给</strong>。</p><p>​    </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;重要采样和样本选择偏差和因果推断中的倾向性得分加权、强化学习off-policy等内容高度相关，是一个基本工具。&lt;/p&gt;
    
    </summary>
    
      <category term="基础知识" scheme="http://wangdongdong122.github.io/categories/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
    
      <category term="基础知识" scheme="http://wangdongdong122.github.io/tags/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
      <category term="重要采样" scheme="http://wangdongdong122.github.io/tags/%E9%87%8D%E8%A6%81%E9%87%87%E6%A0%B7/"/>
    
      <category term="样本偏差" scheme="http://wangdongdong122.github.io/tags/%E6%A0%B7%E6%9C%AC%E5%81%8F%E5%B7%AE/"/>
    
  </entry>
  
  <entry>
    <title>双重差分法（DID）</title>
    <link href="http://wangdongdong122.github.io/2021/09/05/2021-09-05-%E5%8F%8C%E9%87%8D%E5%B7%AE%E5%88%86%E6%B3%95(DID)/"/>
    <id>http://wangdongdong122.github.io/2021/09/05/2021-09-05-双重差分法(DID)/</id>
    <published>2021-09-05T06:19:46.000Z</published>
    <updated>2021-11-18T15:06:12.169Z</updated>
    
    <content type="html"><![CDATA[<p>双重差分法,Differences-in-Differences，又名“倍差法”。</p><span id="more"></span><h2 id="双重差分法-DID"><a href="#双重差分法-DID" class="headerlink" title="双重差分法(DID)"></a>双重差分法(DID)</h2><h3 id="计算方法"><a href="#计算方法" class="headerlink" title="计算方法"></a>计算方法</h3><p>做随机实验或自然随机实验后，实验的效果常常需要一段时间才能呈现出来，而我们关注的恰恰是 $y$ 实验前后的变化 。因此，考虑</p><script type="math/tex; mode=display">y_{it}=\alpha + \gamma D_t + \beta x_{it} +u_{i} + \epsilon_{it} \ \ (i=1,...,n;\ t=1,2)</script><p>其中，$t=1$表示实验之前，$t=2$表示实验之后，$D_1=0,\ D_2=1$；$x_{it}=D_t * treat_i$，表示处理动作是否生效，只有试验后的实验组为1；$u_{i}$表示用户特征。</p><p>用实验后方程-实验前方程（面板数据的一阶差分），可以消去$u_i$，得到</p><script type="math/tex; mode=display">\Delta y_i = \gamma + \beta x_{i2}+\Delta\epsilon_{i}</script><p>用OLS估计上式，即可得到一致性估计。</p><script type="math/tex; mode=display">\begin{eqnarray*}\hat \beta_{OLS} &=& \Delta \overline y_{treat}-\Delta \overline y_{control}\\\\&=&(\overline y_{treat,2}-\overline y_{treat,1})-(\overline y_{control,2}-\overline y_{control,1})\end{eqnarray*}</script><p>图示</p><img src="/2021/09/05/2021-09-05-%E5%8F%8C%E9%87%8D%E5%B7%AE%E5%88%86%E6%B3%95(DID)/5b0988e595225.cdn.sohucs.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=jpeg" class="" title="img"><p>注</p><ol><li>既然是随机测试，为什么处理组和对照组在政策干预之前不是重合，而是平行？</li></ol><h3 id="核心逻辑"><a href="#核心逻辑" class="headerlink" title="核心逻辑"></a>核心逻辑</h3><p>利用<strong>共同趋势（平行趋势）</strong>，也就是说，处理组和对照组在treatment实施之前必须具有相同的发展趋势，然后利用对照组在treatment实施之后的趋势外推处理组的趋势，从而获得处理组的因果效应。</p><h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><p>适用场景：一般而言，DID仅<strong>适用于面板数据</strong>$^{[1]}$，因此在只有截面数据时，还是不要浪费心思在DID上了</p><p>​    </p><p>​    </p><p>注</p><p>[1]. 面板数据（panel data 或 longitudinal data，也译为“平行数据”），指的是在一段时间内跟踪同一组个体（individual）的数据。《高级计量经济学及Stata应用》,（第二版），P250</p><p>​    </p><p>参考内容</p><ol><li>《高级计量经济学及Stata应用》,（第二版），P339</li><li><a href="https://zhuanlan.zhihu.com/p/48952513">知乎专栏，双重差分法（DID）介绍</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;双重差分法,Differences-in-Differences，又名“倍差法”。&lt;/p&gt;
    
    </summary>
    
      <category term="经典算法" scheme="http://wangdongdong122.github.io/categories/%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="因果推断" scheme="http://wangdongdong122.github.io/tags/%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD/"/>
    
      <category term="计量经济学" scheme="http://wangdongdong122.github.io/tags/%E8%AE%A1%E9%87%8F%E7%BB%8F%E6%B5%8E%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>因果推断在业界的应用</title>
    <link href="http://wangdongdong122.github.io/2021/09/05/2021-09-05-%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD%E5%9C%A8%E4%B8%9A%E7%95%8C%E5%BA%94%E7%94%A8/"/>
    <id>http://wangdongdong122.github.io/2021/09/05/2021-09-05-因果推断在业界应用/</id>
    <published>2021-09-05T03:19:46.000Z</published>
    <updated>2021-09-05T06:39:37.468Z</updated>
    
    <content type="html"><![CDATA[<p>当前可调研到的资料中，因果推断在各家大厂中的应用情况。</p><span id="more"></span><h2 id="快手"><a href="#快手" class="headerlink" title="快手"></a>快手</h2><p><a href="https://mp.weixin.qq.com/s/svVl1eiVUH6rOYG3p2YiGg">快手因果推断与实验设计</a></p><p>​    </p><h3 id="场景与问题"><a href="#场景与问题" class="headerlink" title="场景与问题"></a>场景与问题</h3><p><strong>问题</strong></p><ol><li>用户激励设计</li><li>推荐策略评估</li><li>产品功能迭代</li><li>预估产品和方向的长期价值。</li></ol><p>​    </p><p><strong>解决思路</strong></p><ul><li>基于观测数据的因果推断，即从已有实验和非实验数据中提炼因果关系；</li><li>在产品设计上构建正确的AB实验，合理计算指标，度量产品功能和迭代的影响；</li><li>通过经济模型、机器学习算法和数据、实验的结合构造反事实推理来回答长期效应问题。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;当前可调研到的资料中，因果推断在各家大厂中的应用情况。&lt;/p&gt;
    
    </summary>
    
      <category term="应用调研" scheme="http://wangdongdong122.github.io/categories/%E5%BA%94%E7%94%A8%E8%B0%83%E7%A0%94/"/>
    
    
      <category term="因果推断" scheme="http://wangdongdong122.github.io/tags/%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD/"/>
    
      <category term="应用落地" scheme="http://wangdongdong122.github.io/tags/%E5%BA%94%E7%94%A8%E8%90%BD%E5%9C%B0/"/>
    
  </entry>
  
  <entry>
    <title>因果推断：the book of why</title>
    <link href="http://wangdongdong122.github.io/2021/08/08/2021-08-08-%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD-the-book-of-why/"/>
    <id>http://wangdongdong122.github.io/2021/08/08/2021-08-08-因果推断-the-book-of-why/</id>
    <published>2021-08-08T01:30:35.000Z</published>
    <updated>2021-11-09T12:43:52.455Z</updated>
    
    <content type="html"><![CDATA[<p>记录学习《因果之书》的笔记。</p><span id="more"></span><h2 id="因果关系介绍"><a href="#因果关系介绍" class="headerlink" title="因果关系介绍"></a>因果关系介绍</h2><h3 id="因果关系"><a href="#因果关系" class="headerlink" title="因果关系"></a>因果关系</h3><p>因果关系的定义一直是个哲学问题，作为一个不可知论者，我一直觉得因果关系涉及最基本的哲学，是<strong>不可知</strong>的。下雨后草木生长，按照唯物主义观点，下雨是因，草木生长是果；但是如果上帝存在，上次可能是希望草木生长所以创造了雨，才会下雨，因果似乎颠倒了。</p><p>我们现实生活中常常也有此类的描述，我们要去吃饭是因为要填饱肚子，填饱肚子是我们要去吃饭的原因。这里的因果似乎也有点奇怪，当存在一个<strong>有意识，有目的，有预测能力</strong>的主体时，其可能会根据因果关系来决定行为，这种情况下“因果关系”（预测结果）是”因”发生的原因。</p><p>​    </p><p>看一下哲学家给出过的定义</p><ul><li>亚里士多德：“质料因”，“形式因”，“动力因”，“目的因”</li><li>休谟：我们可以给一个因下定义说，它是<strong>先行于</strong>、接近于另一个对象的一个对象，而且在这里，凡与前一个对象类似的一切对象都和与后一个对象类似的那些对象处在类似的先行关系和接近关系中。或者，换言之，<strong>假如</strong>没有前一个对象，那么后一个对象就不可能存在。</li></ul><p>​    </p><p><strong>时间先后</strong>的引入可以避免很多问题。很多地方也引入这个约束，比如在信号与系统里，对因果系统的定义是：”零状态响应不出现在激励之前的系统为因果系统”，简单说就是，输入引起的响应不出现在输入之前。但当时间只能单向流动的规则被打破之后，因果关系开始模糊，变得难以理解。比如星际穿越中，未来的人类掌握了五维空间的黑洞，时间变成一个维度，可以在其中移动。然后未来的人类放了一个五维黑洞，让现在的人类可以继续研究如何拯救人类，现在的人类生存下来才有了未来的人类。这里又很难区分因和果。</p><p>不过本文不讨论<strong>因果发现问题</strong>，只讨论在给定因果关系的假设下，<strong>因果效应的量化</strong>问题和<strong>潜在结果</strong>的计算。</p><p>近年来，因果推断越来越火，相关方向的paper越来越多，业界也开始有越来越多相关的方向。</p><img src="/2021/08/08/2021-08-08-%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD-the-book-of-why/image-20210807163727974-1628325552533.png" class="" title="学术热度"><p>​    </p><h3 id="因果与相关"><a href="#因果与相关" class="headerlink" title="因果与相关"></a>因果与相关</h3><p><strong>相关关系不能说明有因果关系</strong>，一个典型的案例是小孩的阅读能力和年龄有关，衣服的大小也有年龄有关，导致衣服大小也与阅读能力有相关性。但两者之间显然没有因果关系，给小孩穿更大的衣服不会提升其阅读能力。</p><p>那么如果给我们10000个小孩的年龄、衣服大小、阅读能力得分，10000行×3列的数据，我们如何只根据数据判断三者之间的因果关系呢？答案是不行，年龄和衣服大小之间的相关性没有方向，如果不结合日常生活的常识，我们无法只通过数据判断谁是因谁是果。</p><ul><li>统计学本身无法告诉哪个是因，哪个是果</li></ul><p><strong>人类的理解世界是以因果关系为基础</strong>而非统计关系，这对应人类做决策判断至关重要。当我们想达到某个结果Y而做一件事X时，实际是因为我们相信做了X，会让Y发生的概率变大，而这正是因果关系而非相关关系。这还有一个可能的原因是<strong>因果关系比相关关系稳健</strong>，相关关系太受各种因素影响，不够稳定，比如观测的范围的偏差、X变量的分布；相对而言，因果关系更多的是直接由客观规律和事情发生的机制直接决定的，接近于<strong>逻辑与规律</strong>的概念，通用性更强。</p><ul><li>人类为什么不一样，人类的直觉是以因果关系而统计关系为组织核心</li><li>深度学习只是让机器具备了高超的能力，而非智能，其致力于拟合出一个函数</li><li>相关性关系并不稳定，但因果关系却更为稳健</li></ul><p>​    </p><h3 id="因果与贝叶斯"><a href="#因果与贝叶斯" class="headerlink" title="因果与贝叶斯"></a>因果与贝叶斯</h3><p>贝叶斯网络并未假设箭头有任何因果意义，只意味着前向概率</p><p>贝叶斯网络和因果图的区别：</p><ul><li>贝叶斯网络并未假设箭头有任何因果意义（所以箭头的方向可以反过来？）</li><li>贝叶斯网络中箭头A-&gt;B，B的概率通过条件概率与A的值相关，且该相关关系是完备的（p144）</li><li>因果图中箭头A-&gt;B，在假设实验角度解释，表示如果我只调整A，就可以看到C的概率发生变化（p145）</li></ul><p>​    </p><h2 id="因果关系的表示"><a href="#因果关系的表示" class="headerlink" title="因果关系的表示"></a>因果关系的表示</h2><h3 id="三节点网络"><a href="#三节点网络" class="headerlink" title="三节点网络"></a>三节点网络</h3><p>三种基本的三节点网络</p><img src="/2021/08/08/2021-08-08-%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD-the-book-of-why/image-20210808103010446.png" class="" title="三节点网络"><p>​    </p><p><strong>案例</strong></p><p>链式：火灾 → 烟雾 → 警报</p><p>叉式：鞋子尺寸 ← 孩子的年龄 → 阅读能力</p><p>对撞：才华 → 名人 ← 美貌；名人才华与美貌负相关</p><p>​    </p><p><strong>独立性质</strong></p><p>链式：A与C不独立；给定B后，A与C独立，其中B为中介物</p><p>叉式：A与C不独立；给定B后，A与C独立</p><p>对撞：A与C独立；给定B后，A与C不独立</p><p>​    </p><p>注：</p><ul><li><p>上述独立性，建立在三节点网络已经是完备因果图的基础上</p></li><li><p>各个节点都可能还有原因，但这个原因不影响其他变量即可不表示为节点</p></li><li><p>我们不能通过数据区分链式和叉式接合，因为他们具有相同的条件独立性</p></li></ul><p>​    </p><h3 id="独立性质证明"><a href="#独立性质证明" class="headerlink" title="独立性质证明"></a>独立性质证明</h3><p><strong>背景知识</strong></p><p>概率全展开公式</p><script type="math/tex; mode=display">p(x_1,…,x_K)=p(x_K│x_1,…,x_{K−1}),…,p(x_2|x_1)p(x_1)</script><p>该公式对应全连接有向无环图，任意两个节点之间都有连接。</p><p>有向图中：变量（节点）的条件概率与其父节点相关联，且此相关关系是充分的（即其他祖先不会影响这一公式）</p><script type="math/tex; mode=display">p(\boldsymbol x)=\prod_{k=1}^K p(x_k|pa_k)</script><p>其中，$p(x)$表示所有节点的联合分布，k表示第k个节点，$pa_k$表示第k个节点的所有父节点。（PRML，P382）</p><p>​    </p><p><strong>链式证明</strong></p><p>​    </p><h2 id="后门路径"><a href="#后门路径" class="headerlink" title="后门路径"></a>后门路径</h2><h3 id="混杂定义"><a href="#混杂定义" class="headerlink" title="混杂定义"></a>混杂定义</h3><p>关于混杂的定义有很多，这里不再展开解释，给出一个本文的定义</p><ul><li>混杂偏倚（confounding bias）：$p(y|x)≠p(y|do(x))$</li></ul><p>比如给土壤质量差的土地施肥的情况下，肥料和产量之间存在混杂；由于随年龄的变化，人对运动的偏好有明显差异，此时运动和死亡率之间也存在混杂。</p><img src="/2021/08/08/2021-08-08-%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD-the-book-of-why/image-20210808131455727.png" class="" title="混杂案例"><p>要消除这种混杂，可以做随机测试，抽签决定每块土地是否施肥，此时就可以消除这个场景中的混杂。</p><img src="/2021/08/08/2021-08-08-%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD-the-book-of-why/image-20210808131717441.png" class="" title="随机实验"><p>这里可以消除混杂是因为抽签和产量之间没有因果关系，在这个实验中应该是没问题的。但在一些医学问题中，由于安慰剂效应，抽签结果可能还是会影响病人康复情况，所以要进行双盲实验。</p><p>​    </p><h3 id="后门路径-1"><a href="#后门路径-1" class="headerlink" title="后门路径"></a>后门路径</h3><p>书中对后门路径的定义是：<strong>X到Y之间所有的路径中，以指向X的箭头开始的路径</strong>。</p><blockquote><p>按照这个定义，包含对撞的路径不算后门路径，这会导致我们需要单独判断对撞是否产生偏倚。个人认为可以将包含<strong>指向X方向的箭头的路径</strong>定义为后门路径，这样就包括了对撞路径，一起判断就可以。</p></blockquote><p>​    </p><p><strong>阻断信息规则</strong>（只针对路径上的信息流动）</p><ol><li>链式（A-&gt;B-&gt;C）、叉式（A&lt;-B-&gt;C）：控制B可阻断A和C间信息流动</li><li>对撞（A-&gt;B&lt;-C）：A和C本身就是阻断的（控制B将导致信息流通）</li><li>控制一个变量的后代节点，将“部分地”控制变量本身</li></ol><p>​    </p><p><strong>去除混杂规则</strong></p><p>阻断了所有的后门路径，则完成了X和Y的去混杂</p><p>去混因子：一个可以阻断干预与结果之间所有后门路径的变量集</p><p>​    </p><p>如果将X和Y之间所有的路径分为三种，对应的<strong>去混杂的原则</strong>如下</p><ol><li>因果路径：反映了X→Y的因果关系，不应该被阻断</li><li>后门路径：产生混杂，应该被阻断，阻断时不应该违反第一条</li><li>对撞路径：原本处于阻断状态，不应该再控制导致混杂，如果已经控制了，则需要再阻断该路径，阻断时不能违反前两条。</li></ol><p>比如下图，第一个，不应该控制Z，第二个和第三个都是Z和M都不应该被控制。从这些案例我们知道，并<strong>不是控制变量越多越好</strong>，基于观察数据的分析，什么变量需要控制，什么变量不需要控制需要基于因果图来确定。</p><img src="/2021/08/08/2021-08-08-%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD-the-book-of-why/image-20210808133506939.png" class="" title="控制变量"><p>​    </p><h3 id="解释悖论"><a href="#解释悖论" class="headerlink" title="解释悖论"></a>解释悖论</h3><h4 id="相亲对象"><a href="#相亲对象" class="headerlink" title="相亲对象"></a>相亲对象</h4><p>问题：你有没有发现，你的相亲对象要么笨，要么丑！</p><p>因果图：才华 → 和你约会 ← 颜值</p><p>解释：才华和颜值可能没什么相关性，但介绍给你相亲的人中，两者之间可能就有相关性。又笨又丑的多半也不会介绍给你，高富帅、白富美多半也不会。</p><p>​    </p><h4 id="邮件骗局"><a href="#邮件骗局" class="headerlink" title="邮件骗局"></a>邮件骗局</h4><p>问题：如果你收到预测股市涨跌的邮件，连续10次都是准的，第11次预测结果要收费，要不要买呢？</p><p>因果图：预测 → 命中 ← 股市</p><p>解释：可能是每次给10万人发随机的预测结果，这样也会有大概千分之一的人都预测对，这部分人的第11次预测结果和股市涨跌之间并没有什么因果关系。</p><p>​    </p><h4 id="出生体重悖论"><a href="#出生体重悖论" class="headerlink" title="出生体重悖论"></a>出生体重悖论</h4><p>悖论：出生体重轻的婴儿中，母亲吸烟的，死亡率低</p><p>因果图：</p><img src="/2021/08/08/2021-08-08-%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD-the-book-of-why/image-20210808170719233.png" class="" title="出生体重悖论"><p>解释：控制出生体重导致了偏倚，先天缺陷且母亲吸烟的，可能死亡的概率大很多。</p><p>​    </p><h4 id="辛普森悖论"><a href="#辛普森悖论" class="headerlink" title="辛普森悖论"></a>辛普森悖论</h4><p>悖论：辛普森医生发现一种新药，吃了的人比不吃的人发病率低（0.78 vs 0.83），但这种药似乎对男性和女性都有害，吃了的男性比不吃的男性发病率高（0.93 vs 0.87），吃了药的女性也比不吃药的女性发病率高（0.73 vs 0.69）。</p><img src="/2021/08/08/2021-08-08-%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD-the-book-of-why/image-20210808145736231.png" class="" title="辛普森悖论-数据"><p>因果图：</p><img src="/2021/08/08/2021-08-08-%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD-the-book-of-why/image-20210808145524642.png" class="" title="辛普森悖论"><p>解释：男女患病风险不同，对服药的偏好也不同。</p><p>扩展：辛普森悖论反映的是整体的统计结果和分层的统计结果可能相反。那么是整体统计结果正确还是分层结果正确呢？</p><ul><li>不同情况下不一样，需要根据具体研究的问题和因果图来判断。</li></ul><p>​    </p><h4 id="伯克利大学招生悖论"><a href="#伯克利大学招生悖论" class="headerlink" title="伯克利大学招生悖论"></a>伯克利大学招生悖论</h4><p>悖论：1973年，有学者发现伯克利大学招生的时候男女录取比例不一样，男生录取率明显高于女生，男生44%，女生35%。当时公众很关注男女歧视问题，怎么办，伯克利大学的招生是各个系自己定的，所以挨个系查。但是查完就懵了，因为每个系的招生：都有利于女生，实际结果也都是女生录取比例比男生高。</p><p>因果图：</p><img src="/2021/08/08/2021-08-08-%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD-the-book-of-why/image-20210809214449045.png" class="" title="伯克利大学招生悖论简化版"><p>解释：也是一个辛普森悖论，女生喜欢申请难录取的院系，但此时院系不是混杂。</p><p>扩展：上图看起来似乎是学校不存在歧视，但实际上如果因果图没画全，院系不仅有性别歧视还有地域歧视。比如说自己的家乡只录取男生，其他地方只录取女生（觉得女生读书就是祸害），如下面的因果图，那也有可能得到完全一样的数据。所以说因果推断的问题不能只从数据中求解，相同的数据，不同的因果假设，得到的结论完全不同。</p><img src="/2021/08/08/2021-08-08-%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD-the-book-of-why/image-20210808145630481.png" class="" title="伯克利大学招生悖论"><p>​    </p><h4 id="蒙提霍尔悖论"><a href="#蒙提霍尔悖论" class="headerlink" title="蒙提霍尔悖论"></a>蒙提霍尔悖论</h4><p>悖论：有三扇门，门后有车、山羊、山羊。你如果选中有车的门，车就归你。现在你选择了A门，主持人打开了另一扇后面有山羊的B门，问你是否要将你的选择换成C门。</p><img src="/2021/08/08/2021-08-08-%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD-the-book-of-why/image-20210808145659844.png" class="" title="蒙提霍尔悖论-场景"><p>因果图：</p><img src="/2021/08/08/2021-08-08-%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD-the-book-of-why/image-20210808145809152.png" class="" title="蒙提霍尔悖论"><p>解释：答案是要换，不换1/3，换了2/3。直觉上理解车子在哪个门后面的概率似乎不会变，都一直是1/3，但实际上，得知主持人打开哪个门之后，你选的门和车子的位置之间是相关的。</p><p>​    </p><h2 id="因果效应量化"><a href="#因果效应量化" class="headerlink" title="因果效应量化"></a>因果效应量化</h2><h3 id="后门标准"><a href="#后门标准" class="headerlink" title="后门标准"></a>后门标准</h3><p><strong>使用条件</strong>：去混因子可观测</p><p>已掌握了变量的一个充分集（包含去混因子）的数据，可以用来阻断干预和结果之间的所有的后门路径。</p><p>​    </p><p><strong>后门调整公式</strong></p><script type="math/tex; mode=display">P(Y|do(X))=∑_ZP(Z=z,X) P(Z=z)</script><blockquote><p>去混因子的各层中的因果效应的趋势是一致的（因为去混因子不影响因果效应，P235）</p></blockquote><p>​    </p><p><strong>做法</strong></p><ol><li>估计去混因子每个水平（分层）上，干预的平均因果效应</li><li>计算各层因果效应的加权平均值，权重为每层在总体中的分布频率</li></ol><ul><li>线性近似将问题大大简化了，每个因果效应都可以用一个数字来表示（P236）</li><li>偏回归系数暗中执行了后门调整（P239）</li></ul><p>​    </p><h3 id="前门标准"><a href="#前门标准" class="headerlink" title="前门标准"></a>前门标准</h3><p>以吸烟为例，假设研究人员可以测量吸烟者肺部的焦油沉积量，且吸烟只通过焦油沉淀引发癌症</p><img src="/2021/08/08/2021-08-08-%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD-the-book-of-why/image-20210808150116299.png" class="" title="前门路径-吸烟案例"><p>前门调整公式：</p><script type="math/tex; mode=display">P(Y│do(X))=∑_ZP(Z=z,X)∑_XP(Y│X=x,Z=z)P(X=x)</script><p>在吸烟的案例中，X代表“吸烟”，Y代表“癌症”，Z代 表“焦油沉积”，U（在此例中显然没有出现在公式中）代表不可观测的变量，即“吸烟基因”。</p><p>​    </p><p><strong>使用条件</strong>：</p><p>X对Y的因果效应被一组变量C混杂，又被另一组变量M介导，并且中介变量M不受C的影响。（此处笔者认为M也应该不能有其他混杂）</p><p>​    </p><h3 id="do运算"><a href="#do运算" class="headerlink" title="do运算"></a>do运算</h3><blockquote><p>目标：消除do算子，只留下经典的概率表达式，如$P(Y|X)$</p></blockquote><p>do运算<strong>3条法则</strong></p><ol><li><p>如果我们观察到W与Y无关（可在给定其他变量Z的条件下），那么Y的概率分布就不会随W而变</p><script type="math/tex; mode=display">P(Y|do(X),Z,W)=P(Y|do(X),Z)</script><p>如，$P(警报|do(烟雾),火灾)=P(警报|do(烟雾))$</p><p>本法则允许增加或删除某个观察结果。</p></li><li><p>如果变量集Z阻断了从X到Y的所有后门路径，则以Z为条件时，$do(X)$等同于$see(X)$</p><script type="math/tex; mode=display">P(Y|do(X),Z)=P(Y|X,Z)</script><p>本法则允许用观察替换干预，或反之。</p></li><li><p>若X到Y没有因果路径，则可将$do(X)$从$P(Y|do(X))$中移除</p><script type="math/tex; mode=display">P(Y|do(X),Z)=P(Y)</script><p>本法则允许删除或添加干预。</p></li></ol><p>​    </p><p><strong>案例</strong>：</p><img src="/2021/08/08/2021-08-08-%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD-the-book-of-why/image-20210808150517716.png" class="" title="do运算案例"><script type="math/tex; mode=display">P(Y|do(X))=∑_ZP(Y|do(X),Z)P(Z=z|do(X))                                   =∑_ZP(Y|X,Z=z)P(Z=z)</script><p>​    </p><h2 id="因果图下的新视角"><a href="#因果图下的新视角" class="headerlink" title="因果图下的新视角"></a>因果图下的新视角</h2><blockquote><p>本部分内容存在大量自己总结的内容，可能有误</p></blockquote><p>​    </p><h3 id="工具变量法"><a href="#工具变量法" class="headerlink" title="工具变量法"></a>工具变量法</h3><p><a href="https://zh.wikipedia.org/wiki/%E5%86%85%E7%94%9F%E5%8F%98%E9%87%8F">内生变量</a>：所有与扰动项相关的解释变量都称为“内生变量”。$^{[1]}$</p><p>内生变量导致的问题：OLS得到的估计量不一致。</p><p>工具变量法：</p><img src="/2021/08/08/2021-08-08-%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD-the-book-of-why/image-20210808154124101.png" class="" title="工具变量法"><p>在线性假设下，$ab=r_{ZY},a=r_{ZX}$，因此，$b=r_{ZY}/r_{ZX}$。其中$r_{ZY}$ 表示Z在Y上回归线的斜率， $a$表示Z增加一个单位的干预将导致X增加 $a$个单位。</p><p>​    </p><h3 id="偏回归"><a href="#偏回归" class="headerlink" title="偏回归"></a>偏回归</h3><p><strong>FWL定理</strong></p><p>主要思想：剔除一个变量（如，月收入）对另一个变量（如，额度）的影响，对参数（如，额度对逾期的影响）进行更为精准的估计</p><p>FWL定理：</p><ul><li>对于多元线性回归模型: $y_i=a+b_1x_{1i}+b_2x_{2i}+b_3x_{3i}+\epsilon _i$</li><li>当$x_1 \leftrightarrow x_2$之间相互影响，会导致估计偏差，解决步骤如下：<ol><li>x1对其他x回归得到误差v：</li><li>y对其他的x回归得到误差w：</li><li>w对v回归得到系数η：</li></ol></li><li>$η$ 就是b1的无偏估计</li></ul><p>可理解为，$η$ 表示的是“额度中与月收入无关的部分”对“逾期率中与月收入无关的部分”的解释力</p><p><strong>与后门路径之间的关系</strong></p><p>偏回归系数 实际和控制其他变量之后，X对Y的影响 ；实际逻辑后门标准是一致的</p><img src="/2021/08/08/2021-08-08-%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD-the-book-of-why/image-20210808160721150.png" class="" title="image-20210808160721150"><ul><li>减去均值会影响偏置，但不影响斜率</li><li>该计算方式下，计算得到的相关性系数和后门标准一致：$P(Y│do(X))=∑_ZP(Z=z,X) P(Z=z)$</li><li>如果Z是中介值，偏回归系数应该不对应自然因果效应，而对应直接因果效应。</li></ul><p>​    </p><h2 id="反事实"><a href="#反事实" class="headerlink" title="反事实"></a>反事实</h2><p>本部分介绍如何计算出潜在结果</p><h3 id="潜在结果"><a href="#潜在结果" class="headerlink" title="潜在结果"></a>潜在结果</h3><p>什么是潜在结果(potential outcomes)？定义一个Y的潜在结果，$Y_{X=x}(u)$：假如X的取值为x，那么Y在个体u上的取值（<strong>个体层面</strong>上的定义）。</p><p>如何推断潜在结果？</p><p>给个案例：如果<strong>工资</strong>（S）由<strong>学历</strong>（ED，0：高中，1：大学，2，硕士）和<strong>工作经验</strong>（EX，年数）决定。现在有以下数据，问题是<strong>如果爱丽丝上大学了，工资是多少</strong>？</p><img src="/2021/08/08/2021-08-08-%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD-the-book-of-why/image-20210808172100062.png" class="" title="潜在结果-案例"><p>​    </p><h3 id="常见错误做法"><a href="#常见错误做法" class="headerlink" title="常见错误做法"></a>常见错误做法</h3><p><strong>错误的做法是：将因果推断问题看作数据缺失问题</strong>，利用差值法填补空格，或者推断出缺失数据。</p><ul><li>错误方法1：匹配法<ul><li>做法：伯特、卡罗琳工作年限相同 → 伯特的$S_2=9700$，卡罗琳的$S_1=92500$。</li><li>存在问题：没有考虑学历和工作经验之间的影响</li></ul></li><li>错误做法2：线性回归<ul><li>做法：回归观测结果为：$S=65000+2500EX+5000ED$，因此爱丽丝有大学文凭时（EX=6，ED=1），S=85000</li><li>存在问题：没有考虑学历和工作经验之间的影响</li></ul></li></ul><p>​    </p><h3 id="奈曼-鲁宾因果模型"><a href="#奈曼-鲁宾因果模型" class="headerlink" title="奈曼-鲁宾因果模型"></a>奈曼-鲁宾因果模型</h3><p>先来看下两位大佬的合影（左Pearl，右Robins），后面我也会再写一篇Robin的书的学习笔记，这里只简单介绍一下其<strong>三个假设</strong>。</p><img src="/2021/08/08/2021-08-08-%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD-the-book-of-why/image-20210809220700019.png" class="" title="2014年，唐纳德·鲁宾（右）与本书第一作者（资料来源：照片由格雷斯·铉·金提供）"><p>​    </p><h4 id="稳定性"><a href="#稳定性" class="headerlink" title="稳定性"></a>稳定性</h4><p>单位处理效应稳定：stable unittreatment value assumption，SUTVA</p><p>个体的处理效应稳定不变，和其他个体是否接受处理无关（大部分情况下是合理的）</p><p>​    </p><h4 id="一致性"><a href="#一致性" class="headerlink" title="一致性"></a>一致性</h4><p>无论是否主动接受到处理，其效应相同（无安慰剂效应）</p><p>​    </p><h4 id="可交换性"><a href="#可交换性" class="headerlink" title="可交换性"></a>可交换性</h4><p>又称可忽略性，ignorability</p><p>给定某组去混因子Z的值，$Y_X$独立于实际接受的处理X</p><p>​    </p><h3 id="结构因果模型"><a href="#结构因果模型" class="headerlink" title="结构因果模型"></a>结构因果模型</h3><p>在学历工资的案例中，如果我们使用结构因果模型，可以得到以下两个方程：</p><script type="math/tex; mode=display">S=65000+2500×EX+5000×ED+U_S\\EX = 10-4×ED+U_{EX}</script><p>第一个式子看起来和直接求S在ED、EX上的回归得到的结果完全一样，但解释的含义不同：一旦我们生命方程是“结构的”，则该方程就反映了我们所认定的因果假设，即EX和ED是S的原因，且不存在$ED=f(EX，S，U_{ED})$，也就是说我们认为ED对EX或S不敏感。</p><p>其中该方程中，<strong>$U_s$和$U_{EX}$为特质因子，每个人不同</strong>，无法观测，但可以根据已观测到的数据推测出来（外展）。</p><p>使用结构模型推导反事实<strong>步骤</strong>：</p><ol><li><strong>外展</strong>：利用关于爱丽丝和其他员工的数据来估计爱丽丝的特质因子（idiosyncratic factors）：$U_s(爱丽丝)$和$U_{EX}(爱丽丝)$。</li><li><strong>干预</strong>：利用do算子改变模型，以反映我们提出的反事实假设，在这个案例中即，假如爱丽丝有大学学位：$ED(爱丽丝)=1$。</li><li><strong>预测</strong>：利用修改后的模型及有关外生变量（exogenous variables）的更新信息$U_S(爱丽丝)$、$U_{EX}(爱丽丝)$和$ED(爱丽丝)$来估算爱丽丝的工资水平。新的工资水平就等于$S_{ED=1}(爱丽丝)$</li></ol><p>​    </p><h3 id="因果关系细分"><a href="#因果关系细分" class="headerlink" title="因果关系细分"></a>因果关系细分</h3><p>必要因；充分因；充要因</p><p>​    </p><h2 id="中介效应"><a href="#中介效应" class="headerlink" title="中介效应"></a>中介效应</h2><p>中介分析为什么重要呢？因为其对我们弄清楚事情发生的真正机制至关重要。比如说人类对坏血病的认识，就是对其中介物的逐步认识。人们很早就知道柑橘类水果可以避免坏血病，但</p><p>​    </p><h3 id="直接、间接效应"><a href="#直接、间接效应" class="headerlink" title="直接、间接效应"></a>直接、间接效应</h3><p>给定：处理X、结果Y、中介物M，但扰动X而保持M恒定时，得到X对Y的<strong>直接效应</strong>。</p><p>保持X恒定，并将M增加到X增加1个单位的情况下M所能达到的量，则我们所看到的Y增量就是X对Y的<strong>间接效应</strong>。</p><p>​    </p><h3 id="受控直接效应"><a href="#受控直接效应" class="headerlink" title="受控直接效应"></a>受控直接效应</h3><p>CDE（controlled direct effect）</p><script type="math/tex; mode=display">CDE(0)=P(Y=1|do(X=1),do(M=0))-P(Y=1|do(X=0),do(M=0))</script><p>同理也有$CDE(1)$。但直接保持中介物恒定，有时是过度对照试验，比如为了判断各个院系是否有性别歧视时，让一个热爱计算机的人去申请考古专业。</p><p>​    </p><h3 id="自然直接效应"><a href="#自然直接效应" class="headerlink" title="自然直接效应"></a>自然直接效应</h3><p>NDE（natural direct effect）：中介物按X=0的取值时，$do(X=1)$的因果效应</p><script type="math/tex; mode=display">NDE=P(Y_{M=M_0}=1|do(X=1))-P(Y_{M=M_0}=1|do(X=0))</script><p>其中$M_0$表示：$do(X=0)$时，中介物M按其自然状态下的取值。</p><p>案例：伯克利大学招生中，如果一个女生将她的性别报告为“男性”，即$do(X=1)$，并让其申请自己想去的院系$M=M_0$的情况下录取概率相对于其将她的性别报告为“女性”的录取概率差。</p><p>​    </p><h3 id="自然间接效应"><a href="#自然间接效应" class="headerlink" title="自然间接效应"></a>自然间接效应</h3><p>NIE（natural indirect effect）：让X=0、中介物按X=1后的取值时的因果效应。</p><script type="math/tex; mode=display">NIE=P(Y_{M=M_1}=1|do(X=0))-P(Y_{M=M_0}=1|do(X=0))</script><p>其中$M_1$表示：$do(X=1)$后，中介物M按其自然状态下的取值。</p><p>案例：作者朋友收养了一只叫黛西的狗子，天天在家里搞破坏，但主人收养了另外三只小猫之后，黛西不再搞破坏了。在小猫被送回收容所的几天后，黛西故态萌发，又开始搞破坏。在小猫在的时候，黛西长时间地被人密切监视，小猫离开后，就不再密切监督它。那么黛西不搞破坏的原因究竟是什么呢？这个问题中，NIE指的是，第一个概率指不引入其他宠物($X=0$)，但将中介物设置为引入其他宠物时会有的值($M=M_1$)，此时黛西的行为有所改善($Y=1$)的概率，第二个概率指正常情况下不引入其他动物时，黛西不搞破坏的概率。</p><img src="/2021/08/08/2021-08-08-%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD-the-book-of-why/image-20210809001807145.png" class="" title="黛西"><p>​    </p><h3 id="总效应"><a href="#总效应" class="headerlink" title="总效应"></a>总效应</h3><script type="math/tex; mode=display">总效应(X=0→X=1)=NDE(X=0→X=1)–NIE(X=1→X=0)</script><p>​    </p><h2 id="我的问题"><a href="#我的问题" class="headerlink" title="我的问题"></a>我的问题</h2><p>记录一下我在学习中想到的问题，以及我的一些想法。</p><p>​     </p><h3 id="随机测试可做因果发现吗"><a href="#随机测试可做因果发现吗" class="headerlink" title="随机测试可做因果发现吗"></a>随机测试可做因果发现吗</h3><p>似乎是可以的，但只能发现处理变量和其他变量之间的因果关系。</p><p>​     </p><h3 id="无法干预时的因果效应"><a href="#无法干预时的因果效应" class="headerlink" title="无法干预时的因果效应"></a>无法干预时的因果效应</h3><p>如果一个变量我们<strong>无法干预，还存在因果关系</strong>吗？这又如何理解？比如年龄→阅读能力，直接<strong>提高年龄会导致阅读能力的提升</strong>吗？</p><p>回答：</p><ul><li><p>这种情况下干预似乎就直接控制了中介因子。但这个是语言的问题，如果大家把年龄看做了自己身份证上的属性，甚至可以直接修改，那就应该把“年龄”替换为“出生年数”，因果关系变成：年龄←出生年数→阅读能力。</p></li><li><p>如果真的是完全没有可能干预呢？<strong>在Robin的潜在因果模型中，不讨论无法干预的因果效应！</strong></p></li></ul><p>​     </p><h3 id="因果图似乎不完备"><a href="#因果图似乎不完备" class="headerlink" title="因果图似乎不完备"></a>因果图似乎不完备</h3><p>在处理交叉影响时，因果图似乎不完备（或有问题）。如，女生更偏好难录取的院系，院长根据居住州和性别共同决定录取结果，只要家乡的男生，其他地方的女生。此时居住州对性别和院系都没有因果关系，即改变居住州，不会影响学生申请的院系或性别。所以因果图应该如下：</p><pre class="mermaid">graph LRA[居住州] --> B((录取结果))C(性别) --> BC(性别) --> D{院系}D{院系} --> B</pre><p>问题1：控制院系后，不存在后门偏倚，但仍然无法得到直接因果效应，因为这切断了性别→ 院系的路径，但性别、院系、州三者之间是交叉影响结果的。</p><p>问题2：这种多阶交叉下的因果效应，看起来是否控制州对判断性别→录取的因果效应没有影响，但实际上结论可能完全相反</p><p>​      </p><p><strong>参考信息</strong></p><p>[1] 《高级计量经济学及Stata应用》,（第二版），P136</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;记录学习《因果之书》的笔记。&lt;/p&gt;
    
    </summary>
    
      <category term="书本学习" scheme="http://wangdongdong122.github.io/categories/%E4%B9%A6%E6%9C%AC%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="因果推断" scheme="http://wangdongdong122.github.io/tags/%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD/"/>
    
      <category term="因果之书" scheme="http://wangdongdong122.github.io/tags/%E5%9B%A0%E6%9E%9C%E4%B9%8B%E4%B9%A6/"/>
    
  </entry>
  
  <entry>
    <title>tf.function的使用</title>
    <link href="http://wangdongdong122.github.io/2021/08/05/2021-08-05-TensorFlow2.0%E4%BD%BF%E7%94%A8/"/>
    <id>http://wangdongdong122.github.io/2021/08/05/2021-08-05-TensorFlow2.0使用/</id>
    <published>2021-08-05T14:02:38.000Z</published>
    <updated>2021-08-29T07:05:51.703Z</updated>
    
    <content type="html"><![CDATA[<p>tf.function的用法</p><span id="more"></span><p>TensorFlow中有三种计算图的：静态计算图，动态计算图，以及Autograph。</p><ul><li>在<strong>TensorFlow1中，采用的是静态计算图</strong>，需要先使用TensorFlow的各种算子创建计算图，然后再开启一个会话Session，显式执行计算图</li><li><strong>TensorFlow2默认采用的是动态计算图</strong>，即每使用一个算子后，该算子会被动态加入到隐含的默认计算图中立即执行得到结果。</li></ul><p>对于动态图的好处显而易见，它方便调试程序，让TensorFlow代码的表现和Python原生代码的表现一样，写起来就像写numpy一样，各种日志打印，控制流全部都是可以使用的，当然，这相对于静态图来讲牺牲了些效率，因为使用动态图会有许多次Python进程和TensorFlow的C++进程之间的通信，而静态计算图构建完成之后几乎全部在TensorFlow内核上使用C++代码执行，效率更高。此外静态图会对计算步骤进行一定的优化，剪去和结果无关的计算步骤。</p><p>如果需要<strong>在TensorFlow2.0中使用静态图，可以使用@tf.function装饰器</strong>将普通Python函数转换成对应的TensorFlow计算图构建代码。运行该函数就相当于在TensorFlow1.0中用Session执行代码，使用tf.function构建静态图的方式<strong>叫做 Autograph</strong>。</p><p><strong>参考资料</strong></p><p><a href="https://zhuanlan.zhihu.com/p/344846077">知乎：如何理解TensorFlow计算图？</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;tf.function的用法&lt;/p&gt;
    
    </summary>
    
      <category term="工具使用" scheme="http://wangdongdong122.github.io/categories/%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/"/>
    
    
      <category term="工具使用" scheme="http://wangdongdong122.github.io/tags/%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/"/>
    
      <category term="TensorFlow2" scheme="http://wangdongdong122.github.io/tags/TensorFlow2/"/>
    
      <category term="非原创" scheme="http://wangdongdong122.github.io/tags/%E9%9D%9E%E5%8E%9F%E5%88%9B/"/>
    
  </entry>
  
  <entry>
    <title>GPU使用中的问题</title>
    <link href="http://wangdongdong122.github.io/2021/08/05/2021-08-05-GPU%E4%BD%BF%E7%94%A8%E9%97%AE%E9%A2%98%E9%9B%86%E9%94%A6/"/>
    <id>http://wangdongdong122.github.io/2021/08/05/2021-08-05-GPU使用问题集锦/</id>
    <published>2021-08-05T05:02:38.000Z</published>
    <updated>2021-08-25T16:38:21.802Z</updated>
    
    <content type="html"><![CDATA[<p>使用GPU训练模型中出现的问题记录</p><span id="more"></span><h2 id="查看GPU状态"><a href="#查看GPU状态" class="headerlink" title="查看GPU状态"></a>查看GPU状态</h2><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># <span class="built_in">cmd</span></span><br><span class="line"><span class="built_in">cd</span> C:\Program Files\NVIDIA Corporation\NVSMI</span><br><span class="line">nvidia-smi.exe</span><br></pre></td></tr></table></figure><h2 id="内存溢出"><a href="#内存溢出" class="headerlink" title="内存溢出"></a>内存溢出</h2><p>pycharm报错：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensorflow.python.framework.errors_impl.InternalError: Blas xGEMM launch failed : a.shape=[1,1,100], b.shape=[1,100,12544], m=1, n=12544, k=100 [Op:MatMul]</span><br></pre></td></tr></table></figure><p>原因是内存溢出</p><p>参考<a href="https://stackoverflow.com/questions/43990046/tensorflow-blas-gemm-launch-failed">stackoverflow</a>，增加两行代码后解决</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">physical_devices = tf.config.list_physical_devices(&#x27;GPU&#x27;) </span><br><span class="line">tf.config.experimental.set_memory_growth(physical_devices[0], True)</span><br></pre></td></tr></table></figure><p>设置使用量(暂时没起到作用)</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">gpus = tf.config.experimental.list_physical_devices(device_type=&#x27;GPU&#x27;)</span><br><span class="line">for gpu in gpus:</span><br><span class="line">    tf.config.experimental.per_process_gpu_memory_fraction = 0.9</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;使用GPU训练模型中出现的问题记录&lt;/p&gt;
    
    </summary>
    
      <category term="环境配置" scheme="http://wangdongdong122.github.io/categories/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"/>
    
    
      <category term="bug" scheme="http://wangdongdong122.github.io/tags/bug/"/>
    
      <category term="环境配置" scheme="http://wangdongdong122.github.io/tags/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"/>
    
      <category term="GPU" scheme="http://wangdongdong122.github.io/tags/GPU/"/>
    
  </entry>
  
  <entry>
    <title>win10配置GPU版TensorFlow.md</title>
    <link href="http://wangdongdong122.github.io/2021/07/30/2021-07-30-win10%E9%85%8D%E7%BD%AEGPU%E7%89%88TensorFlow/"/>
    <id>http://wangdongdong122.github.io/2021/07/30/2021-07-30-win10配置GPU版TensorFlow/</id>
    <published>2021-07-30T05:02:38.000Z</published>
    <updated>2021-08-25T16:38:28.434Z</updated>
    
    <content type="html"><![CDATA[<p>尝试在win10的台式机上，把我的3060用起来。</p><span id="more"></span><h2 id="安装Anaconda"><a href="#安装Anaconda" class="headerlink" title="安装Anaconda"></a>安装Anaconda</h2><p>我已经安装好了anaconda，但还是需要配置环境变量</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">D:\ProgramData\Anaconda3\Scripts</span><br><span class="line">D:\ProgramData\Anaconda3\pkgs\python-3.8.8-hdbf39b2_5</span><br></pre></td></tr></table></figure><h2 id="确认GPU类型"><a href="#确认GPU类型" class="headerlink" title="确认GPU类型"></a>确认GPU类型</h2><p>查看GPU类型</p><p>然后用谷歌搜索“NVIDIA GeForce RTX 3060+SPECIFICATION”，翻到最后在官网查它是否支持CUDA，可以看到其支持的列表，我的既然写了CUDA cores数量应该是支持的。</p><img src="/2021/07/30/2021-07-30-win10%E9%85%8D%E7%BD%AEGPU%E7%89%88TensorFlow/SPECS.png" class="" title="SPECS"><p>在<a href="https://www.tensorflow.org/install/source_windows">TensorFlow官网查询</a>版本和python版本、CUDA等对于关系。</p><h2 id="安装CUDA"><a href="#安装CUDA" class="headerlink" title="安装CUDA"></a>安装CUDA</h2><h3 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h3><p>下载CUDA，去<a href="https://developer.nvidia.com/cuda-toolkit-archive">官网</a>下载并傻瓜式安装。我这里下载的是<a href="https://developer.nvidia.com/cuda-11.0-download-archive">CUDA Toolkit 11.0</a></p><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><img src="/2021/07/30/2021-07-30-win10%E9%85%8D%E7%BD%AEGPU%E7%89%88TensorFlow/cuda-install.png" class="" title="cuda-install"><p>尝试了一下，在cmd中输入<code>nvcc -V</code>，显示cuda版本为V11，安装成功。</p><img src="/2021/07/30/2021-07-30-win10%E9%85%8D%E7%BD%AEGPU%E7%89%88TensorFlow/cmd.png" class="" title="cmd"><h2 id="安装cuDNN"><a href="#安装cuDNN" class="headerlink" title="安装cuDNN"></a>安装cuDNN</h2><p>依然是去<a href="https://developer.nvidia.com/rdp/cudnn-archive">官网</a>下载，下载之后解压，并将文件夹下的文件拷贝到cuda中与之相对应的<a href="C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.0">文件夹</a>下，即可。</p><h2 id="安装tensorflow"><a href="#安装tensorflow" class="headerlink" title="安装tensorflow"></a>安装tensorflow</h2><p>直接安装tensorflow2.5版即可</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install tensorflow==2.5</span><br></pre></td></tr></table></figure><p>安装需要的库</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda install matplotlib</span><br><span class="line">conda install pandas</span><br></pre></td></tr></table></figure><h2 id="测试是否成功"><a href="#测试是否成功" class="headerlink" title="测试是否成功"></a>测试是否成功</h2><p>使用代码测试是否可以调用GPU，运行以下代码。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">tf.config.list_physical_devices(<span class="string">&#x27;GPU&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;GPU&#x27;</span>, tf.test.is_gpu_available())</span><br><span class="line"></span><br><span class="line">a = tf.constant(<span class="number">2.0</span>)</span><br><span class="line">b = tf.constant(<span class="number">4.0</span>)</span><br><span class="line"><span class="built_in">print</span>(a + b)</span><br></pre></td></tr></table></figure><p>并没有成功，因为很多dll文件，tensorflow找不到，是环境变量问题。根据报错提示，去<a href="https://www.tensorflow.org/install/gpu">官网</a>看了一下。让我将这些路径加到PATH中。其中cuDNN需要解压后拷贝一下，我这里按他的提示，拷贝到<code>C:\tools\cuda</code>，并增加到系统路径中。</p><p>都加了之后确实大部分dll文件都能找到了，但下面这个<code>cusolver64_11.dll</code>还是找不到，去cuda的文件夹（<code>C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.0</code>）中搜了一下，发现有个<code>cusolver64_10.dll</code>，但没有<code>cusolver64_11.dll</code>。于是我直接粗暴地将这个文件复制一下，新文件命名为<code>cusolver64_11.dll</code>，再测试一下，成功了。</p><img src="/2021/07/30/2021-07-30-win10%E9%85%8D%E7%BD%AEGPU%E7%89%88TensorFlow/test-gpu.png" class="" title="test-gpu">]]></content>
    
    <summary type="html">
    
      &lt;p&gt;尝试在win10的台式机上，把我的3060用起来。&lt;/p&gt;
    
    </summary>
    
      <category term="环境配置" scheme="http://wangdongdong122.github.io/categories/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"/>
    
    
      <category term="bug" scheme="http://wangdongdong122.github.io/tags/bug/"/>
    
      <category term="环境配置" scheme="http://wangdongdong122.github.io/tags/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"/>
    
  </entry>
  
  <entry>
    <title>强化学习简介</title>
    <link href="http://wangdongdong122.github.io/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/"/>
    <id>http://wangdongdong122.github.io/2021/07/29/2021-07-29-强化学习简介/</id>
    <published>2021-07-29T13:56:44.000Z</published>
    <updated>2021-11-18T14:05:28.923Z</updated>
    
    <content type="html"><![CDATA[<p>本篇为学习强化学习笔记，主要是学习李宏毅老师的<a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML20.html">课程</a>的笔记。内容是强化学习的简单介绍，了解一下框架。</p><span id="more"></span><h2 id="场景描述"><a href="#场景描述" class="headerlink" title="场景描述"></a>场景描述</h2><p>强化学习的基本场景是Agent和Environment之间交互，Environment给出一个state，Agent看到了这个state（等同于observation，更好理解）并根据这个state做出某个Action，这个Action会影响Environment，state会发生改变，同时Environment会反馈一个Reward。</p><img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Scenario-of-Reinforcement-Learning.png" class="" title="Scenario-of-Reinforcement-Learning"><p>强化学习的目的，就是找到一个Actor或者叫Policy，让Reward最大，其输入使observation，输出是Action。</p><img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Looking-for-a-Function.png" class="" title="Looking-for-a-Function"><h3 id="一些基本问题"><a href="#一些基本问题" class="headerlink" title="一些基本问题"></a>一些基本问题</h3><ol><li>为什么不直接学习人类的behavior，将问题转化为监督学习？<ul><li>机器一般没有办法完全模仿人类，此时机器很难知道哪些行为重要，哪些行为不重要。</li><li>即使机器可以完全拷贝人类，也没办法通过这种方式超过人类。</li></ul></li></ol><p>​    </p><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><p>以玩游戏为例。</p><img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Playing-Video-Game.png" class="" title="Playing-Video-Game"><p>有以下概念：</p><ul><li>state: observation</li><li>玩一局游戏称为一个episode</li></ul><p>​    </p><h2 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h2><h3 id="Model-based-amp-Model-free"><a href="#Model-based-amp-Model-free" class="headerlink" title="Model-based &amp; Model-free"></a>Model-based &amp; Model-free</h3><p>根据是否需要建模来模拟环境，可以分为Model-free Approach（不需要对环境建模）和Model-based Approach，前者不需要对环境建模，后者需要。</p><p>Model-free Approach中，可以直接学习什么Actor(policy)最好，即Policy-based。也可以学习一个评估函数，来判断不同决策的价值是多少，即可得到每一步最好的Action。如果将两者结合起来则可以综合两方面的优势。</p><img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/RL-Outline.png" class="" title="RL-Outline"><p>​    </p><h3 id="On-Policy-amp-Off-Policy"><a href="#On-Policy-amp-Off-Policy" class="headerlink" title="On Policy &amp; Off Policy"></a>On Policy &amp; Off Policy</h3><ul><li><p>On-policy: 需要学习的Actor和与环境互动的Actor一样.</p></li><li><p>Off-policy: 需要学习的Actor和与环境互动的Actor不一样.</p></li></ul><img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/image-20210830234507908.png" class="" title="image-20210830234507908"><p>off-policy的好处：可以通过一批数据学习多次Actor。Off-policy需要基于<a href="/2021/09/18/2021-09-18-%E9%87%8D%E8%A6%81%E9%87%87%E6%A0%B7/" title="重要采样">重要采样</a>。 </p><p>​    </p><h2 id="Policy-based-Approach"><a href="#Policy-based-Approach" class="headerlink" title="Policy-based Approach"></a>Policy-based Approach</h2><h3 id="问题介绍"><a href="#问题介绍" class="headerlink" title="问题介绍"></a>问题介绍</h3><p>前面介绍了Policy-based Approach 就是要学习一个Actor，它会根据看到的情况来判断下一步动作，即输入是state（等同于observation），输出是Action。将神经网络的模型作为一个actor。以上面的游戏为例，其输入就是当前游戏画面，输出是需要进行的游戏操作，用$\pi_{\theta}(s)$来表示。</p><img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Neural-network-as-Actor.png" class="" title="Neural-network-as-Actor"><p>要训练这个神经网络模型，关键的问题是，它的训练目标是什么？强化学习的目标就是得到最多的奖励，所以应该用一个Actor所能得到的期望奖励作为评估其好坏的依据，可以通过蒙特卡洛法，多次实验用平均值作为期望值的矩估计。展开一个游戏过程来看，可以将游戏得分作为奖励，同一个Actor玩了很多局游戏，则用每局游戏奖励的平均值<strong>$\overline R_{\theta}$作为评估Actor $\pi_{\theta}(s)$好坏的依据</strong>。所以我们的只要将$\overline R_{\theta}$用$\theta$表示，最大化$\overline R_{\theta}$就可以。</p><img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Goodness-of-Actor.png" class="" title="Goodness-of-Actor"><p>​    </p><h3 id="Policy-Gradient"><a href="#Policy-Gradient" class="headerlink" title="Policy Gradient"></a>Policy Gradient</h3><p>将上面的问题抽象一下，我们的目标就是找最优$\theta$，最大化$\overline R_{\theta}$。</p><img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Problem-statement.png" class="" title="Problem-statement"><p><strong>梯度怎么求呢？</strong>这里$\overline R_{\theta}$是由episode的奖励$R(\tau)$和其概率$p(\tau|\theta)$决定，前者和Actor无关。这里对求导做了个变换，目的是为了把概率提出来，这样就可以表示成期望，然后利用蒙特卡洛来用均值代替。</p><img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/policy-gredient-define.png" class="" title="policy-gredient-define"><p>注（<strong>重要采样</strong>）：这里通过乘$p(\tau|\theta)$后再除$p(\tau|\theta)$可以让该项可以表示成期望，但同时也将原来的$\nabla p(\tau|\theta)$变成了$\nabla log\ p(\tau|\theta)$，这会产生什么影响呢？$\nabla log\ p(\tau|\theta)$后面的求导中，还是会变成$1/p(\tau|\theta)$乘$\nabla p(\tau|\theta)$。原本更新$\theta$，按照$p(\tau|\theta)$的梯度反向传导，再用$R(\tau)$作为权重就可以了，前面这个$1/p(\tau|\theta)$是干啥的呢？是为了<strong>平衡样本偏差</strong>，本来应该完全按照$\tau$所带来的奖励决定其最终的概率，但现在样本是按照$p(\tau|\theta)$采样出来的，所以可能导致一个好的$\tau$在样本里极少出现，而未能对$\theta$的优化产生多大影响，所以$1/p(\tau|\theta)$就刚好是来解决这个问题。即：乘$p(\tau|\theta)$导致了偏差问题，再除$p(\tau|\theta)$有可以解决该问题。</p><p>$\nabla log\ p(\tau|\theta)$怎么求呢？将一局游戏(episode)拆开，可以表示为$\tau =\{s_1,a_1,r_1,s_2,a_2,r_2,···,s_T,a_T,r_T \}$，其似然为给定actor时产生该$\tau$的概率：$P(\tau|\theta)$。对其展开，整个游戏过程可以看做：给定了一个初始的状态$s_1$之后，Actor根据其观测到状态$s_t$和自身参数$\theta$产生一个动作$a_t$，环境根据状态$s_t$和Actore做出的动作$a_t$给出奖励$r_t$和下一各状态$s_{t+1}$。</p><p>$p(s_1)$表示初始状态是$s_1$的概率，由环境决定，不受Actor影响；$p(a_t|s_t,\theta)$表示其观测到状态$s_t$和自身参数$\theta$产生一个动作$a_t$的概率；$p(r_t,s_{t+1}|s_t,a_t)$表示Actor在$s_t$状态下做出动作$a_t$之后，得到的这局游戏中实际观测到的奖励$r_t$和下一个状态$s_{t+1}$的概率，这个概率也由环境决定，不受actor影响。所以<strong>似然中，只有$p(a_t|s_t,\theta)$与要优化的Actor相关</strong>。</p><p>将$p(\tau|\theta)$的计算公式代入$\nabla log\ p(\tau|\theta)$，其中只有$p(a_t|s_t,\theta)$与Actor相关。</p><img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/gradient-of-log-p-1627997683506.png" class="" title="gradient-of-log-p"><p>代入$\nabla\overline R_{\theta}$中，得到更新公式</p><img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/gradient-of-R.png" class="" title="gradient-of-R"><p>​    </p><h4 id="总结一下"><a href="#总结一下" class="headerlink" title="总结一下"></a>总结一下</h4><p>Actor的学习，就是要解决以下问题，我这里用期望代替均值，感觉表述更加准确。</p><script type="math/tex; mode=display">\theta ^* = arg\ \underset{\theta}{max}\ E[R_{\theta}]</script><p>其中$E[R_{\theta}]$可以表示为</p><script type="math/tex; mode=display">E[R_{\theta}] = \sum_{\tau} R(\tau) P(\tau|\theta)</script><p>其导数为</p><script type="math/tex; mode=display">\begin{eqnarray*}\nabla E[R_{\theta}] &=&  \nabla\sum_{\tau} R(\tau) P(\tau|\theta) \\\\&=& \sum_{\tau} R(\tau) \nabla P(\tau|\theta)\tag{$R(\tau)$与$\theta$无关}\\\\&=& \sum_{\tau} R(\tau)P(\tau|\theta) \frac{\nabla P(\tau|\theta)}{P(\tau|\theta)}\tag{增加$P(\tau|\theta)$凑为期望}\\\\&=& \sum_{\tau}P(\tau|\theta) R(\tau) \nabla log P(\tau|\theta)\tag{换为log导数}\\\\&\approx& \frac{1}{N}\sum^N_{n=1}R(\tau^n)\nabla log P(\tau^n|\theta)\tag{期望用均值代替}\end{eqnarray*}</script><p>其中$P(\tau|\theta)$可以展开其生成过程，并对其展开，并求解一下</p><script type="math/tex; mode=display">\begin{eqnarray*}&&P(\tau|\theta) = p(s_1)\prod ^T_{t=1}p(a_t|s_t,\theta)p(r_t,s_{t+1}|s_t,a_t)\\\\&&log\ P(\tau|\theta) = log\ p(s_1) + \sum ^T_{t=1}\left [log\ p(a_t|s_t,\theta) + log \ p(r_t,s_{t+1}|s_t,a_t)\right ]\\\\&&\nabla log P(\tau|\theta) = \sum ^T_{t=1}\nabla log p(a_t|s_t,\theta)\end{eqnarray*}</script><p>因此</p><script type="math/tex; mode=display">\begin{eqnarray*}\color{red}\nabla E[R_{\theta}] &\color{red}\approx& \frac{1}{N}\sum^N_{n=1}R(\tau^n)\nabla log P(\tau^n|\theta)\\\\&=& \color{red}\frac{1}{N}\sum^N_{n=1}\sum ^{T_n}_{t=1}R(\tau^n)\nabla log p(a^n_t|s^n_t,\theta)\tag{仅$p(a_t|s_t,\theta)$与$\theta$相关}\\\\\end{eqnarray*}</script><p>​    </p><h4 id="视作多分类"><a href="#视作多分类" class="headerlink" title="视作多分类"></a>视作多分类</h4><p>这其实和多分类问题相同。如果将学习Actor看做一个多分类问题，数据就是在每局游戏中Actor实际根据状态做出的动作。</p><img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/classify-data.png" class="" title="classify-data"><p>其中每个$(s,a)$对中，$s$作为输入，$a$作为输出，对应episode的$R(\tau)$作为权重。</p><p>多分类交叉熵为</p><script type="math/tex; mode=display">J_{CE}(\theta)  = -\frac1N \sum^{N}_{i=1}log(\hat y_{iy_i})</script><img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/classify-task.png" class="" title="classify-task"><p>此处的$y_{i}$指的就是对应的$a$，$\hat y_{iy_i}$即$p(a^n_t|s^n_t,\theta)$，总样本数量为$N×T$个，权重为$R(\tau)$，则</p><script type="math/tex; mode=display">J(\theta) =-\frac{1}{N×T}\sum^N_{n=1}\sum ^{T}_{t=1}R(\tau^n) log\ p(a^n_t|s^n_t,\theta)</script><p>这里全面的求平均是除$N×T$（实际上这样表示不准确，因为每个episode不一定都是T步），和上面Actor的公式有一点不一样，因为<strong>Actor是最大化</strong>的一次episode的<strong>联合概率</strong>，<strong>多分类是拆解成动作</strong>，最大化每个动作的概率。</p><p>​    </p><h4 id="Tips"><a href="#Tips" class="headerlink" title="Tips"></a>Tips</h4><h5 id="BaseLine"><a href="#BaseLine" class="headerlink" title="BaseLine"></a>BaseLine</h5><p>BaseLine就是指，将每个$(s,a)$对的权重由整个episode的奖励，改为这个<strong>奖励相对于平均奖励的差值（增益）</strong>，从而避免模型未能采样全带来的错误优化，一定程度上降低训练难度/对样本量要求：</p><script type="math/tex; mode=display">\color{red}\begin{eqnarray*}\nabla E[R_{\theta}] &\approx&\frac{1}{N}\sum^N_{n=1}\sum ^{T_n}_{t=1}\left(R(\tau^n) -b\right)\nabla log\ p(a^n_t|s^n_t,\theta)\end{eqnarray*}</script><p>在上面的案例中，奖励$R(\tau)$永远是正的，在数据量非常大的时候这样也没什么问题，因为Actor一次做出各种动作的概率和为1，即使较差的动作对应的权重$R(\tau)$也是正的，但应该没有较好的动作权重高，因此最终差的动作概率还是会下降。但实际训练中，数据是采样得到的，有可能真正好的动作没有采样到（下图的a），这样就会导致优秀的动作a的概率下降，差一点的b,c概率上升。解决这个问题，可以给奖励，加一个baseline，低于base line奖励为负。</p><img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Add-a-Baseline.png" class="" title="Add-a-Baseline"><p>实际操作时，可以不断将最新的$E[R(\tau)]$记录下来作为$b$。</p><p>​    </p><h5 id="Assign-Suitable-Credit"><a href="#Assign-Suitable-Credit" class="headerlink" title="Assign Suitable Credit"></a>Assign Suitable Credit</h5><p>Assign Suitable Credit就是将每个$(s,a)$对的权重由整个episode的奖励增益，改进为<strong>该action执行之后</strong>，<strong>时间衰减加权</strong>的累积奖励增益，从而使权重更加准确地表示action的影响：</p><script type="math/tex; mode=display">\color{red}\begin{eqnarray*}\nabla E[R_{\theta}] &\approx&\frac{1}{N}\sum^N_{n=1}\sum ^{T_n}_{t=1}\left(\sum^{T_n}_{t^{\prime}=t} \gamma^{t^{\prime}-t} r^n_{t^{\prime}} -b\right)\nabla log\ p(a^n_t|s^n_t,\theta)\end{eqnarray*}</script><p>前面的方法，将episode整体的奖励$R(\tau)$，作为每个$(s,a)$对的权重，这点可以改进为<strong>$(s,a)$对执行之后得到的奖励作为其权重</strong>。比如下图中，$(s_b,a_2)$操作之后，可能只能带来减2分，但因为第1个动作的奖励比较高(5)，才导致整体的奖励是正向的。这个问题在数据量足够大时，也可以解决，模型总会看到第一个动作没那么高时，$(s_b,a_2)$操作带来的副作用（右图）。</p><img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Assign-Suitable-Credit1.png" class="" title="Assign-Suitable-Credit1"><p>更进一步，$(s,a)$对的权重，由其之后的奖励<strong>加上时间衰减</strong>（$\gamma$）之后作为权重。</p><img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Assign-Suitable-Credit-r.png" class="" title="Assign-Suitable-Credit-r"><p>从这个过程可以看到每个$(s,a)$对的权重其实大有文章，我们直接用$A^{\theta}(s_t,a_t)$来表示，计算的是在$s_t$采取$a_t$时，其权重有多大。其上标$\theta$表示用对应Actor和环境做互动后得到的轨迹，不同的Actor对于的平均水平不同，所以A必然和$\theta$相关。其含义为：在$s_t$采取$a_t$，<strong>相对于</strong>其他可能的action，有多好。通常$A^{\theta}(s_t,a_t)$可以直接由一个网络来学习。</p><img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Assign-Suitable-Credit-A.png" class="" title="Assign-Suitable-Credit-A"><p>一个典型的A是$Q^{\pi}(s,a)-V^{\pi}(s)$，表示采取当前 $a$ 后的奖励比Actor自己操作的奖励之差。</p><p>​    </p><h3 id="Proximal-Policy-Optimization-PPO"><a href="#Proximal-Policy-Optimization-PPO" class="headerlink" title="Proximal Policy Optimization(PPO)"></a>Proximal Policy Optimization(PPO)</h3><p>PPO是OpenAI默认的强化学习算法，其地位可想而知。PPO是policy gradient的一个变形，所谓Proximal是近邻的意思，指off-policy中，和环境互动的policy要和更新的policy相近。</p><img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/PPO-prc.png" class="" title="PPO-prc"><p>​    </p><h4 id="on-policy-→-off-policy"><a href="#on-policy-→-off-policy" class="headerlink" title="on-policy → off-policy"></a>on-policy → off-policy</h4><p>on-policy中，当我们更新一个Actor（$\pi_\theta$）时，会用这个Actor去和环境交互采样得到数据，利用下面的公式优化该Actor。Actor一旦更新，就需要用新的Actor再次采样数据，循环往复。</p><script type="math/tex; mode=display">\begin{eqnarray*}\nabla E[R_{\theta}] =E_{\tau \sim p_{\theta}(\tau)}\left [R(\tau)\ \nabla log\ p_\theta(\tau)\right ]\end{eqnarray*}</script><p>变成off-policy之后，我们就可以利用某个Actor（$\pi_{\theta^\prime }$）和环境交互的数据来优化当前的Actor（$\pi_\theta$），利用<a href="/2021/09/18/2021-09-18-%E9%87%8D%E8%A6%81%E9%87%87%E6%A0%B7/" title="重要采样">重要采样</a>：</p><script type="math/tex; mode=display">\begin{eqnarray*}\nabla E[R_{\theta}] =E_{\tau \sim p_{\theta^\prime}(\tau)}\left [\frac{p_\theta(\tau)}{p_{\theta^\prime}(\tau)}R(\tau)\ \nabla log\ p_\theta(\tau)\right ]\end{eqnarray*}</script><p>使用Advantage Function：$A^{\theta}(s_t,a_t)$作为每步的权重时，将整个episode($\tau$)拆开为各个($s_t,a_t$)pair，则可表示为：</p><script type="math/tex; mode=display">\begin{eqnarray*}\nabla E[R_{\theta}]&=&E_{(s_t,a_t)\sim \pi_{\theta}}\left [A^{\theta}(s_t,a_t)\nabla log\ p_{\theta}(a^n_t|s^n_t)\right ]\\\\&=&E_{(s_t,a_t)\sim \pi_{\theta^\prime}}\left [\frac{P_{\theta}(s_t,a_t)}{P_{\theta^\prime}(s_t,a_t)}A^{\theta^\prime}(s_t,a_t)\nabla log\ p_{\theta}(a^n_t|s^n_t)\right ]\\\\&=&E_{(s_t,a_t)\sim \pi_{\theta^\prime}}\left [\frac{P_{\theta}(a_t|s_t)}{P_{\theta^\prime}(a_t|s_t)}\frac{P_{\theta}(s_t)}{P_{\theta^\prime}(s_t)}A^{\theta^\prime}(s_t,a_t)\nabla log\ p_{\theta}(a^n_t|s^n_t)\right ]\\\\&\approx& E_{(s_t,a_t)\sim \pi_{\theta^\prime}}\left [\frac{P_{\theta}(a_t|s_t)}{P_{\theta^\prime}(a_t|s_t)}A^{\theta^\prime}(s_t,a_t)\nabla log\ p_{\theta}(a^n_t|s^n_t)\right ]\end{eqnarray*}</script><p>这里第二步中，$A^{\theta}(s_t,a_t)$变成了$A^{\theta^\prime}(s_t,a_t)$，是因为这一项是从采样数据（由$\theta^\prime$决定）中算出来的，常用的做法是真实奖励与平均奖励的差，这个平均奖励是从采样数据中算出来的。</p><p>最后一步，约等于是因为将$\frac{P_{\theta}(s_t)}{P_{\theta^\prime}(s_t)}$假设为1了，即出现某个observation的概率和用哪个Actor无关，这样假设一方面是很多场景下确实成立，另一方面是由于observation出现的概率很难计算。上面这个式子是梯度计算公式，根据这个公式可以反推其对应的代价函数，根据公式：$\nabla f(x)=f(x)\nabla log\ f(x)$ 可知上式对应的<strong>代价函数</strong>为：</p><script type="math/tex; mode=display">J^{\theta^\prime}(\theta) =E_{(s_t,a_t)\sim \pi_{\theta^\prime}}\left [\frac{P_{\theta}(a_t|s_t)}{P_{\theta^\prime}(a_t|s_t)}A^{\theta^\prime}(s_t,a_t)\right ]</script><p>​    </p><h4 id="Add-Constraint"><a href="#Add-Constraint" class="headerlink" title="Add Constraint"></a>Add Constraint</h4><p>在重要采样中提到过，当两个分布差异过大时，由重要采样得出的结果方差会很大。在off-policy中，和环境互动的Actor不能和我们要学习的Actor差异太多，<strong>即$\pi_{\theta^\prime}$和$\pi_{\theta}$不能差异太大</strong>。</p><p>PPO(Proximal Policy Optimization)将$\pi_{\theta}$和$\pi_{\theta^\prime}$的差异加到代价函数中，来保证$\pi_{\theta}$和$\pi_{\theta^\prime}$不会相差太大：</p><script type="math/tex; mode=display">\begin{eqnarray*}& J^{\theta^\prime}_{PPO}(\theta) = J^{\theta^\prime}(\theta)-\beta KL(\theta,\theta^\prime)\\\\& J^{\theta^\prime}(\theta) = E_{(s_t,a_t)\sim \pi_{\theta^\prime}}\left [\frac{P_{\theta}(a_t|s_t)}{P_{\theta^\prime}(a_t|s_t)}A^{\theta^\prime}(s_t,a_t)\right ]\end{eqnarray*}</script><p>TRPO (Trust Region Policy Optimization)则直接将$\pi_{\theta}$和$\pi_{\theta^\prime}$的差异小于设定阈值作为代价函数的不等式约束：</p><script type="math/tex; mode=display">\begin{eqnarray*}& J^{\theta^\prime}_{TRPO}(\theta) = E_{(s_t,a_t)\sim \pi_{\theta^\prime}}\left [\frac{P_{\theta}(a_t|s_t)}{P_{\theta^\prime}(a_t|s_t)}A^{\theta^\prime}(s_t,a_t)\right ]\\\\& KL(\theta,\theta^\prime) < \delta\end{eqnarray*}</script><p>​    </p><h4 id="PPO-algorithm"><a href="#PPO-algorithm" class="headerlink" title="PPO algorithm"></a>PPO algorithm</h4><p>按照刚才的思路，可以给出PPO算法的步骤。值得注意的是，① 每次iteration，$\theta$会在近邻约束下收敛，随后使用新的$\theta$重新采样得到新的数据用于训练；② 此处的$KL(\theta,\theta^k)$，指的是其对应行为之间的KL散度，并非真的是参数之间的KL散度，实际计算时，是用采样数据s输入两个Actor，得到对应action的分布，从而计算KL散度；③ 算法中加入了自适应的权重，适用过程很直观，KL项大于预设值就增大其权重，反之则减小。</p><img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/PPO-algorithm.png" class="" title="PPO-algorithm"><p>​    </p><h4 id="PPO2-algorithm"><a href="#PPO2-algorithm" class="headerlink" title="PPO2 algorithm"></a>PPO2 algorithm</h4><p>PP2算法则直接将过大或者过小的权重截断，来限制重要采样结果的方差：</p><script type="math/tex; mode=display">\begin{eqnarray*}J^{\theta^\prime}_{PPO2}(\theta) &\approx& min \left (\frac{P_{\theta}(a_t|s_t)}{P_{\theta^k}(a_t|s_t)}A^{\theta^k}(s_t,a_t),clip \left(\frac{P_{\theta}(a_t|s_t)}{P_{\theta^k}(a_t|s_t)},1-\epsilon,1+\epsilon\right )A^{\theta^k}(s_t,a_t)\right )\end{eqnarray*}</script><p>截断的方式如下图所示</p><img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/PPO2-algorithm.png" class="" title="PPO2-algorithm"><p>这一点其实很像倾向性得分中，为了减小方差，而采用的各种重叠区域的选取方法。2去</p><p>​    </p><h2 id="Value-based-Approach"><a href="#Value-based-Approach" class="headerlink" title="Value-based Approach"></a>Value-based Approach</h2><p>Value-based方法指学习一个Critic来判断Actor的好坏。Critic不决定Action，但可以从Critic得到一个最优的Actor。Critic分为两种：</p><ol><li>$V^{\pi}(s)$ ：在看见状态$s$后，使用Actor $\pi$产生的累积奖励的期望值</li><li>$Q^{\pi}(s,a)$：在看见状态$s$后，先采取Action $a$，然后使用Actor $\pi$产生的累积奖励的期望值</li></ol><blockquote><p>此处的$Q$指强制采取Action $a$，和因果推断中的干预是一个意思。</p></blockquote><p>比如说阿光在下棋的时候，旁边佐为会告诉他，现在的他能驾驭大马步飞了，下大马步飞是好棋，以前太弱不能这么下，即相同的State的情况下，不同的Actor的V是不一样的。</p><img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/sai2-1635782987756.png" class="" title="sai2"><p>​    </p><h3 id="estimate-V-pi-s"><a href="#estimate-V-pi-s" class="headerlink" title="estimate $V^{\pi}(s)$"></a>estimate $V^{\pi}(s)$</h3><p>估计$V^{\pi}(s)$ 有两种方法：<strong>蒙特卡洛法</strong>和<strong>差分法</strong></p><p>​    </p><h4 id="Monte-Carlo-based-approach"><a href="#Monte-Carlo-based-approach" class="headerlink" title="Monte-Carlo based approach"></a>Monte-Carlo based approach</h4><p>蒙特卡洛法就是让Actor $\pi$玩游戏，产生episode，来训练critic。在看见状态$s_a$之后，直到游戏结束产生的累积奖励为$G_a$。这里的$G$和之前每个episode的总奖赏$R$不同，$G$表示转态$s_a$之后的累计奖励。</p><img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Monte-Carlo-based-approach.png" class="" title="Monte-Carlo-based-approach"><p>​    </p><h4 id="Temporal-difference-approach"><a href="#Temporal-difference-approach" class="headerlink" title="Temporal-difference approach"></a>Temporal-difference approach</h4><p>差分法，策略$\pi$下产生的episode，相邻状态$s_t$和$s_{t+1}$下的$V$值相差$r_t$，可以以此为目标训练$V^{\pi}(s)$ 。</p><img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Temporal-difference-approach.png" class="" title="Temporal-difference-approach"><p>​    </p><h4 id="MC-vs-TD"><a href="#MC-vs-TD" class="headerlink" title="MC vs TD"></a>MC vs TD</h4><p>第一点是目标值的方差和偏差的不同。MC法直接预测的是累积奖励$G_a$，方差会更大。而TD法，标签值是单步的$r$，方差相对小，但其用到了$V^{\pi}(s_{t+1})$，这个值一定准。</p><img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/MC-vs-TD-1.png" class="" title="MC-vs-TD-1"><p>第二点是基于的假设不同。在下面的数据中，一共有8个episode，不管我们用哪个方法，很明显都能得到$V^{\pi}(s_{b})=3/4$，那么$V^{\pi}(s_{a})$应该等于多少呢。按照MC方法，$V^{\pi}(s_{a})=0$（只有一条数据）；按照TD方法，$V^{\pi}(s_{a})=V^{\pi}(s_{b})+0=3/4$。MC的假设下状态前后之间可能有相互影响。</p><img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/MC-vs-TD-2.png" class="" title="MC-vs-TD-2"><p>第三点是TD方法不需要一整个episode结束（游戏结束）才能开始训练，只要知道其中一段就可以开始训练。</p><p>​    </p><p><strong>问题</strong>：</p><ol><li>看起来针对不同的Actor $\pi$有一个模型$V^{\pi}(s)$， Actor如果更新了，V要重新训练吗？</li><li>得到这个V之后<strong>如何使用</strong>？<ol><li>可以用于AC算法</li></ol></li><li>MC和TD之间的假设差异如何理解？</li></ol><p>​    </p><h3 id="Q-Learning"><a href="#Q-Learning" class="headerlink" title="Q-Learning"></a>Q-Learning</h3><p>Q-Learning学习的目标是$Q^{\pi}(s,a)$，一样可以用MC和TD方法。我们可以通过Q-Learning找到一个好的Actor。</p><img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/use-Q-learning-1632410014443.png" class="" title="use-Q-learning"><p>当学习到某个Actor $\pi$ 对应的$Q^{\pi}(s,a)$之后，则根据这个$Q^{\pi}(s,a)$来决策作为新的Actor $\pi^\prime$ 会比$\pi$ 更好</p><img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Q-learning-Better-Actor.png" class="" title="Q-learning-Better-Actor"><p>证明：先证$V^{\pi}(s) \leq Q^{\pi}(s,\pi ^\prime (s))$，然后逐项展开，递推得到$V^{\pi}(s) \leq V^{\pi^ \prime}(s)$的结论。</p><img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Q-learning-Better-Actor-proof-1632410694932.png" class="" title="Q-learning-Better-Actor-proof"><p>​    </p><h4 id="TD-approach"><a href="#TD-approach" class="headerlink" title="TD approach"></a>TD approach</h4><p>通过TD的方式训练，此时会有一个问题，目标值：$r_t+Q^{\pi}(s_{t+1},\pi(s_{t+1}))$也会变化，会导致很难训练，因此可以将右边的网络（Target Network）<strong>固定</strong>，在左边网络训练多轮之后，更新一次右边的Target Network。</p><img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Target-Network.png" class="" title="Target-Network"><p>基于已有的Q函数可以产出action，即$a= arg\ \underset{a}{max}\ Q(s,a)$，这本身就是一个policy。但这样不利于收集数据，给定s后，action是确定性的，不会<strong>探索</strong>其他的Action。</p><img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Never-explore.png" class="" title="Never-explore"><p>有几种方法解决这个问题，第一种<strong>Epsilon Greedy</strong>：就是有$\varepsilon$的概率随机给出一个Action；第二种<strong>Boltzmann Exploration</strong>：就是根据每个动作的Q值，利用softmax给出每个动作出现的概率。</p><img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Exploration.png" class="" title="Exploration"><p>除此之外还可以通过<strong>Replay Buffer</strong>，用off-policy的方式更新Q函数。让Buffer中存入一定熟练的交互数据，当buffer中的数据满了，去掉最老的数据。当学习Q函数时，从buffer中采样数据。</p><img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Replay-Buffer.png" class="" title="Replay-Buffer"><p>Replay Buffer的方法是异策略，但此时产生数据的actor和学习的Q对应的$\pi$不同并不会导致问题。原因应该是此处是差分方法，且Q函数中的a本身就是强制设定的，和$\pi$无关。</p><p>疑问：</p><ol><li>TD方法，特别是Replay Buffer方法学习出来的Q对应的$\pi$到底是啥呢？</li></ol><p>​    </p><h4 id="Typical-Q-Learning-Algorithm"><a href="#Typical-Q-Learning-Algorithm" class="headerlink" title="Typical Q-Learning Algorithm"></a>Typical Q-Learning Algorithm</h4><ul><li>初始化Q-function，target Q-function（产生训练目标的Q） $\hat Q=Q$</li><li>每个episode<ul><li>每个时间步骤 $t$：<ul><li>给定状态$s_t$，根据$Q$（epsilon greedy），采取动作$a$</li><li>获得奖励$r_t$，和新的状态$s_{t+1}$</li><li>将$(s_t,a_t,r_t,s_{t+1})$存入buffer</li><li>从buffer中采样出样本$(s_i,a_i,r_i,s_{i+1})$（通常是采一个batch）</li><li>设定目标$y=r_i+ \underset{a}{max}\ \hat Q(s_{i+1},a)$</li><li>更新Q的参数，使$Q(s_i,a_i)$接近$y$</li><li>每隔C步，设置 $\hat Q=Q$</li></ul></li></ul></li></ul><p>​    </p><h4 id="Tips-1"><a href="#Tips-1" class="headerlink" title="Tips"></a>Tips</h4><h5 id="Double-DQN"><a href="#Double-DQN" class="headerlink" title="Double DQN"></a>Double DQN</h5><p>Double DQN通过增加一个Q函数作为target Q-function，来解决DQN对Q值高估的问题。</p><p>基础的Q-Learning算法一般都会对Q值高估，这是因为Q-Learning中的Q对应的policy就是按Q-function（target function）选取最大Q值对应的a产生的。因此在每次学习中，会选取最高估的$Q(s_{t+1},a)$作为回归的目标，这就会导致Q值不断被高估。</p><img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/over-estimated.png" class="" title="over-estimated"><p>解决这个问题的办法，可以用$Q$来决定动作a，但用另外一个$Q^{\prime}$作为target Q-function，此时$Q^{\prime}$是否高估与$Q$无关。</p><img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Q_prime.png" class="" title="Q_prime"><p>效果如下：</p><img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/over-estimated-vs.png" class="" title="over-estimated-vs"><p><a href="https://proceedings.neurips.cc/paper/2010/file/091d584fced301b442654dd8c23b3fc9-Paper.pdf">Hado V. Hasselt, “Double Q-learning”, NIPS 2010</a></p><p>​    </p><h5 id="Dueling-DQN"><a href="#Dueling-DQN" class="headerlink" title="Dueling DQN"></a>Dueling DQN</h5><p>Dueling DQN通过将Q值拆为V值和$A(s,a)$（一般A会加和为1的正则化），使Q-function更具推理能力，提高学习效率。</p><img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Dueling-DQN-1635808866948.png" class="" title="Dueling-DQN"><p>为什么这样做可以提高学习效率？当某个state下，值采样到一部分action下的结果时，如下图的4和0，由于A被加了正则化，此时模型更倾向于更新V来拟合4和0，这样一来，即使更新过程中没有采样到最后一个action的数据也能对其Q值有效更新。</p><img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/dueling-table.png" class="" title="dueling-table"><p>要想达到刚才所讲的效果，必不可少的是对A做限制，比如说使其每列为0均值，这有点像layer normalization。</p><p><a href="http://proceedings.mlr.press/v48/wangf16.pdf">Ziyu Wang, Tom Schaul, Matteo Hessel, Hado van Hasselt, Marc Lanctot, Nando de Freitas, “Dueling Network Architectures for Deep Reinforcement Learning”, arXiv preprint, 2015</a></p><p>​    </p><h5 id="Prioritized-Reply"><a href="#Prioritized-Reply" class="headerlink" title="Prioritized Reply"></a>Prioritized Reply</h5><p>Prioritized Reply通过将基础Q-Learning中在Experience Buffer中均匀采样改为对预测效果差的数据优先采样，来提升效果。此时参数的更新过程也会有所调整，不做展开。</p><img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Prioritized-Reply.png" class="" title="Prioritized-Reply"><p><a href="https://arxiv.org/pdf/1511.05952.pdf">Schaul T, Quan J, Antonoglou I, et al. Prioritized experience replay[J]. arXiv preprint arXiv:1511.05952, 2015.</a></p><p>​    </p><h5 id="Multi-step"><a href="#Multi-step" class="headerlink" title="Multi-step"></a>Multi-step</h5><p>Multi-step通过将TD的拟合一步的$r_t$改进为拟合多步的$r_t,…,r_{t+N}$，使模型在TD和MC之间平衡，兼具两者的优点（和缺点），既减小了目标由训练模型定义导致的偏差，又使回归值$\sum r_{t^\prime}$的方差不至于太大。</p><img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Multi-step.png" class="" title="Multi-step"><p>​    </p><h5 id="Noisy-Net"><a href="#Noisy-Net" class="headerlink" title="Noisy Net"></a>Noisy Net</h5><p>Noisy Net将Epsilon Greedy的有一定概率随机探索改进为对Q-function的参数加上噪音，使探索更有体系，试验中发现能够学习出更优的Actor。</p><img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Noisy-Net.png" class="" title="Noisy-Net"><p>这种方式Q作为Actor，在给定一个state时，会采取相同的Action（称为State-dependent Exploration），比Epsilon Greedy更像一个真实的policy。</p><p>​    </p><h5 id="Distributional-Q-function"><a href="#Distributional-Q-function" class="headerlink" title="Distributional Q-function"></a>Distributional Q-function</h5><p>Distributional Q-function将基础的Q-function中求Reward的期望值改进预测Reward的分布，从而可以根据我们其他目标选择其他的Action，比如想要比较稳定的较好结果，就可以倾向于选Reward分布稳定一点Action。</p><img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Distributional-Q-function.png" class="" title="Distributional-Q-function"><h5 id="Rainbow"><a href="#Rainbow" class="headerlink" title="Rainbow"></a>Rainbow</h5><p>将上述的六招都用上，每种方法用一种颜色表示，就是Rainbow。左图是每种方法各自的效果，右图是去掉其中一种方法之后的效果。</p><img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/Rainbow-1636166579356.png" class="" title="Rainbow"><p><a href="https://arxiv.org/pdf/1710.02298.pdf">Rainbow</a></p><h4 id="Continuous-Actions"><a href="#Continuous-Actions" class="headerlink" title="Continuous Actions"></a>Continuous Actions</h4><p>当Action是连续值，则根据已有的Q函数得到Action:$a= arg\ \underset{a}{max}\ Q(s,a)$不再能枚举所有action求解。常用的做法有（1）采用很多个Action，计算其对应的Q值，最大的作为结果；（2）使用梯度下降求解上面的优化问题；（3）通过网络的设计，让这个优化问题可以简单地求解。</p><p>第三种方法可以通过将Q值设定为 $a$ 的二次函数，比如：</p><img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/ContinuousActions.png" class="" title="ContinuousActions"><h2 id="Actor-Critic"><a href="#Actor-Critic" class="headerlink" title="Actor-Critic"></a>Actor-Critic</h2><h3 id="Actor-Critic-1"><a href="#Actor-Critic-1" class="headerlink" title="Actor-Critic"></a>Actor-Critic</h3><p>在Policy Gradient中说过，增加了baseline和Assign Suitable Credit这两个Tips之后，Actor的更新公式变成：</p><script type="math/tex; mode=display">\begin{eqnarray*}\nabla E[R_{\theta}] &\approx&\frac{1}{N}\sum^N_{n=1}\sum ^{T_n}_{t=1}\left(\color{red} \sum^{T_n}_{t^{\prime}=t} \gamma^{t^{\prime}-t} r^n_{t^{\prime}}-b\right)\nabla log\ p(a^n_t|s^n_t,\theta)\end{eqnarray*}</script><p>其中令 $G^n_t=\sum^{T_n}_{t^{\prime}=t} \gamma^{t^{\prime}-t} r^n_{t^{\prime}}$，是$a_t$之后时间衰减的累积奖励，这个值在实际与环境互动中得到的，很不稳定，如果G的方差比较大，尤其严重。因此我们使用G的期望值更为合理。</p><p>在Q-Learning中，我们所学习的Q-function就是用来估计Q值，也就是执行$a_t$之后的累积奖励，在很多Q-Learning中也同样会考虑时间衰减orT步之内的累积奖励，即：$\color{red}E [G^n_t]=Q^ {\pi_{\theta}}(s^n_t,a^n_t)$。而baseline：$b$想要表示的是平均水平的奖励，和V值的含义相同，因此令$\color{red}b=V ^{\pi_{\theta}}(s^n_t)$。因此我们可以借鉴Value-base的方法，用$\color{red}Q^ {\pi_{\theta}}(s^n_t,a^n_t) - V^{\pi_{\theta}}(s^n_t)$代替$\left( \sum^{T_n}_{t^{\prime}=t} \gamma^{t^{\prime}-t} r^n_{t^{\prime}}-b\right)$，这就是<strong>Actor-Critic</strong>，即：</p><script type="math/tex; mode=display">\begin{eqnarray*}\nabla E[R_{\theta}] &\approx&\frac{1}{N}\sum^N_{n=1}\sum ^{T_n}_{t=1}\left(\color{red}Q^ {\pi_{\theta}}(s^n_t,a^n_t) - V^{\pi_{\theta}}(s^n_t)\right)\nabla log\ p(a^n_t|s^n_t,\theta)\end{eqnarray*}</script><h3 id="Advantage-Actor-Critic"><a href="#Advantage-Actor-Critic" class="headerlink" title="Advantage Actor-Critic"></a>Advantage Actor-Critic</h3><p>上述的AC算法需要估计Q和V两个模型，通过<strong>将Q改为用V代替</strong>，可以转换为只需要V-function。Q可以转化为下一步的奖励和V值相加：</p><script type="math/tex; mode=display">Q^ {\pi}(s^n_t,a^n_t) = E[r^n_t + V^{\pi}(s^n_{t+1})]</script><p>其中V用模型代替时就是期望值，再忽略$r^n_t$的期望运算，用真实值代替期望，则可以表示为：$Q^ {\pi}(s^n_t,a^n_t) = r^n_t + V^{\pi}(s^n_{t+1})$。此时：</p><script type="math/tex; mode=display">\begin{eqnarray*}\nabla E[R_{\theta}] &\approx&\frac{1}{N}\sum^N_{n=1}\sum ^{T_n}_{t=1}\left(\color{red}r^n_t + V^{\pi}(s^n_{t+1}) - V^{\pi_{\theta}}(s^n_t)\right)\nabla log\ p(a^n_t|s^n_t,\theta)\end{eqnarray*}</script><p>实际的训练流程就是：$\pi$和环境互动；然后用TD or MC方法学习$V^{\pi}(s)$；然后根据互动的数据和$V^{\pi}(s)$使用policy gradient（上面的公式）更新$\pi$。</p><img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/AdvantageActor-Critic.png" class="" title="AdvantageActor-Critic"><p>​    </p><p>有两个Tip，第一个是$\pi(s)$和$V(s)$的输入都输入相同，他们的前几层<strong>参数是可以共享</strong>的；第二个是$\pi(s)$的输出可以加<strong>entropy不能太小的正则</strong>，以起到探索的作用。</p><img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/AAC-tips.png" class="" title="AAC-tips"><p>​    </p><h3 id="Asynchronous-Advantage-Actor-Critic"><a href="#Asynchronous-Advantage-Actor-Critic" class="headerlink" title="Asynchronous Advantage Actor-Critic"></a>Asynchronous Advantage Actor-Critic</h3><p>增加一个分布式异步训练可以提高模型学习效率。分为4步：① 拷贝全局参数；② 和环境互动采样数据；③ 计算梯度；④ 更新全局参数。 </p><img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/A3C.png" class="" title="A3C"><p><a href="https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2#.68x6na7o9">Simple Reinforcement Learning with Tensorflow Part 8: Asynchronous Actor-Critic Agents (A3C)</a></p><p><a href="http://proceedings.mlr.press/v48/mniha16.pdf">Mnih V, Badia A P, Mirza M, et al. Asynchronous methods for deep reinforcement learning[C]//International conference on machine learning. PMLR, 2016: 1928-1937.</a></p><p>​    </p><h3 id="Pathwise-Derivative-Policy-Gradient"><a href="#Pathwise-Derivative-Policy-Gradient" class="headerlink" title="Pathwise Derivative Policy Gradient"></a>Pathwise Derivative Policy Gradient</h3><p>这个方法在<strong>Actor后面接一个Q-Learning</strong>，让Actor学出Q看来最好的Action。</p><img src="/2021/07/29/2021-07-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/PathwiseDerivativePolicyGradient.png" class="" title="PathwiseDerivativePolicyGradient"><p>可以将其<strong>看做GAN</strong>，Actor产生action，Q评判这个Action是否好。也可以看做是Actor-Critic的加强，这里的Critic不仅可以评判Action是否好，还能告诉Actor什么样的Action是最好的。还可以看做Q-Learning的加强，此时产生Action不再是由Q直接产生，而是由Actor产生，从而可以产出连续值的Action。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇为学习强化学习笔记，主要是学习李宏毅老师的&lt;a href=&quot;http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML20.html&quot;&gt;课程&lt;/a&gt;的笔记。内容是强化学习的简单介绍，了解一下框架。&lt;/p&gt;
    
    </summary>
    
      <category term="模块简介" scheme="http://wangdongdong122.github.io/categories/%E6%A8%A1%E5%9D%97%E7%AE%80%E4%BB%8B/"/>
    
    
      <category term="机器学习" scheme="http://wangdongdong122.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="学习笔记" scheme="http://wangdongdong122.github.io/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
      <category term="强化学习" scheme="http://wangdongdong122.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
</feed>
