<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>天气桑的blog</title>
  
  <subtitle>这个人很懒，还没写介绍</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://tessiehe.github.io/"/>
  <updated>2024-02-26T05:51:45.538Z</updated>
  <id>http://tessiehe.github.io/</id>
  
  <author>
    <name>Tenki San</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title></title>
    <link href="http://tessiehe.github.io/2024/02/25/obsidian/20240225-%E5%A4%A7%E6%A8%A1%E5%9E%8B100%E9%97%AE/"/>
    <id>http://tessiehe.github.io/2024/02/25/obsidian/20240225-大模型100问/</id>
    <published>2024-02-25T07:13:09.257Z</published>
    <updated>2024-02-26T05:51:45.538Z</updated>
    
    <content type="html"><![CDATA[<ol><li><strong>transformer是什么？用于处理什么问题？长什么样？</strong><br>transformer是一种深度学习模型，由Google Brain团队在2017年《attention is all you need 》中首次提出，用于解决自然语言处理中的Seq2Seq类任务，如机器翻译任务。</li></ol><p>其核心创新点是完全采用注意力机制处理序列类数据，该方法能有效解决传统的RNN和CNN类序列处理方法存在的梯度消失/爆炸和无法并行的问题。</p><p>具体来说，transformer包括编码器和解码器两个部分，编码器通过自注意力机制对输入序列进行编码，核心思想是用输入序列中的上下文的加权表示当前字（Multi-head Attention）；解码器先用生成的前序序列表示当前词(Masked Multi-head Attention), 再用编码器编码过的输入序列表示当前字（Multi-head Attention）,最后以当前词的表示作为输入，预测下一个词。 </p><p>由于attention机制没有序的概念，还会通过position embedding的方式将词的位置信息融入词的编码中。同时会增加LayerNorm和ResNet时模型更容易收敛；通过FFN对每个词的编码先升维再降维，增加模型表达能力。<img src="/2024/02/25/obsidian/20240225-%E5%A4%A7%E6%A8%A1%E5%9E%8B100%E9%97%AE/20240225-大模型100问/image-20240225155727980.png" alt="|250"><br>transformer模型的提出不仅在机器翻译任务上取得突破性性能，其构架也称为了后续很多NLP模型的基础，如Bert、GPT等，甚至是CV领域。<br>transformer在CV领域的成功运用通常归功于Google Research在2020年提出的Vision Transformer[1]，用于图片分类任务，其核心思想是把图片分成很多个小块(patch)，每个patch是一个向量，拼接后一张图片的表示就类似于一个句子的表示，通过transformer进行编码，然后预测目标类别。</p><ol><li><strong>如何计算大模型参数计算量</strong><br><a href="https://zhuanlan.zhihu.com/p/624740065">分析transformer模型的参数量、计算量、中间激活、KV cache</a><br>gpt参数量计算方法</li></ol><ul><li>MHA：4个h×h的矩阵（QKV的三个和最后线性变换的一个）</li><li>AddAndNorm 忽略不计</li><li>feed forward： h先升维到4h再降维到h,  8hxh+5h<br>L个transformer堆叠的模型参数量$L(12h^2+5h+Vh)$  V是词表大小。忽略一次项即12Lh^2<br><img src="/2024/02/25/obsidian/20240225-%E5%A4%A7%E6%A8%A1%E5%9E%8B100%E9%97%AE/20240225-大模型100问/image-20240225200352419.png" alt><br><img src="/2024/02/25/obsidian/20240225-%E5%A4%A7%E6%A8%A1%E5%9E%8B100%E9%97%AE/20240225-大模型100问/image-20240225194708508.png" alt></li></ul><p>上面是一个简化的算法，multi-head attention也符合这个参数量，但实际上需要分head计算再组合，即还是通过同样大小的三个W矩阵对QKV进行线性变换后拆分成n_head组QKV，每一组的h’=h/n_head, 每组attention后得到n_head个维度为h’的向量，concat到一起后进行一个线性变换。</p><p>gpt3 L=96，h=12288，词表 计算得到1.74×10^11</p><ol><li><strong>GPT系列的参数量是多少</strong><br>参考<a href="https://zhuanlan.zhihu.com/p/656192138">GPT系列解读（一）</a><br>GPT系列有大大小小很多个模型，175B参数的是GPT3的参数量，GPT3.5是在GPT3的基础上+指令调优和RLHF，参数量估计也是175B。gpt-3.5-turbo是经过蒸馏的，实际参数可能十几亿。<img src="/2024/02/25/obsidian/20240225-%E5%A4%A7%E6%A8%A1%E5%9E%8B100%E9%97%AE/02/25/obsidian/20240225-%E5%A4%A7%E6%A8%A1%E5%9E%8B100%E9%97%AE/image-20240225201622511-8926674.png" class title="image-20240225201622511.png"><img src="/2024/02/25/obsidian/20240225-%E5%A4%A7%E6%A8%A1%E5%9E%8B100%E9%97%AE/02/25/obsidian/20240225-%E5%A4%A7%E6%A8%A1%E5%9E%8B100%E9%97%AE/image-20240225202845150.png" class title="GPT系列技术对比"></li></ol><p>L: transformer block数，n_head:头数，V:词表大，h:隐层大小，max_token：最长上下文长度</p><ul><li>GPT-1：      L=12,n_head=12,h=768,V=50257,max_token=512,参数量0.1B</li><li>GPT-2(XL): L=48,n_head=64,h=1600, V=50257,  max_token=1024, 参数量1.5B</li><li>GPT-3：     L=96,n_head=96,h=12288, V=50257,  max_token=4K/16K(3.5-turbo)/32K?, 参数量1.5B，训练数据45T<img src="/2024/02/25/obsidian/20240225-%E5%A4%A7%E6%A8%A1%E5%9E%8B100%E9%97%AE/02/25/obsidian/20240225-%E5%A4%A7%E6%A8%A1%E5%9E%8B100%E9%97%AE/image-20240225203858573.png" class title="image-20240225203858573.png"></li></ul><ol><li><p><strong>手写GPT代码</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LlamaAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Multi-headed attention from &#x27;Attention Is All You Need&#x27; paper&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config: LlamaConfig</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.config = config</span><br><span class="line">        self.hidden_size = config.hidden_size</span><br><span class="line">        self.num_heads = config.num_attention_heads</span><br><span class="line">        self.head_dim = self.hidden_size // self.num_heads</span><br><span class="line">        self.max_position_embeddings = config.max_position_embeddings</span><br><span class="line"></span><br><span class="line">        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=<span class="literal">False</span>)</span><br><span class="line">        self.k_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=<span class="literal">False</span>)</span><br><span class="line">        self.v_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=<span class="literal">False</span>)</span><br><span class="line">        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self,</span></span></span><br><span class="line"><span class="params"><span class="function">        hidden_states: torch.Tensor,</span></span></span><br><span class="line"><span class="params"><span class="function">        attention_mask: <span class="type">Optional</span>[torch.Tensor] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        position_ids: <span class="type">Optional</span>[torch.LongTensor] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        past_key_value: <span class="type">Optional</span>[<span class="type">Tuple</span>[torch.Tensor]] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        output_attentions: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        use_cache: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    </span>) -&gt; <span class="type">Tuple</span>[torch.Tensor, <span class="type">Optional</span>[torch.Tensor], <span class="type">Optional</span>[<span class="type">Tuple</span>[torch.Tensor]]]:</span></span><br><span class="line">        bsz, q_len, _ = hidden_states.size()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 获得qkv向量</span></span><br><span class="line">        query_states = self.q_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        key_states = self.k_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        value_states = self.v_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 拼接kvcache</span></span><br><span class="line">        kv_seq_len = key_states.shape[-<span class="number">2</span>]</span><br><span class="line">        <span class="keyword">if</span> past_key_value <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            kv_seq_len += past_key_value[<span class="number">0</span>].shape[-<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> past_key_value <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># reuse k, v, self_attention</span></span><br><span class="line">            key_states = torch.cat([past_key_value[<span class="number">0</span>], key_states], dim=<span class="number">2</span>)</span><br><span class="line">            value_states = torch.cat([past_key_value[<span class="number">1</span>], value_states], dim=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        past_key_value = (key_states, value_states) <span class="keyword">if</span> use_cache <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算attention权重</span></span><br><span class="line">        attn_weights = torch.matmul(query_states, key_states.transpose(<span class="number">2</span>, <span class="number">3</span>)) / math.sqrt(self.head_dim)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 加入mask矩阵，decoder-only为下三角</span></span><br><span class="line">        <span class="keyword">if</span> attention_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            attn_weights = attn_weights + attention_mask</span><br><span class="line">            dtype_min = torch.tensor(</span><br><span class="line">                torch.finfo(attn_weights.dtype).<span class="built_in">min</span>, device=attn_weights.device, dtype=attn_weights.dtype</span><br><span class="line">            )</span><br><span class="line">            attn_weights = torch.<span class="built_in">max</span>(attn_weights, dtype_min)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算softmax，这里需要从fp16升为fp32</span></span><br><span class="line">        <span class="comment"># upcast attention to fp32</span></span><br><span class="line">        attn_weights = nn.functional.softmax(attn_weights, dim=-<span class="number">1</span>, dtype=torch.float32).to(query_states.dtype)</span><br><span class="line">        attn_output = torch.matmul(attn_weights, value_states)</span><br><span class="line"></span><br><span class="line">        attn_output = attn_output.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)</span><br><span class="line"></span><br><span class="line">        attn_output = self.o_proj(attn_output)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> output_attentions:</span><br><span class="line">            attn_weights = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> attn_output, attn_weights, past_key_value</span><br></pre></td></tr></table></figure></li><li><p><strong>NLU和NLG的区别</strong><br>NLU（Neural Language Understanding）是自然语言理解，通常是分类模型：文本分类，意图识别，关系抽取，阅读理解等<br>NLG(Neural Language Genaration )是自然语言生成，通常是生成模型：机器翻译、文本摘要、故事续写、问答等</p></li><li><p><strong>BPE 和BBPE的区别是什么</strong><br>二者都是NLP领域的分词方法（tokenization）.BPE（Byte Pair Encoding）是字符级别的分词方法，即以字符为最小单位，逐步合并频率最高的字符对，直到达到预设词表大小。可有效减少词表的大小。BBPE（Byte-level Byte Pair Encoding）是以字节为单位，UTF-8中每个字符可以用1~4个字节表示，在字节粒度进行合并生成词表，该方法理论上可以表示任何字符，对多语言支持好，可有效解决OOV（Out Of Vocalbulary）问题.</p></li><li><p><strong>bert参数量计算</strong><br>bert-base: L=12，h=768,V=30522,max_token=512,参数量110M<br>bert-Large: L=24, h=1024,</p></li><li><p><strong>参数量和显存、模型大小之间的关系</strong><br>参考<a href="https://zhuanlan.zhihu.com/p/624740065">分析transformer模型的参数量、计算量、中间激活、KV cache</a></p></li></ol><ul><li>推理时显存的下限是 2n GB ，至少要把模型加载完全。</li><li>训练时，如果用Adam优化器，有个<strong>2+2+12</strong>的公式，<strong>训练时显存下限是16n GB</strong>，需要把模型参数、梯度和优化器状态（4+4+4），保持在显存，具体可以参考微软的ZeRO论文。</li></ul><ol><li><p><strong>attention为什么要进行scaling?</strong><br>防止随着h增大导致内积QK内积过大</p></li><li><p><strong>为什么transformer要用LayerNorm,可以用BatchNorm吗？</strong><br>norm是为了防止梯度爆炸或小时，不能用batchNorm, LayerNorm是每个样本的每一层进行归一化，保证各个特征之间的相对大小；batchNorm是batch内样本间同一个特征的归一化，保证样本间的相对大小。NLP的样本间没有关系，输入的词序列间有关系，所以应该用LayerNorm</p></li><li><p>transformer为什么要用三个不一样的QKV</p></li><li>Bert中为什么要在开头加个[CLS]?</li><li>有什么技术降低复杂度提升输入长度的？</li><li>Bert是如何处理传统方法难以搞定的溢出词表词(oov)的语义学习的？</li><li>中文是如何处理溢出词表词(oov)的语义学习的？</li><li>Bert如何处理一词多义？</li><li>Bert中的transformer和原生的transformer有什么区别？</li></ol><p>[1]: |Dosovitskiy A, Beyer L, Kolesnikov A, et al. An image is worth 16x16 words: Transformers for image recognition at scale[J]. arXiv preprint arXiv:2010.11929, 2020</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ol&gt;
&lt;li&gt;&lt;strong&gt;transformer是什么？用于处理什么问题？长什么样？&lt;/strong&gt;&lt;br&gt;transformer是一种深度学习模型，由Google Brain团队在2017年《attention is all you need 》中首次提出，用于解决自
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://tessiehe.github.io/2022/07/24/0_%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/pytorch%E5%9F%BA%E7%A1%80%E6%93%8D%E4%BD%9C/"/>
    <id>http://tessiehe.github.io/2022/07/24/0_代码相关/pytorch基础操作/</id>
    <published>2022-07-24T01:45:12.716Z</published>
    <updated>2022-07-24T02:19:47.338Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><h1 id="数据操作"><a href="#数据操作" class="headerlink" title="数据操作"></a>数据操作</h1><p>torch.utils.data.Dataset： 存储sample数据集</p><p>torch.utils.data.DataLoader ： 包装数据可迭代对象</p><h2 id="数据集下载"><a href="#数据集下载" class="headerlink" title="数据集下载"></a>数据集下载</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> torchvision.transforms <span class="keyword">import</span> ToTensor</span><br><span class="line"></span><br><span class="line"><span class="comment"># Download training data from open datasets.</span></span><br><span class="line">training_data = datasets.FashionMNIST(</span><br><span class="line">    root=<span class="string">&quot;data&quot;</span>,</span><br><span class="line">    train=<span class="literal">True</span>,</span><br><span class="line">    download=<span class="literal">True</span>,</span><br><span class="line">    transform=ToTensor(),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Download test data from open datasets.</span></span><br><span class="line">test_data = datasets.FashionMNIST(</span><br><span class="line">    root=<span class="string">&quot;data&quot;</span>,</span><br><span class="line">    train=<span class="literal">False</span>,</span><br><span class="line">    download=<span class="literal">True</span>,</span><br><span class="line">    transform=ToTensor(),</span><br><span class="line">)</span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create data loaders.</span></span><br><span class="line">train_dataloader = DataLoader(training_data, batch_size=batch_size)</span><br><span class="line">test_dataloader = DataLoader(test_data, batch_size=batch_size)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> X, y <span class="keyword">in</span> test_dataloader:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Shape of X [N, C, H, W]: <span class="subst">&#123;X.shape&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Shape of y: <span class="subst">&#123;y.shape&#125;</span> <span class="subst">&#123;y.dtype&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure><h2 id="数据可视化"><a href="#数据可视化" class="headerlink" title="数据可视化"></a>数据可视化</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据可视化</span></span><br><span class="line">labels_map = &#123;</span><br><span class="line">    <span class="number">0</span>: <span class="string">&quot;T-Shirt&quot;</span>,</span><br><span class="line">    <span class="number">1</span>: <span class="string">&quot;Trouser&quot;</span>,</span><br><span class="line">    <span class="number">2</span>: <span class="string">&quot;Pullover&quot;</span>,</span><br><span class="line">    <span class="number">3</span>: <span class="string">&quot;Dress&quot;</span>,</span><br><span class="line">    <span class="number">4</span>: <span class="string">&quot;Coat&quot;</span>,</span><br><span class="line">    <span class="number">5</span>: <span class="string">&quot;Sandal&quot;</span>,</span><br><span class="line">    <span class="number">6</span>: <span class="string">&quot;Shirt&quot;</span>,</span><br><span class="line">    <span class="number">7</span>: <span class="string">&quot;Sneaker&quot;</span>,</span><br><span class="line">    <span class="number">8</span>: <span class="string">&quot;Bag&quot;</span>,</span><br><span class="line">    <span class="number">9</span>: <span class="string">&quot;Ankle Boot&quot;</span>,</span><br><span class="line">&#125;</span><br><span class="line">figure = plt.figure(figsize=(<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line">cols, rows = <span class="number">3</span>, <span class="number">3</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, cols * rows + <span class="number">1</span>):</span><br><span class="line">    sample_idx = torch.randint(<span class="built_in">len</span>(training_data), size=(<span class="number">1</span>,)).item()</span><br><span class="line">    img, label = training_data[sample_idx]</span><br><span class="line">    figure.add_subplot(rows, cols, i)</span><br><span class="line">    plt.title(labels_map[label])</span><br><span class="line">    plt.axis(<span class="string">&quot;off&quot;</span>)</span><br><span class="line">    plt.imshow(img.squeeze(), cmap=<span class="string">&quot;gray&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="自定义数据集"><a href="#自定义数据集" class="headerlink" title="自定义数据集"></a>自定义数据集</h2><p>继承  类，实现<strong>init</strong>, <strong>len</strong>, and <strong>getitem</strong>方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> torchvision.io <span class="keyword">import</span> read_image</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomImageDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, annotations_file, img_dir, transform=<span class="literal">None</span>, target_transform=<span class="literal">None</span></span>):</span></span><br><span class="line">        self.img_labels = pd.read_csv(annotations_file)</span><br><span class="line">        self.img_dir = img_dir</span><br><span class="line">        self.transform = transform</span><br><span class="line">        self.target_transform = target_transform</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span> <span class="comment">#返回sample数</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.img_labels)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, idx</span>):</span></span><br><span class="line">        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, <span class="number">0</span>])</span><br><span class="line">        image = read_image(img_path)</span><br><span class="line">        label = self.img_labels.iloc[idx, <span class="number">1</span>]</span><br><span class="line">        <span class="keyword">if</span> self.transform:</span><br><span class="line">            image = self.transform(image)</span><br><span class="line">        <span class="keyword">if</span> self.target_transform:</span><br><span class="line">            label = self.target_transform(label)</span><br><span class="line">        <span class="keyword">return</span> image, label</span><br></pre></td></tr></table></figure><h1 id="Creating-Models"><a href="#Creating-Models" class="headerlink" title="Creating Models"></a>Creating Models</h1><h2 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h2><p>继承nn.Model并实现__init__和forward函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Get cpu or gpu device for training.</span></span><br><span class="line">device = <span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Using <span class="subst">&#123;device&#125;</span> device&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define model</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NeuralNetwork</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(NeuralNetwork, self).__init__()</span><br><span class="line">        self.flatten = nn.Flatten()</span><br><span class="line">        self.linear_relu_stack = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">28</span>*<span class="number">28</span>, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span> <span class="comment">#实现前向传导</span></span><br><span class="line">        x = self.flatten(x)</span><br><span class="line">        logits = self.linear_relu_stack(x)</span><br><span class="line">        <span class="keyword">return</span> logits</span><br><span class="line"></span><br><span class="line">model = NeuralNetwork().to(device)</span><br><span class="line"><span class="built_in">print</span>(model)</span><br></pre></td></tr></table></figure><h2 id="优化模型"><a href="#优化模型" class="headerlink" title="优化模型"></a>优化模型</h2><p>定义loss和optimizer</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">dataloader, model, loss_fn, optimizer</span>):</span></span><br><span class="line">    size = <span class="built_in">len</span>(dataloader.dataset)</span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">for</span> batch, (X, y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):</span><br><span class="line">        X, y = X.to(device), y.to(device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compute prediction error</span></span><br><span class="line">        pred = model(X) <span class="comment">#自动执行forward</span></span><br><span class="line">        loss = loss_fn(pred, y)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Backpropagation</span></span><br><span class="line">        optimizer.zero_grad() <span class="comment">#梯度清零（某些优化方法可用梯度累加等）</span></span><br><span class="line">        loss.backward() <span class="comment">#计算梯度</span></span><br><span class="line">        optimizer.step() <span class="comment">#更新变量</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> batch % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            loss, current = loss.item(), batch * <span class="built_in">len</span>(X)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;loss: <span class="subst">&#123;loss:&gt;7f&#125;</span>  [<span class="subst">&#123;current:&gt;5d&#125;</span>/<span class="subst">&#123;size:&gt;5d&#125;</span>]&quot;</span>)</span><br><span class="line">            </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span>(<span class="params">dataloader, model, loss_fn</span>):</span></span><br><span class="line">    size = <span class="built_in">len</span>(dataloader.dataset)</span><br><span class="line">    num_batches = <span class="built_in">len</span>(dataloader)</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    test_loss, correct = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad(): <span class="comment">#整个网络都停止自动求导，可以大大加快速度，也可以使用大的batch_size来测试</span></span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> dataloader:</span><br><span class="line">            X, y = X.to(device), y.to(device)</span><br><span class="line">            pred = model(X)</span><br><span class="line">            test_loss += loss_fn(pred, y).item()</span><br><span class="line">            correct += (pred.argmax(<span class="number">1</span>) == y).<span class="built_in">type</span>(torch.<span class="built_in">float</span>).<span class="built_in">sum</span>().item()</span><br><span class="line">    test_loss /= num_batches</span><br><span class="line">    correct /= size</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Test Error: \n Accuracy: <span class="subst">&#123;(<span class="number">100</span>*correct):&gt;<span class="number">0.1</span>f&#125;</span>%, Avg loss: <span class="subst">&#123;test_loss:&gt;8f&#125;</span> \n&quot;</span>)  </span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">epochs = <span class="number">5</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Epoch <span class="subst">&#123;t+<span class="number">1</span>&#125;</span>\n-------------------------------&quot;</span>)</span><br><span class="line">    train(train_dataloader, model, loss_fn, optimizer)</span><br><span class="line">    test(test_dataloader, model, loss_fn)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Done!&quot;</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;[TOC]&lt;/p&gt;
&lt;h1 id=&quot;数据操作&quot;&gt;&lt;a href=&quot;#数据操作&quot; class=&quot;headerlink&quot; title=&quot;数据操作&quot;&gt;&lt;/a&gt;数据操作&lt;/h1&gt;&lt;p&gt;torch.utils.data.Dataset： 存储sample数据集&lt;/p&gt;
&lt;p&gt;torc
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>2022[XDM](Alibaba)</title>
    <link href="http://tessiehe.github.io/2022/06/14/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022%5BXDM%5D(Alibaba)/"/>
    <id>http://tessiehe.github.io/2022/06/14/2_算法相关/2022[XDM](Alibaba)/</id>
    <published>2022-06-14T09:13:52.745Z</published>
    <updated>2022-06-14T11:33:58.186Z</updated>
    
    <content type="html"><![CDATA[<p>Lv, F.; Li, M.; Guo, T.; Yu, C.; Sun, F.; Jin, T.; Ng, W. XDM: Improving Sequential Deep Matching with Unclicked User Behaviors for Recommender System. <em>arXiv:2010.12837 [cs]</em> <strong>2022</strong>.</p><p><strong>简介</strong>：阿里向量召回模型。工业界中基于embedding的信息检索（EBR）中很重要的一个问题时捕捉用户的兴趣，常用的做法是以用户点击过购买序列作为输入进行兴趣编码，但此方式忽略了用户完整的行为中的曝光数据。本文引入曝光数据，通过三元损失学习未点击样本的表示，并通过<strong>置信度融</strong>网络与现有序列模型进行融合。</p><img src="/2022/06/14/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022[XDM](Alibaba)/06/14/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022%5BXDM%5D(Alibaba)/image-20220614192601290.png" class title="image-20220614192601290"><span id="more"></span><h3 id="解读"><a href="#解读" class="headerlink" title="解读"></a>解读</h3><h4 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h4><p>序列推荐通常近采用用户正反馈序列进行建模，忽略了用户的其他反馈，导致不全面的用户表示和次优模型性能。少数使用负反馈的也只是特征层面，缺乏正负反馈的交互。</p><h4 id="解法"><a href="#解法" class="headerlink" title="解法"></a>解法</h4><p>本文基于的假设是曝光未点击有样本是介于点击和随机样本中的一个中间形式。本文引入三元损失来建模这种中间状态。三元指的是：历史点击，历史未点击，点击三种行为序列。通过加入triplet loss使得1）历史点击向量空间远离历史未点击向量空间，同时又不超过m（超参）；2）点击向量空间靠近label向量空间。通过<strong>置信度融合网络（confidence fusion network）</strong>自适应的学习历史点击向量和曝光向量的权重，从而得到用户兴趣表征。</p><p>input：t时刻前历史行为序列（曝光未点击序列，点击序列）</p><p>output: t时刻后k次点击</p><p>loss:  h(历史点击)和c(label)更近，h和n(历史未点击)更远，距离差不大于m，从而符合以上观察。</p><ul><li><p>对称三元损失</p><p>三元包括历史点击和标签的距离（e_hc）,历史点击和历史未点击（e_hn）,历史未点击和标签（e_nc）。核心思想就是e_hc要近，e_hn和e_nc要远。三元损失就是e_hn &gt; e_hc+m1 ,e_nc &gt; e_hc + m2。</p></li></ul><img src="/2022/06/14/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022[XDM](Alibaba)/06/14/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022%5BXDM%5D(Alibaba)/image-20220614192810379.png" class title="image-20220614192810379"><ul><li>置信度融合网络：融合历史点击和未点击向量</li></ul><img src="/2022/06/14/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022[XDM](Alibaba)/06/14/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022%5BXDM%5D(Alibaba)/image-20220614192846924.png" class title="image-20220614192846924"><ul><li>整体loss:</li></ul><img src="/2022/06/14/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022[XDM](Alibaba)/06/14/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022%5BXDM%5D(Alibaba)/image-20220614192905549.png" class title="image-20220614192905549"><h4 id="评论"><a href="#评论" class="headerlink" title="评论"></a>评论</h4><p>本文借鉴对比学习的方式引入曝光信息，能更有效的学习embedding，从而优化向量召回的结果。但是CTR模型中embedding的学习只是第一步，其增益会随着层数的增加而降低。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Lv, F.; Li, M.; Guo, T.; Yu, C.; Sun, F.; Jin, T.; Ng, W. XDM: Improving Sequential Deep Matching with Unclicked User Behaviors for Recommender System. &lt;em&gt;arXiv:2010.12837 [cs]&lt;/em&gt; &lt;strong&gt;2022&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;简介&lt;/strong&gt;：阿里向量召回模型。工业界中基于embedding的信息检索（EBR）中很重要的一个问题时捕捉用户的兴趣，常用的做法是以用户点击过购买序列作为输入进行兴趣编码，但此方式忽略了用户完整的行为中的曝光数据。本文引入曝光数据，通过三元损失学习未点击样本的表示，并通过&lt;strong&gt;置信度融&lt;/strong&gt;网络与现有序列模型进行融合。&lt;/p&gt;
&lt;img src=&quot;/2022/06/14/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022[XDM](Alibaba)/06/14/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022%5BXDM%5D(Alibaba)/image-20220614192601290.png&quot; class title=&quot;image-20220614192601290&quot;&gt;
    
    </summary>
    
    
      <category term="算法相关 - 表示学习 - 负反馈 - 召回 - E-commerce - categories - 算法相关" scheme="http://tessiehe.github.io/tags/%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3-%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0-%E8%B4%9F%E5%8F%8D%E9%A6%88-%E5%8F%AC%E5%9B%9E-E-commerce-categories-%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/"/>
    
  </entry>
  
  <entry>
    <title>2020[DFN](IJCAI)(Tencent)</title>
    <link href="http://tessiehe.github.io/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2020%5BDFN%5D(IJCAI)(Tencent)/"/>
    <id>http://tessiehe.github.io/2022/05/20/2_算法相关/2020[DFN](IJCAI)(Tencent)/</id>
    <published>2022-05-20T03:32:38.000Z</published>
    <updated>2022-06-13T09:49:05.996Z</updated>
    
    <content type="html"><![CDATA[<p>(1) Xie R, Ling C, Wang Y, Wang R, Xia F, Lin L. Deep Feedback Network for Recommendation. In: <em>Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence</em>. International Joint Conferences on Artificial Intelligence Organization; 2020:2519-2525. doi:<a href="https://doi.org/10.24963/ijcai.2020/349">10.24963/ijcai.2020/349</a></p><p><strong>代码链接</strong>：<a href="https://github.com/qqxiaochongqq/DFN">https://github.com/qqxiaochongqq/DFN</a></p><p><strong>简介</strong></p><p>本文是微信团队在头条新闻场景的精排模型。用户的显式、隐式反馈都反映了其兴趣偏好了，但是目前大多数推荐模型都只利用了用户的隐式正反馈（点击）。本文综合利用用户的[显式，隐式]×[正反馈，负反馈]联合建模用户的<strong>无偏兴趣</strong>（unbiased preference）</p><span id="more"></span><h3 id="解读"><a href="#解读" class="headerlink" title="解读"></a>解读</h3><p><strong>动机</strong></p><p>用户的显式、隐式反馈都反映了其兴趣偏好。但是目前大多数推荐模型都只利用了用户的隐式正反馈（点击），这会导致以下几个问题：</p><ol><li>仅关注到用户喜欢什么而忽略了用户不喜欢什么，另一方面导致模型短视且倾向于提供同质的结果，最终影响用户体验。</li><li>除了被动接收模型选择的信息外，用户还需要有效且高效的反馈机制来主动与推荐系统交互。负反馈的缺失会损害这种反馈机制。</li><li>隐反馈存在噪声</li></ol><p>[显式，隐式]×[正反馈，负反馈]能够相互补充，从而建模用户的<strong>无偏兴趣</strong>（unbiased preference）</p><p><strong>解法</strong></p><p>本文场景是微信的头条新闻推荐场景。反馈包括隐式正反馈（点击），隐式负反馈（曝光未点击），显式负反馈（不喜欢）。本文通过transformer分别对三个序列进行编码，再通过external feedback interac- tion component用点击和不喜欢信息对曝光未点击信息进行去噪。本文相关研究领域包括推荐系统、隐反馈建模、负反馈建模。</p><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2020[DFN](IJCAI)(Tencent)/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2020%5BDFN%5D(IJCAI)(Tencent)/image-20220613165420841-5110465.png" class title="image-20220613165420841"><p><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2020[DFN](IJCAI)(Tencent)/image-20220613172517134.png" alt="image-20220613172517134" style="zoom:100%;"></p><ul><li><p>Internal Feedback Interaction Component</p><p>行为内部使用transformer+avg_pooling进行编码，最后生成点击行为向量、曝光未点击向量、不喜欢向量。具体来说，以点击序列为例，将target加入序列中，然后进行正常的transformer编码。</p><p><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2020[DFN](IJCAI)(Tencent)/image-20220613173119757.png" alt="image-20220613173119757" style="zoom:100%;"></p><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2020[DFN](IJCAI)(Tencent)/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2020%5BDFN%5D(IJCAI)(Tencent)/image-20220613173148124.png" class title="image-20220613173148124"></li><li><p>External Feedback Interaction Component</p><p>基本思路是通过点击和不喜欢信息甄别曝光未点击序列中哪些是真正不喜欢的。即用点击和不喜欢的行为向量分别作为query对曝光未点击系列进行attention聚合。</p></li></ul><p>最后用户行为序列包含5个部分，分别是：点击向量，不喜欢向量，曝光未点击向量，曝光未点击中偏向于喜欢的向量，曝光未点击中偏向于不喜欢的向量。</p><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2020[DFN](IJCAI)(Tencent)/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2020%5BDFN%5D(IJCAI)(Tencent)/image-20220613173553259.png" class title="image-20220613173553259"><ul><li><p>优化目标</p><p>优化目标包含点击、不喜欢、曝光未点击3部分</p><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2020[DFN](IJCAI)(Tencent)/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2020%5BDFN%5D(IJCAI)(Tencent)/image-20220613174416596.png" class title="image-20220613174416596"></li><li><p>后续规划</p></li></ul><ol><li>使用更复杂的模型进行特征交互</li><li>增加更多显示反馈</li></ol><p><strong>评论</strong></p><p>微信头条（WeChat Top Stories ）的精排模型。用点击、曝光未点击、不喜欢三个序列，序列内部用transformer编码，<strong>序列间用点击和不喜欢两个序列对曝光未点击序列进行增强和过滤</strong>，优化目标中预估点击、曝光未点击、不喜欢三种行为。</p><p>session内隐式负反馈的编码可借鉴此方法</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;(1) Xie R, Ling C, Wang Y, Wang R, Xia F, Lin L. Deep Feedback Network for Recommendation. In: &lt;em&gt;Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence&lt;/em&gt;. International Joint Conferences on Artificial Intelligence Organization; 2020:2519-2525. doi:&lt;a href=&quot;https://doi.org/10.24963/ijcai.2020/349&quot;&gt;10.24963/ijcai.2020/349&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;代码链接&lt;/strong&gt;：&lt;a href=&quot;https://github.com/qqxiaochongqq/DFN&quot;&gt;https://github.com/qqxiaochongqq/DFN&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;简介&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;本文是微信团队在头条新闻场景的精排模型。用户的显式、隐式反馈都反映了其兴趣偏好了，但是目前大多数推荐模型都只利用了用户的隐式正反馈（点击）。本文综合利用用户的[显式，隐式]×[正反馈，负反馈]联合建模用户的&lt;strong&gt;无偏兴趣&lt;/strong&gt;（unbiased preference）&lt;/p&gt;
    
    </summary>
    
      <category term="算法相关" scheme="http://tessiehe.github.io/categories/%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/"/>
    
    
      <category term="论文阅读" scheme="http://tessiehe.github.io/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
      <category term="用户行为建模" scheme="http://tessiehe.github.io/tags/%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E5%BB%BA%E6%A8%A1/"/>
    
      <category term="负反馈建模" scheme="http://tessiehe.github.io/tags/%E8%B4%9F%E5%8F%8D%E9%A6%88%E5%BB%BA%E6%A8%A1/"/>
    
  </entry>
  
  <entry>
    <title>2021[EdgeRec](Alibaba)(CIKM)</title>
    <link href="http://tessiehe.github.io/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2020%5BEdgeRec%5D/"/>
    <id>http://tessiehe.github.io/2022/05/20/2_算法相关/2020[EdgeRec]/</id>
    <published>2022-05-20T03:32:38.000Z</published>
    <updated>2022-06-14T07:16:22.056Z</updated>
    
    <content type="html"><![CDATA[<p>(1) Gong Y, Jiang Z, Feng Y, et al. EdgeRec: recommender system on edge in Mobile Taobao[C]//Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management. 2020: 2477-2484.</p><p><strong>简介</strong></p><p>首次尝试设计和实现新颖的端上推荐系统（EdgeRec），它实现了实时用户感知和实时系统反馈。此外，我们提出了<strong>异构行为建模（Heterogeneous User Behavior Sequence Modeling）</strong>对曝光页和商品详情页用户行为进行编码，<strong>上下文感知重排（ Context-aware Reranking with Behavior Attention Network）</strong>对历史兴趣向量进行聚合，以捕捉用户的不同兴趣并相应地调整推荐结果。应用在淘宝首页feed流场景重排阶段。</p><p>文章内容比较充实，在架构设计、特征系统设计、负反馈利用方面都值得参考。</p><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2020[EdgeRec]/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2020%5BEdgeRec%5D/image-20220526183035304.png" class title="image-20220526183035304"><span id="more"></span><h3 id="解读"><a href="#解读" class="headerlink" title="解读"></a>解读</h3><p>第一部分介绍了整个推荐系统框架，包括客户端（CN），模型服务（MS），推荐系统(RS)，离线训练（OT）。</p><ul><li>MS是端上的重排模块，也是系统的核心，接收CN的触发请求（三种触发请求）并进行实时重排。</li><li>CN负责前端展现和信息手机</li><li>RS是云上的推荐系统，包括召回和排序，返回客户端一个分页内的初始排序</li><li>OT：端上的模型训练。embedding存储在云上，其他模型结构存储在端上（3M左右大小）</li></ul><p>计算效率优化：用户行为建模和基于上下文的重排模型是异步进行的，用户行为建模因为是GRU，通过流式计算复杂度为O(1)</p><p>存储优化：embedding矩阵存储在云端，以特征形式下发到客户端</p><p>整个推荐流程为：</p><ol><li><p>请求服务器—服务器召回和排序模型得到一个page内的初始item排序（50个item）</p></li><li><p>客户端接收触发器信号对页面内item进行重排</p></li><li>触发重排的行为包括：1）点击item; 2）删除item ; 3）k次曝光未点击</li></ol><p>第二部分介绍了重排模型。</p><h4 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h4><p>信息流广告推荐在能效方面存在3个痛点：</p><ol><li>用户实时感知/决策弱：分页请求导致页内无法实时调整；数据延迟导致实时性较弱</li><li>云端存储和计算瓶颈</li><li>千人一模的局限性：对长尾数据拟合较弱</li></ol><p>根本原因是<strong>用户意图的变化</strong>与<strong>推荐系统的请求时机</strong>和<strong>商品展现时机</strong>不匹配。因此，我们希望将<strong>请求决策</strong>和<strong>展现决策</strong>这两项最接近用户的任务放到端上，利用端智能的能力，进行进一步的优化</p><h4 id="解法"><a href="#解法" class="headerlink" title="解法"></a>解法</h4><p><strong>问题定义</strong>：已知初始排序$S_r$,预估每个item在当前上下文下的评分$\phi(x_i,s,c)$，其中$x_i$是item特征，s是原始上下文特征（local ranking context）,c是用户实时行为特征（real-time user behavior context），主要工作是对c的编码</p><p><strong>特征体系</strong>：特征包括曝光页面action、商详页页面action、item特征</p><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2020[EdgeRec]/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2020%5BEdgeRec%5D/image-20220614145858688.png" class title="image-20220614145858688"><p><strong>异构行为建模 (Heterogeneous User Behavior SequenceModeling, HUBSM）</strong>:即模型图中左右两个框内结构。</p><p>用户行为包含两种异构性（heterogeneity）。第一种是瀑布流页面行为和商品详情页行为。由于商品详情页行为更密集，所以两种行为分开建模。第二种异构性是用户的行为和商品属性。用户行为动作特征揭示了用户对物品的行为分布，而物品特征表示相应物品特征的分布。对二者分别编码（本文采用GRU）再进行融合（本文采用concat）。</p><ul><li>曝光页行为编码</li></ul><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2020[EdgeRec]/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2020%5BEdgeRec%5D/image-20220527094756107.png" class title="image-20220527094756107"><ul><li><p>详情页行为编码</p><p><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2020[EdgeRec]/image-20220527095112504.png" alt="image-20220527095112504" style="zoom:100%;"></p></li></ul><p><strong>上下文感知的行为编码Context-aware Reranking with Behavior Attention Networks</strong></p><ul><li><p>candidate编码（初始序列）</p><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2020[EdgeRec]/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2020%5BEdgeRec%5D/image-20220527095218176.png" class title="image-20220527095218176"></li><li><p>behavior attention：candidate为Q，历史item为K，历史item和action的融合embedding为value进行attention</p><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2020[EdgeRec]/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2020%5BEdgeRec%5D/image-20220527100225837.png" class title="image-20220527100225837"></li></ul><p><strong>在线实验</strong>：实验表明曝光页更多捕捉用户不感兴趣的程度，详情页更多捕捉用户感兴趣的程度</p><h4 id="评论"><a href="#评论" class="headerlink" title="评论"></a>评论</h4><p>本文算法部分包括两个创新点，一个是异构用户行为建模—即对action和item分别编码再进行融合。另一个是上下文感知的行为建模—-即以item相关信息为Q，K，以融合item和action的向量作为V对用户历史行为进行attention聚合。action和item分别建模除了因为action和item不在一个向量空间，也因为淘宝场景下action种类众多，包含更多信息，需要进行独立编码。</p><h3 id="备注"><a href="#备注" class="headerlink" title="备注"></a>备注</h3><ul><li><p>Our work, to our best knowledge, is the first attempt to design and implement the novel Recommender System on Edge (EdgeRec), which achieves Real-time User Perception and Real-time System Feedback</p><p>我们的工作首次尝试设计和实现端上推荐系统，实现实时的用户感知和系统反馈。</p></li><li><p>Moreover, we propose Heterogeneous User Behavior Sequence Modeling and Context-aware Reranking with Behavior Attention Networks to cap- ture user’s diverse interests and adjust recommendation results accordingly.</p><p>我们提出异构的用户行为序列建模和上下文感知的attention进行重排序，来捕捉用户丰富的行为并对推荐结果进行重排序</p></li><li><p>To summarize, feature system in our work is novel and pro- moted (1) from “relying on only positive feedback interactions” to “simultaneously paying attention to positive and negative feedback interactions”, (2) from “concerning on only interacted items” to “considering both interacted items and their corresponding actions”, and (3) from “in quasi real-time way” to “in ultra real-time way”.</p><p>特征系统的新颖性体现在：1)正反馈-&gt;同时关注正负反馈； 2)仅关注交互的item-&gt;关注item及其动作（收藏，加购等 3）；从准实时-&gt;超实时</p></li></ul><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p><a href="https://mp.weixin.qq.com/s/LgF3FkLCid4tv_zKjU1C3g">双11专栏 | EdgeRec：电商信息流的端上推荐系统</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;(1) Gong Y, Jiang Z, Feng Y, et al. EdgeRec: recommender system on edge in Mobile Taobao[C]//Proceedings of the 29th ACM International Conference on Information &amp;amp; Knowledge Management. 2020: 2477-2484.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;简介&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;首次尝试设计和实现新颖的端上推荐系统（EdgeRec），它实现了实时用户感知和实时系统反馈。此外，我们提出了&lt;strong&gt;异构行为建模（Heterogeneous User Behavior Sequence Modeling）&lt;/strong&gt;对曝光页和商品详情页用户行为进行编码，&lt;strong&gt;上下文感知重排（ Context-aware Reranking with Behavior Attention Network）&lt;/strong&gt;对历史兴趣向量进行聚合，以捕捉用户的不同兴趣并相应地调整推荐结果。应用在淘宝首页feed流场景重排阶段。&lt;/p&gt;
&lt;p&gt;文章内容比较充实，在架构设计、特征系统设计、负反馈利用方面都值得参考。&lt;/p&gt;
&lt;img src=&quot;/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2020[EdgeRec]/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2020%5BEdgeRec%5D/image-20220526183035304.png&quot; class title=&quot;image-20220526183035304&quot;&gt;
    
    </summary>
    
      <category term="算法相关" scheme="http://tessiehe.github.io/categories/%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/"/>
    
    
      <category term="论文阅读" scheme="http://tessiehe.github.io/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
      <category term="用户行为建模" scheme="http://tessiehe.github.io/tags/%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E5%BB%BA%E6%A8%A1/"/>
    
      <category term="异构用户行为建模" scheme="http://tessiehe.github.io/tags/%E5%BC%82%E6%9E%84%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E5%BB%BA%E6%A8%A1/"/>
    
      <category term="attention" scheme="http://tessiehe.github.io/tags/attention/"/>
    
  </entry>
  
  <entry>
    <title>2021[CL4SRec](SIGIR)(Alibaba)</title>
    <link href="http://tessiehe.github.io/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2021%5BCL4SRec%5D(SIGIR)(Alibaba)/"/>
    <id>http://tessiehe.github.io/2022/05/20/2_算法相关/2021[CL4SRec](SIGIR)(Alibaba)/</id>
    <published>2022-05-20T03:32:38.000Z</published>
    <updated>2022-07-27T07:57:53.093Z</updated>
    
    <content type="html"><![CDATA[<p>(1) Xie, X.; Sun, F.; Liu, Z.; Wu, S.; Gao, J.; Ding, B.; Cui, B. Contrastive Learning for Sequential Recommendation. <em>arXiv preprint arXiv:2010.14395</em> <strong>2020</strong></p><p>本文来自阿里团队。序列推荐是RS中重要部分，尽管近年来取得了很大成功，但仍然因为数据稀疏难以通过优化大量的参数来学习高质量的用户表征。为了解决这个问题，受CV领域CL的启发，我们提出基于CL的多任务模型CL4SRec,一方面能通过CL的框架从用户自身行为中获取自监督信号，从而更高效的编码用户行为模式。另一方面通过3中数据增强方式构建自监督信号。</p><span id="more"></span><h3 id="解读"><a href="#解读" class="headerlink" title="解读"></a>解读</h3><h4 id="简介"><a href="#简介" class="headerlink" title="简介"></a><strong>简介</strong></h4><p>本文来自阿里团队。序列推荐是RS中重要部分，尽管近年来取得了很大成功，但仍然因为数据稀疏难以通过优化大量的参数来学习高质量的用户表征。为了解决这个问题，受CV领域CL的启发，我们提出基于CL的多任务模型CL4SRec,一方面能通过CL的框架从用户自身行为中获取自监督信号，从而更高效的编码用户行为模式。另一方面通过3中数据增强方式构建自监督信号。</p><h5 id="贡献："><a href="#贡献：" class="headerlink" title="贡献："></a>贡献：</h5><ol><li>第一次把CL引入sequence recommendation</li><li>通过3种方式构建自监督信号</li></ol><h5 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h5><ol><li><p>Sequence Recommendation</p><p>RNN,LSTM,GRU,Caser,Transformer,GNN</p></li><li><p>Self-supervised Learning</p><p>CV/NLP</p></li></ol><p><strong>解法</strong></p><p><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2021[CL4SRec](SIGIR)(Alibaba)/image-20220727144809769.png" alt="image-20220727144809769" style="zoom:50%;"></p><ul><li><p>问题定义</p></li><li><p>CL4SRec框架</p><ul><li><p>数据增强</p><p>Crop: 是用户行为的一个局部表示，在没有全面信息的情况下学习用户的广义表示；无关的局部可以看做是next item预测的变种</p><p>mask：防止编码器的过拟合</p><p>reorder：推荐场景下用户行为顺序概念较弱，减轻编码器对顺序的依赖</p></li></ul></li></ul><p><strong>评论</strong></p><h3 id="备注"><a href="#备注" class="headerlink" title="备注"></a>备注</h3><ul><li><p>dif- ferent from dot product, distances typically satisfy triangle inequal- ity1, which transits additional collaborative closeness and benefits a lot in item cold start issue.</p><p>用距离代替点击能缓解协同传递性问题，因为距离满足三角不等式。这在冷启动场景有为有用</p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;(1) Xie, X.; Sun, F.; Liu, Z.; Wu, S.; Gao, J.; Ding, B.; Cui, B. Contrastive Learning for Sequential Recommendation. &lt;em&gt;arXiv preprint arXiv:2010.14395&lt;/em&gt; &lt;strong&gt;2020&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;本文来自阿里团队。序列推荐是RS中重要部分，尽管近年来取得了很大成功，但仍然因为数据稀疏难以通过优化大量的参数来学习高质量的用户表征。为了解决这个问题，受CV领域CL的启发，我们提出基于CL的多任务模型CL4SRec,一方面能通过CL的框架从用户自身行为中获取自监督信号，从而更高效的编码用户行为模式。另一方面通过3中数据增强方式构建自监督信号。&lt;/p&gt;
    
    </summary>
    
      <category term="算法相关" scheme="http://tessiehe.github.io/categories/%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/"/>
    
    
      <category term="论文阅读" scheme="http://tessiehe.github.io/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
      <category term="用户行为建模" scheme="http://tessiehe.github.io/tags/%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E5%BB%BA%E6%A8%A1/"/>
    
      <category term="对比学习" scheme="http://tessiehe.github.io/tags/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="序列推荐" scheme="http://tessiehe.github.io/tags/%E5%BA%8F%E5%88%97%E6%8E%A8%E8%8D%90/"/>
    
  </entry>
  
  <entry>
    <title>2020[S3-Rec](ICKM)</title>
    <link href="http://tessiehe.github.io/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2020%5BS3-Rec%5D(ICKM)/"/>
    <id>http://tessiehe.github.io/2022/05/20/2_算法相关/2020[S3-Rec](ICKM)/</id>
    <published>2022-05-20T03:32:38.000Z</published>
    <updated>2022-06-24T08:12:57.480Z</updated>
    
    <content type="html"><![CDATA[<p>Zhou K, Wang H, Zhao W X, et al. S3-rec: Self-supervised learning for sequential recommendation with mutual information maximization[C]//Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management. 2020: 1893-1902 </p><p><strong>简介</strong>：本文来自美团和人大，采用预训练+微调的方式训练序列推荐模型。序列预测任务中通常用预测任务相关的loss进行优化，这会导致模型过分强调最终性能，没有很好的捕捉上下文和序列之间的关联。S3-Rec基于互信息最大化（mutual information maximization ,MIM）的思想，通过在预训练中引入4个互信息相关的任务来自监督挖掘数据内在关联，优化最终的表征。</p><span id="more"></span><h3 id="解读"><a href="#解读" class="headerlink" title="解读"></a>解读</h3><p><strong>动机</strong></p><p>以预测任务loss为优化目标的序列推荐存在两个问题：</p><ol><li>模型的优化仅依赖最终预测结果和标签，会带来数据稀疏的问题</li><li>模型过分强调最终性能，难以学习更抽象的数据模式，忽略了数据内在关系</li></ol><p>序列推荐场景下上下文特征有不同的存在形式和特性，设计一种统一的方式来捕捉各种特征间的关系是很难的，本文介于MIM的思想统一的建模不同上线文特征间的关系。</p><p><strong>解法</strong></p><p><strong>评论</strong></p><h3 id="备注"><a href="#备注" class="headerlink" title="备注"></a>备注</h3><ul><li><p>Since it overemphasizes the final performance, the association or fusion between context data and sequence data has not been well captured and utilized for sequential recommendation</p><p>使用ctr loss使得模型过分强调最终的性能，上下文和序列之间的关联并没有被很好的捕捉</p></li><li><p>The main idea of our approach is to utilize the intrinsic data correlation to derive self-supervision signals and enhance the data representations via pre-training methods for improving sequential recommendation.</p><p>通过预训练，利用数据间的内在相关性来自监督的学习数据表征</p></li><li><p>This method splits the input data into multiple (possibly overlapping) views and maximizes the mutual informa- tion between representations of these views.</p><p>MIM方法把数据拆成多个视图并最大化这些视图中的互信息。</p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Zhou K, Wang H, Zhao W X, et al. S3-rec: Self-supervised learning for sequential recommendation with mutual information maximization[C]//Proceedings of the 29th ACM International Conference on Information &amp;amp; Knowledge Management. 2020: 1893-1902 &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;简介&lt;/strong&gt;：本文来自美团和人大，采用预训练+微调的方式训练序列推荐模型。序列预测任务中通常用预测任务相关的loss进行优化，这会导致模型过分强调最终性能，没有很好的捕捉上下文和序列之间的关联。S3-Rec基于互信息最大化（mutual information maximization ,MIM）的思想，通过在预训练中引入4个互信息相关的任务来自监督挖掘数据内在关联，优化最终的表征。&lt;/p&gt;
    
    </summary>
    
      <category term="算法相关" scheme="http://tessiehe.github.io/categories/%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/"/>
    
    
      <category term="论文阅读" scheme="http://tessiehe.github.io/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
      <category term="用户行为建模" scheme="http://tessiehe.github.io/tags/%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E5%BB%BA%E6%A8%A1/"/>
    
      <category term="表示学习" scheme="http://tessiehe.github.io/tags/%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="预训练" scheme="http://tessiehe.github.io/tags/%E9%A2%84%E8%AE%AD%E7%BB%83/"/>
    
      <category term="自监督学习" scheme="http://tessiehe.github.io/tags/%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="互信息最大化" scheme="http://tessiehe.github.io/tags/%E4%BA%92%E4%BF%A1%E6%81%AF%E6%9C%80%E5%A4%A7%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>2021[NOVA](AAAI)</title>
    <link href="http://tessiehe.github.io/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2021%5BNOVA%5D/"/>
    <id>http://tessiehe.github.io/2022/05/20/2_算法相关/2021[NOVA]/</id>
    <published>2022-05-20T03:32:38.000Z</published>
    <updated>2022-06-14T09:01:32.500Z</updated>
    
    <content type="html"><![CDATA[<p>(1) Liu, C.; Li, X.; Cai, G.; Dong, Z.; Zhu, H.; Shang, L. Non-Invasive Self-Attention for Side Information Fusion in Sequential Recommendation. <em>arXiv:2103.03578 [cs]</em> <strong>2021</strong>.</p><p><strong>简介</strong></p><p>NOVA是华为诺亚方舟实验室2021年提出的一篇论文，用于优化序列推荐中side info的融合问题。相较于传统的推荐算法，基于深度学习的算法在推荐场景取得了显著进步（如CNN，RNN）。最近，BERT 框架也作为一种很有前途的方法出现，这得益于其在处理序列数据中的自注意力机制。 然而，原始 BERT 框架的一个限制是它只考虑自然语言标记的一个输入源。 在 BERT 框架下利用各种类型的信息仍然是一个悬而未决的问题。我们实验中发现简单的直接融合不同种类的side info效果甚微，甚至是负向的，所以<strong>我们提出NOVA(non-invasive self-attention mechanism 非侵入式的自监督)在BERT框架下更有效的利用辅助信息</strong>（side info）。NOVA用辅助信息改变注意力分布，而不是直接改变embedding，后者会导致信息过多。</p><p>后续研究方向：1.更强的融合函数 2.相较于layer by layer的融合更有效的融合方式</p><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2021[NOVA]/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2021%5BNOVA%5D/image-20220523140959081.png" class title="image-20220523140959081"><span id="more"></span><h3 id="解读"><a href="#解读" class="headerlink" title="解读"></a>解读</h3><h4 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h4><p>参考bert模型在NLP上的出色表现，bert模型也用于做序列推荐。但序列推荐相较于NLP有更多的side info。NLP任务中位置特征可以看做一种side info，通过跟token enbedding相加的形式进行融合，但是推荐场景下side info种类更多，重要性差异化更明显。目前少有对side info融合方式的研究，通常是暴力的concat或者相加，或者采用门结构进行融合。理论上越多的side info的引入能提升模型效果，但实际上如果融合方式不合理，信息的引入甚至会降低模型效果。</p><p>作者认为暴力的信息融合方法，虽然可以把item和各种side用一个向量表示（或者说在同一个向量空间中表示），但也会导致side信息不可逆的入侵（invasion）item的信息空间，导致在模型的深层越来越难以利用到原始的item信息，从而导致Bert无法学习到最佳的Attention分布和深层Embedding表示。</p><h4 id="解法"><a href="#解法" class="headerlink" title="解法"></a>解法</h4><p>side info可以分为item相关的（如品牌）和行为相关的（如时段、行为类型），</p><ul><li><p>embedding</p><p>产生两个emebdding序列，分别是融合的embedding（F是融合函数，E是embedding layer）和item的embedding</p><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2021[NOVA]/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2021%5BNOVA%5D/image-20220614163145889.png" class title="image-20220614163145889"><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2021[NOVA]/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2021%5BNOVA%5D/image-20220614163244302.png" class title="image-20220614163244302"><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2021[NOVA]/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2021%5BNOVA%5D/image-20220614163421196.png" class title="image-20220614163421196"></li><li><p>nova attention</p></li></ul><p>用item embedding作为Q，融合embedding作为K，V。由于item信息每层都在变，而side信息是不变的，因此在每一层都会将item信息和side信息用fusing function进行融合，生成只在本层使用的Integrated Embedding。</p><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2021[NOVA]/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2021%5BNOVA%5D/image-20220614163459628.png" class title="image-20220614163459628"><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2021[NOVA]/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2021%5BNOVA%5D/image-20220614163512579.png" class title="image-20220614163512579"><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本文的增益点在于bert框架下nova attention可保证每一层的每个元素还是item的表征。在非bert框架下，如din等结构中attention仅仅是单层结构，增益可能不及预期。且nova attention会导致side info学习不够充分。</p><h3 id="注解"><a href="#注解" class="headerlink" title="注解"></a>注解</h3><ol><li>Theoretically, side information should be beneficial by providing more data. Nonetheless, it is challenging to design models that can efficiently make use of the extra information.</li></ol><p>理论上来说提供更多的辅助信息是有效的，但如何有效的利用辅助信息是很有挑战性的</p><ol><li>As shown in Figure 1, with NOVA, the side information acts as an auxiliary for the self-attention module to learn better attention distribu- tion, instead of being fused into item representations， which might cause side effects such as information overwhelm- ing</li></ol><p>NOVA中辅助信息用来学习attention分布，而不是直接融合到embedding中（如concat,pooling），后者会导致信息冗余</p><ol><li><p>业界对sideinfo的常用处理方法<img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2021[NOVA]/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2021%5BNOVA%5D/image-20220521170335625.png" class title="image-20220521170335625"></p></li><li><p>Side information可以分为item相关的和行为相关的。</p></li><li><p>From this perspective, the original BERT also take positional information as the only side information, using addition as the fusion function F.</p><p>BERT可以看做只有位置这种辅助信息，通过相加的方式融合到item embedding中</p></li><li><p>侵入式的融合方法会使得embedding是一种复合嵌入空间，会给解码带来不必要的麻烦</p></li><li><p>Accordingly, we proposed a novel method called non- invasive self-attention (NOVA), to maintain the consistency of embedding space, while exploiting side information to model the sequences more efficiently.</p><p>NOVA能保持嵌入空间一致性的同时有效利用辅助信息。</p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;(1) Liu, C.; Li, X.; Cai, G.; Dong, Z.; Zhu, H.; Shang, L. Non-Invasive Self-Attention for Side Information Fusion in Sequential Recommendation. &lt;em&gt;arXiv:2103.03578 [cs]&lt;/em&gt; &lt;strong&gt;2021&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;简介&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;NOVA是华为诺亚方舟实验室2021年提出的一篇论文，用于优化序列推荐中side info的融合问题。相较于传统的推荐算法，基于深度学习的算法在推荐场景取得了显著进步（如CNN，RNN）。最近，BERT 框架也作为一种很有前途的方法出现，这得益于其在处理序列数据中的自注意力机制。 然而，原始 BERT 框架的一个限制是它只考虑自然语言标记的一个输入源。 在 BERT 框架下利用各种类型的信息仍然是一个悬而未决的问题。我们实验中发现简单的直接融合不同种类的side info效果甚微，甚至是负向的，所以&lt;strong&gt;我们提出NOVA(non-invasive self-attention mechanism 非侵入式的自监督)在BERT框架下更有效的利用辅助信息&lt;/strong&gt;（side info）。NOVA用辅助信息改变注意力分布，而不是直接改变embedding，后者会导致信息过多。&lt;/p&gt;
&lt;p&gt;后续研究方向：1.更强的融合函数 2.相较于layer by layer的融合更有效的融合方式&lt;/p&gt;
&lt;img src=&quot;/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2021[NOVA]/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2021%5BNOVA%5D/image-20220523140959081.png&quot; class title=&quot;image-20220523140959081&quot;&gt;
    
    </summary>
    
      <category term="算法相关" scheme="http://tessiehe.github.io/categories/%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/"/>
    
    
      <category term="论文阅读" scheme="http://tessiehe.github.io/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
      <category term="辅助信息融合" scheme="http://tessiehe.github.io/tags/%E8%BE%85%E5%8A%A9%E4%BF%A1%E6%81%AF%E8%9E%8D%E5%90%88/"/>
    
      <category term="行为序列建模" scheme="http://tessiehe.github.io/tags/%E8%A1%8C%E4%B8%BA%E5%BA%8F%E5%88%97%E5%BB%BA%E6%A8%A1/"/>
    
  </entry>
  
  <entry>
    <title>2022[CML](WSDM)(Baidu)</title>
    <link href="http://tessiehe.github.io/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022%5BCML%5D(WSDM)/"/>
    <id>http://tessiehe.github.io/2022/05/20/2_算法相关/2022[CML](WSDM)/</id>
    <published>2022-05-20T03:32:38.000Z</published>
    <updated>2022-06-27T11:18:38.006Z</updated>
    
    <content type="html"><![CDATA[<p>link：<a href="https://arxiv.org/pdf/2202.08523.pdf">https://arxiv.org/pdf/2202.08523.pdf</a></p><p>code：<a href="https://github.com/weiwei1206/CML.git">https://github.com/weiwei1206/CML.git</a></p><p>Wei, Wei, et al. “Contrastive meta learning with behavior multiplicity for recommendation.” <em>Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining</em>. 2022</p><p><strong>简介</strong>：本文来自百度搜索团队.通过对比元学习为不同用户维护专用的跨类型行为依赖模型，从而建模个性化的用户-商品间的多重关系。借助对比学习的思想，通过将辅助行为信息作为监督信号引入，能建模不同行为间的依赖性。</p><p>后续研究方向有1. 用CML框架预训练建模用户画像，服务于线上模型; 2.CML解耦用户兴趣向量</p><span id="more"></span><h3 id="解读"><a href="#解读" class="headerlink" title="解读"></a>解读</h3><p><strong>动机</strong></p><p>传统的推荐系统通常假设用户和item之间只有一种交互关系，无法从多种类型的用户行为中建模多重用户-商品的关系。近期有一些工作研究各种行为之间的依赖性，但是存在两个挑战：</p><ol><li>行为稀疏。要预测的行为相对于其他行为是稀疏的</li><li>通过模型来捕捉用户的行为模式。不同用户的行为模式是不同的。当要同时建模多种行为模式时，这个差异性会更大。</li></ol><p>本文通过对比元学习不同用户维护专用的跨类型行为依赖模型，从而建模个性化的用户-商品间的多重关系。</p><p><strong>解法</strong></p><ol><li>构建异构行为图</li></ol><p>节点是user和item，边包含不同的行为。</p><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022[CML](WSDM)/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022%5BCML%5D(WSDM)/image-20220627191534632.png" class title="image-20220627191534632"><p>每个节点的embedding由周围L度邻居节点不同行为聚合得到。</p><p><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022[CML](WSDM)/image-20220627191652389.png" alt="image-20220627191652389" style="zoom:50%;"></p><p>最后将不同行为聚合成一个embedding</p><p><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022[CML](WSDM)/image-20220627191834147.png" alt="image-20220627191834147" style="zoom:50%;"></p><p><strong>评论</strong></p><h3 id="备注"><a href="#备注" class="headerlink" title="备注"></a>备注</h3><ul><li><p>dif- ferent from dot product, distances typically satisfy triangle inequal- ity1, which transits additional collaborative closeness and benefits a lot in item cold start issue.</p><p>用距离代替点击能缓解协同传递性问题，因为距离满足三角不等式。这在冷启动场景有为有用</p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;link：&lt;a href=&quot;https://arxiv.org/pdf/2202.08523.pdf&quot;&gt;https://arxiv.org/pdf/2202.08523.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;code：&lt;a href=&quot;https://github.com/weiwei1206/CML.git&quot;&gt;https://github.com/weiwei1206/CML.git&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Wei, Wei, et al. “Contrastive meta learning with behavior multiplicity for recommendation.” &lt;em&gt;Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining&lt;/em&gt;. 2022&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;简介&lt;/strong&gt;：本文来自百度搜索团队.通过对比元学习为不同用户维护专用的跨类型行为依赖模型，从而建模个性化的用户-商品间的多重关系。借助对比学习的思想，通过将辅助行为信息作为监督信号引入，能建模不同行为间的依赖性。&lt;/p&gt;
&lt;p&gt;后续研究方向有1. 用CML框架预训练建模用户画像，服务于线上模型; 2.CML解耦用户兴趣向量&lt;/p&gt;
    
    </summary>
    
      <category term="算法相关" scheme="http://tessiehe.github.io/categories/%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/"/>
    
    
      <category term="论文阅读" scheme="http://tessiehe.github.io/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
      <category term="用户行为建模" scheme="http://tessiehe.github.io/tags/%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E5%BB%BA%E6%A8%A1/"/>
    
      <category term="表示学习" scheme="http://tessiehe.github.io/tags/%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="不确定性建模" scheme="http://tessiehe.github.io/tags/%E4%B8%8D%E7%A1%AE%E5%AE%9A%E6%80%A7%E5%BB%BA%E6%A8%A1/"/>
    
      <category term="度量学习" scheme="http://tessiehe.github.io/tags/%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>2022[CIM](WWW)(JD)</title>
    <link href="http://tessiehe.github.io/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022%5BCIM%5D/"/>
    <id>http://tessiehe.github.io/2022/05/20/2_算法相关/2022[CIM]/</id>
    <published>2022-05-20T03:32:38.000Z</published>
    <updated>2022-05-31T08:04:33.775Z</updated>
    
    <content type="html"><![CDATA[<p>Zheng K, Wang L, Li Y, et al. Implicit User Awareness Modeling via Candidate Items for CTR Prediction in Search Ads[C]//Proceedings of the ACM Web Conference 2022. 2022: 246-255.</p><p>本文来自京东搜索广告团队。用户的点击行为通常存在对展示的物品的强烈对比模式，而候选item可以作为展示item的替代项，受此启发本文提出基于CIM（candidate item model）来模拟用户对候选集的感知。CIM通过引入额外的模块把候选集编码到上下文向量中，可广泛的扩展到各种CTR模型。隐式的建模用户的<strong>比较行为模式</strong>在其他问题上也有巨大潜力。</p><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022[CIM]/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022%5BCIM%5D/image-20220530165235408.png" class title="image-20220530165235408"><span id="more"></span><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>用户行为存在广泛的对比模式，但这种模式只会发生在用户有感知的item中。如何收集用户感知的item呢？可以收集曝光日志，但这种做会遇到冷启动、噪声（曝光了但其实用户并未注意）、收集信息渠道不完整等问题。所以本文用召回的候选集作为曝光信息的替代项，通过对召回的候选集预估曝光概率来预估用户是否对此有感知，并在此基础上进行”对比行为”建模。为了建模曝光概率，模型中加入曝光损失进行监督学习。</p><p>文章的动机不太有说服力，曝光概率替代曝光信息能减少特征加工和存储的开销，但对于冷启动、曝光噪音问题的解决似乎没有太大作用。当曝光渠道很多，曝光日志难以采集时可以考虑。即以计算换推理阶段的特征处理。</p><h3 id="注解"><a href="#注解" class="headerlink" title="注解"></a>注解</h3><ul><li><p>The idea of implicitly modeling comparison patterns within awareness has great potential to extend to other learning problems.</p><p>隐式的建模用户的”比较行为”在其他问题上也有很大潜力</p></li><li><p>It could be naturally used to capture comparison pat- terns between positive and negative instances during training, by establishing auxiliary pairwise loss between the positive instance and a negative instance</p></li><li><p>In this paper, we propose to implicitly model user awareness from the candidate item set by predicting their impression probabilities</p><p>通过预估候选集的曝光概率来隐式建模用户对候选集的感知</p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Zheng K, Wang L, Li Y, et al. Implicit User Awareness Modeling via Candidate Items for CTR Prediction in Search Ads[C]//Proceedings of the ACM Web Conference 2022. 2022: 246-255.&lt;/p&gt;
&lt;p&gt;本文来自京东搜索广告团队。用户的点击行为通常存在对展示的物品的强烈对比模式，而候选item可以作为展示item的替代项，受此启发本文提出基于CIM（candidate item model）来模拟用户对候选集的感知。CIM通过引入额外的模块把候选集编码到上下文向量中，可广泛的扩展到各种CTR模型。隐式的建模用户的&lt;strong&gt;比较行为模式&lt;/strong&gt;在其他问题上也有巨大潜力。&lt;/p&gt;
&lt;img src=&quot;/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022[CIM]/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022%5BCIM%5D/image-20220530165235408.png&quot; class title=&quot;image-20220530165235408&quot;&gt;
    
    </summary>
    
      <category term="算法相关" scheme="http://tessiehe.github.io/categories/%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/"/>
    
    
      <category term="论文阅读" scheme="http://tessiehe.github.io/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
      <category term="行为序列建模" scheme="http://tessiehe.github.io/tags/%E8%A1%8C%E4%B8%BA%E5%BA%8F%E5%88%97%E5%BB%BA%E6%A8%A1/"/>
    
  </entry>
  
  <entry>
    <title>2022[ContraRec](TOIS)(Tsinghua)</title>
    <link href="http://tessiehe.github.io/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022%5BContraRec%5D(TOIS)(Tsinghua)/"/>
    <id>http://tessiehe.github.io/2022/05/20/2_算法相关/2022[ContraRec](TOIS)(Tsinghua)/</id>
    <published>2022-05-20T03:32:38.000Z</published>
    <updated>2022-07-27T08:00:08.684Z</updated>
    
    <content type="html"><![CDATA[<p>(1)Wang, C.; Ma, W.; Chen, C. Sequential Recommendation with Multiple Contrast Signals. <em>ACM Trans. Inf. Syst.</em> <strong>2022</strong>, 3522673. <a href="https://doi.org/10.1145/3522673">https://doi.org/10.1145/3522673</a>.</p><p><strong>简介</strong>：</p><p>代码：<a href="https://github.com/THUwangcy/ReChorus/tree/TOIS22">https://github.com/THUwangcy/ReChorus/tree/TOIS22</a></p><span id="more"></span><h3 id="解读"><a href="#解读" class="headerlink" title="解读"></a>解读</h3><p><strong>动机</strong></p><p><strong>解法</strong></p><p><strong>实验</strong></p><p><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022[ContraRec](TOIS)(Tsinghua)/image-20220718211321196.png" alt="image-20220718211321196" style="zoom:50%;"></p><p><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022[ContraRec](TOIS)(Tsinghua)/image-20220718211436859.png" alt="image-20220718211436859" style="zoom:50%;"></p><h3 id="备注"><a href="#备注" class="headerlink" title="备注"></a>备注</h3><ul><li>ContraRec和CL4SRec相比，有几个不同点：<ol><li>正样本的构造处理增强的序列，还有同target的序列</li><li>主loss由softmax cross entropy改为InfoNce</li></ol></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;(1)Wang, C.; Ma, W.; Chen, C. Sequential Recommendation with Multiple Contrast Signals. &lt;em&gt;ACM Trans. Inf. Syst.&lt;/em&gt; &lt;strong&gt;2022&lt;/strong&gt;, 3522673. &lt;a href=&quot;https://doi.org/10.1145/3522673&quot;&gt;https://doi.org/10.1145/3522673&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;简介&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;代码：&lt;a href=&quot;https://github.com/THUwangcy/ReChorus/tree/TOIS22&quot;&gt;https://github.com/THUwangcy/ReChorus/tree/TOIS22&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="算法相关" scheme="http://tessiehe.github.io/categories/%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/"/>
    
    
      <category term="论文阅读" scheme="http://tessiehe.github.io/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
      <category term="用户行为建模" scheme="http://tessiehe.github.io/tags/%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E5%BB%BA%E6%A8%A1/"/>
    
      <category term="对比学习" scheme="http://tessiehe.github.io/tags/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>2022[DIF-SR](SIGIR)</title>
    <link href="http://tessiehe.github.io/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022%5BDIF%5D(SIGIR)/"/>
    <id>http://tessiehe.github.io/2022/05/20/2_算法相关/2022[DIF](SIGIR)/</id>
    <published>2022-05-20T03:32:38.000Z</published>
    <updated>2022-07-04T06:31:40.282Z</updated>
    
    <content type="html"><![CDATA[<p>(1)Xie Y, Zhou P, Kim S. Decoupled Side Information Fusion for Sequential Recommendation[J]. arXiv preprint arXiv:2204.11046, 2022.</p><p><strong>简介</strong>：本文来自香港大学。将各种辅助信息融合和item信息融合，并通过attention进行序列编码是一种通用的兴趣提取结构。但是这会存在两个问题：</p><ol><li>异构信息的混合相关性给注意力机制带来了额外的干扰</li><li>embedding的早期融合限制了注意力机制的表达能力</li></ol><p>DIF-SR解耦了item embeding和辅助信息的attention机制，并加入预测side info的辅助任务进一步激活辅助信息的交互。理论和实验都表明DIF能学习到更高秩（RANK）的attention矩阵和更灵活的梯度（attnention中side info和item的梯度分离）。</p><span id="more"></span><h3 id="解读"><a href="#解读" class="headerlink" title="解读"></a>解读</h3><h4 id="动机"><a href="#动机" class="headerlink" title="动机"></a><strong>动机</strong></h4><p>基于辅助信息和item信息融合后复合嵌入进行的embedding会限制attention结构的表征能力，因为复合嵌入空间不可避免的会引入不相关信息，从而造成随机干扰，最终限制attention矩阵的学习。这点可以从attention矩阵的秩定量分析得到。</p><p>attention矩阵的秩通常很小，这回导致低秩瓶颈问题（rank bottleneck），一是因为attention矩阵是key和query的相似度得到的，所以受key和query的向量维度d决定。二是由于上文所说的复合嵌入导致的随机干扰。三是通过简单的融合策略（如add），所有side info共享所有梯度，这导致模型难以学到side info间的相对重要性。</p><p>受BERT中解耦位置信息的成功启发，本文探讨结构辅助信息对模型的影响。</p><h4 id="解法"><a href="#解法" class="headerlink" title="解法"></a><strong>解法</strong></h4><p>基本思路是将item和所有side info分别独立的计算attention，然后再融合（而不是先融合再计算attention）。该方法通过减少不必要的随机扰动和各side info有独立的梯度使得attention 矩阵秩增加。</p><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022[DIF](SIGIR)/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022%5BDIF%5D(SIGIR)/image-20220616170910543.png" class title="image-20220616170910543"><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022[DIF](SIGIR)/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022%5BDIF%5D(SIGIR)/image-20220616175741125-5373463.png" class title="image-20220616175741125"><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022[DIF](SIGIR)/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022%5BDIF%5D(SIGIR)/image-20220616175939060-5373581.png" class title="image-20220616175939060"><p>模型包括三个模块：Embedding Module, Decoupled Side Infor- mation Fusion Module , Prediction Module with AAP。整个模型结构跟SASRec一样，只是把多头自注意力模块变成解耦的辅助信息多头自注意力模块。DIF模块输入包含item和side info两部分，为了防止side info的过拟合，side info在每一层都是一样的。</p><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022[DIF](SIGIR)/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022%5BDIF%5D(SIGIR)/image-20220616180913128.png" class title="image-20220616180913128"><p>其中R表示每个block的item表征；f1-fp表示p种side info的表征。DIF计算如下（以一个head为例：各自分别计算attention score，得到p个n<em>n的attention矩阵，n是序列长度； 再通过融合函数F把p个矩阵融合融合成一个n </em> n的矩阵，融合的当时有加，元素相乘，gate。最后n*n矩阵作为权重对IDembedding进行加权）：</p><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022[DIF](SIGIR)/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022%5BDIF%5D(SIGIR)/image-20220616181250747-5374372.png" class title="image-20220616181250747"><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022[DIF](SIGIR)/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022%5BDIF%5D(SIGIR)/image-20220616181309115-5374390.png" class title="image-20220616181309115"><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022[DIF](SIGIR)/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022%5BDIF%5D(SIGIR)/image-20220616181420328.png" class title="image-20220616181420328"><h4 id="辅助任务"><a href="#辅助任务" class="headerlink" title="辅助任务"></a><strong>辅助任务</strong></h4><p>通过加入预测辅助信息的辅助任务来增强side info的学习，值得一提的是辅助任务是通过影响attention矩阵最终影响item表征的，如之前的先融合再attention的机制无法实现此目的。</p><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022[DIF](SIGIR)/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022%5BDIF%5D(SIGIR)/image-20220616182845186.png" class title="image-20220616182845186"><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022[DIF](SIGIR)/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022%5BDIF%5D(SIGIR)/image-20220616183230015.png" class title="image-20220616183230015"><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022[DIF](SIGIR)/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022%5BDIF%5D(SIGIR)/image-20220616183248598.png" class title="image-20220616183248598"><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022[DIF](SIGIR)/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022%5BDIF%5D(SIGIR)/image-20220616183217773.png" class title="image-20220616183217773"><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022[DIF](SIGIR)/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022%5BDIF%5D(SIGIR)/image-20220616183303260.png" class title="image-20220616183303260"><h4 id="评论"><a href="#评论" class="headerlink" title="评论"></a>评论</h4><p>本文创新点有两个，一是辅助信息解耦的attention机制，二是训练过程加入side info的预测任务。通过分析attention矩阵秩的方法来定量分析attention机制的表达能力也比较新颖。本文是基于transformer结构的生成模型，也可借鉴到CTR模型中。NOVA其实已经实现的item和辅助信息的解耦，DIF在此基础上对各个域的side info进行解耦。</p><p>消融实验中表明如果不采用解耦的attention，辅助任务的加入并不一定能提高模型效果，这个也跟我们实验一致。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;(1)Xie Y, Zhou P, Kim S. Decoupled Side Information Fusion for Sequential Recommendation[J]. arXiv preprint arXiv:2204.11046, 2022.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;简介&lt;/strong&gt;：本文来自香港大学。将各种辅助信息融合和item信息融合，并通过attention进行序列编码是一种通用的兴趣提取结构。但是这会存在两个问题：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;异构信息的混合相关性给注意力机制带来了额外的干扰&lt;/li&gt;
&lt;li&gt;embedding的早期融合限制了注意力机制的表达能力&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;DIF-SR解耦了item embeding和辅助信息的attention机制，并加入预测side info的辅助任务进一步激活辅助信息的交互。理论和实验都表明DIF能学习到更高秩（RANK）的attention矩阵和更灵活的梯度（attnention中side info和item的梯度分离）。&lt;/p&gt;
    
    </summary>
    
      <category term="算法相关" scheme="http://tessiehe.github.io/categories/%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/"/>
    
    
      <category term="论文阅读" scheme="http://tessiehe.github.io/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
      <category term="用户行为建模" scheme="http://tessiehe.github.io/tags/%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E5%BB%BA%E6%A8%A1/"/>
    
      <category term="表示学习" scheme="http://tessiehe.github.io/tags/%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="辅助信息建模" scheme="http://tessiehe.github.io/tags/%E8%BE%85%E5%8A%A9%E4%BF%A1%E6%81%AF%E5%BB%BA%E6%A8%A1/"/>
    
  </entry>
  
  <entry>
    <title>2022[FMLP-Rec](WWW)</title>
    <link href="http://tessiehe.github.io/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022%5BFMLP-Rec%5D/"/>
    <id>http://tessiehe.github.io/2022/05/20/2_算法相关/2022[FMLP-Rec]/</id>
    <published>2022-05-20T03:32:38.000Z</published>
    <updated>2022-05-31T08:56:15.182Z</updated>
    
    <content type="html"><![CDATA[<p>(1) Zhou, K.; Yu, H.; Zhao, W. X.; Wen, J.-R. Filter-Enhanced MLP Is All You Need for Sequential Recommendation. <em>Proceedings of the ACM Web Conference 2022</em> <strong>2022</strong>, 2388–2399. <a href="https://doi.org/10.1145/3485447.3512111">https://doi.org/10.1145/3485447.3512111</a>.</p><p>最近深度学习通过用户的历史行为建模用户的兴趣，从而进行更准确的推荐。但用户的行为是有<strong>噪音</strong>的，深度模型很容易过拟合用户行为。为了解决这个问题，本文借鉴信号处理中的滤波算法的思想（filtering algorithm）在频域中衰减噪声。实验发现过滤算法可以显著改进代表性序列推荐模型的效果，且简单的简单的过滤算法（例如，带阻滤波器）与全 MLP 架构相结合甚至可以胜过基于 Transformer 的竞争模型。受此启发我们提出FMLP-Rec模型，仅采用MLP结构叠加滤波算法。</p><span id="more"></span><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>作者人为用户行为序列中有很多噪音，而深度模型很容易对噪音过拟合。为了去噪，借鉴信号处理中的滤波算法思想，通过快速傅里叶变换（FFT）将行为序列从时域转为频域信号，并在频域信号进行过滤，实验的过滤方法有高通滤波（HPF），低通滤波（LPF），带阻滤波器（BSF）。过滤后的信号通过逆傅里叶变化转化为时域信号。理论上可以证明频域上的滤波器相当于时域上的循环卷积，可以更好的捕捉用户的周期性行为。</p><p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220531164129575.png" alt="image-20220531164129575"><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022[FMLP-Rec]/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022%5BFMLP-Rec%5D/image-20220531165101362.png" class title="image-20220531165101362"></p><h3 id="注解"><a href="#注解" class="headerlink" title="注解"></a>注解</h3><ul><li><p>Considering the above issues, we aim to simplify the Transformer- based sequential recommender as well as increase its robustness to resist the noise in logged data</p><p>本文目标是简化transformer模型，同时增强对行为序列噪音的鲁棒性</p></li><li><p>Theoretically speaking, according to convolution theorem [45], it can be proved that learnable filters are equivalent to the circular convolution in the time domain, which has a larger receptive field on the whole sequence, and can better capture periodic characteris- tics of user behaviors.</p><p>从理论上讲，根据卷积定理[45]，可以证明可学习滤波器相当于时域的循环卷积，它在整个序列上具有更大的感受野，可以更好地捕捉用户的周期性特征 行为。</p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;(1) Zhou, K.; Yu, H.; Zhao, W. X.; Wen, J.-R. Filter-Enhanced MLP Is All You Need for Sequential Recommendation. &lt;em&gt;Proceedings of the ACM Web Conference 2022&lt;/em&gt; &lt;strong&gt;2022&lt;/strong&gt;, 2388–2399. &lt;a href=&quot;https://doi.org/10.1145/3485447.3512111&quot;&gt;https://doi.org/10.1145/3485447.3512111&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;最近深度学习通过用户的历史行为建模用户的兴趣，从而进行更准确的推荐。但用户的行为是有&lt;strong&gt;噪音&lt;/strong&gt;的，深度模型很容易过拟合用户行为。为了解决这个问题，本文借鉴信号处理中的滤波算法的思想（filtering algorithm）在频域中衰减噪声。实验发现过滤算法可以显著改进代表性序列推荐模型的效果，且简单的简单的过滤算法（例如，带阻滤波器）与全 MLP 架构相结合甚至可以胜过基于 Transformer 的竞争模型。受此启发我们提出FMLP-Rec模型，仅采用MLP结构叠加滤波算法。&lt;/p&gt;
    
    </summary>
    
      <category term="算法相关" scheme="http://tessiehe.github.io/categories/%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/"/>
    
    
      <category term="论文阅读" scheme="http://tessiehe.github.io/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
      <category term="用户行为建模" scheme="http://tessiehe.github.io/tags/%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E5%BB%BA%E6%A8%A1/"/>
    
      <category term="用户行为去噪" scheme="http://tessiehe.github.io/tags/%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E5%8E%BB%E5%99%AA/"/>
    
  </entry>
  
  <entry>
    <title>2022[FeedRec](WWW)(Microsoft)</title>
    <link href="http://tessiehe.github.io/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022%5BFeedRec%5D(WWW)(Microsoft)/"/>
    <id>http://tessiehe.github.io/2022/05/20/2_算法相关/2022[FeedRec](WWW)(Microsoft)/</id>
    <published>2022-05-20T03:32:38.000Z</published>
    <updated>2022-07-19T04:39:57.880Z</updated>
    
    <content type="html"><![CDATA[<p>(1)</p><p>Wu, C.; Wu, F.; Qi, T.; Huang, Y. FeedRec: News Feed Recommendation with Various User Feedbacks. arXiv February 4, 2022.</p><p><strong>简介</strong>：.</p><p>&lt;!- more —&gt;</p><h3 id="解读"><a href="#解读" class="headerlink" title="解读"></a>解读</h3><p><strong>动机</strong></p><p><strong>解法</strong></p><p>提出多反馈建模的统一框架，用强反馈对弱反馈进行提纯。</p><p><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022[FeedRec](WWW)(Microsoft)/image-20220719123653782.png" alt="image-20220719123653782" style="zoom:50%;"></p><p><strong>实验</strong></p><p><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022[FeedRec](WWW)(Microsoft)/image-20220719123906323.png" alt="image-20220719123906323" style="zoom:50%;"></p><h3 id="备注"><a href="#备注" class="headerlink" title="备注"></a>备注</h3>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;(1)&lt;/p&gt;
&lt;p&gt;Wu, C.; Wu, F.; Qi, T.; Huang, Y. FeedRec: News Feed Recommendation with Various User Feedbacks. arXiv February 4, 2022.&lt;/p&gt;
&lt;
      
    
    </summary>
    
      <category term="算法相关" scheme="http://tessiehe.github.io/categories/%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/"/>
    
    
      <category term="论文阅读" scheme="http://tessiehe.github.io/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
      <category term="负反馈建模" scheme="http://tessiehe.github.io/tags/%E8%B4%9F%E5%8F%8D%E9%A6%88%E5%BB%BA%E6%A8%A1/"/>
    
      <category term="多行为建模" scheme="http://tessiehe.github.io/tags/%E5%A4%9A%E8%A1%8C%E4%B8%BA%E5%BB%BA%E6%A8%A1/"/>
    
  </entry>
  
  <entry>
    <title>2022[DIHN](WWW)(Alibaba)</title>
    <link href="http://tessiehe.github.io/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022%5BDIHN%5D(WWW)(Alibaba)/"/>
    <id>http://tessiehe.github.io/2022/05/20/2_算法相关/2022[DIHN](WWW)(Alibaba)/</id>
    <published>2022-05-20T03:32:38.000Z</published>
    <updated>2022-07-18T10:02:04.783Z</updated>
    
    <content type="html"><![CDATA[<p>(1) Shen, Q.; Wen, H.; Tao, W.; Zhang, J.; Lv, F.; Chen, Z.; Li, Z. Deep Interest Highlight Network for Click-Through Rate Prediction in Trigger-Induced Recommendation. In <em>Proceedings of the ACM Web Conference 2022</em>; 2022; pp 422–430.</p><p>本文来自阿里飞猪团队，首次提出<strong>触发推荐问题</strong>（Trigger- Induced Recommendation ，TIR)。TIR场景触发item显示的表征了用户的即时兴趣，传统的兴趣建模（DIN等）target attention的方式忽略了触发item的信息，导致对即时兴趣学习不准确。本文通过Deep Interest Highlight Network (DIHN)建模用户即时兴趣</p><span id="more"></span><h3 id="解读"><a href="#解读" class="headerlink" title="解读"></a>解读</h3><p><strong>动机</strong></p><p>TRI场景如下图</p><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022[DIHN](WWW)(Alibaba)/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022%5BDIHN%5D(WWW)(Alibaba)/image-20220718154001194.png" class title="image-20220718154001194"><p>TIR场景触发item显示的表征了用户的即时兴趣，传统的兴趣建模（DIN等）target attention的方式忽略了触发item的信息，导致对即时兴趣学习不准确。本文通过Deep Interest Highlight Network (DIHN)建模用户即时兴趣。</p><p>即时兴趣建模存在两个难点：</p><ol><li>即时兴趣噪音。用户可能意外点击触发item，需要辨别用户的真实即时兴趣</li><li>用户的历史行为包含多种兴趣，需要提取即时兴趣相关部分</li></ol><p><strong>解法</strong></p><p>DIHN包含3个模块。</p><p>UIN: 预估用户对触发item的点击率，以解决第一个难点</p><p>FEM：融合target item和triger item的embedding（后续作为用户行为兴趣提取的query）。weight由UIN的中间层得到</p><p>HIEM: 用FEM的结果作为query，对历史行为进行聚合。Hard模式下会先过滤行为</p><p><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022[DIHN](WWW)(Alibaba)/image-20220718154330203.png" alt="image-20220718154330203" style="zoom:50%;"></p><p><strong>实验</strong></p><p>公开数据集使用alimama数据集，自己构造triger item。base line模型考虑传统序列模型</p><p><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022[DIHN](WWW)(Alibaba)/image-20220718154840729.png" alt="image-20220718154840729" style="zoom:50%;"></p><p><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022[DIHN](WWW)(Alibaba)/image-20220718154917028.png" alt="image-20220718154917028" style="zoom:50%;"></p><p><strong>评论</strong></p><p>UIN模块比较新颖，实时信息的处理可参考</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;(1) Shen, Q.; Wen, H.; Tao, W.; Zhang, J.; Lv, F.; Chen, Z.; Li, Z. Deep Interest Highlight Network for Click-Through Rate Prediction in Trigger-Induced Recommendation. In &lt;em&gt;Proceedings of the ACM Web Conference 2022&lt;/em&gt;; 2022; pp 422–430.&lt;/p&gt;
&lt;p&gt;本文来自阿里飞猪团队，首次提出&lt;strong&gt;触发推荐问题&lt;/strong&gt;（Trigger- Induced Recommendation ，TIR)。TIR场景触发item显示的表征了用户的即时兴趣，传统的兴趣建模（DIN等）target attention的方式忽略了触发item的信息，导致对即时兴趣学习不准确。本文通过Deep Interest Highlight Network (DIHN)建模用户即时兴趣&lt;/p&gt;
    
    </summary>
    
      <category term="算法相关" scheme="http://tessiehe.github.io/categories/%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/"/>
    
    
      <category term="论文阅读" scheme="http://tessiehe.github.io/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
      <category term="用户行为建模" scheme="http://tessiehe.github.io/tags/%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E5%BB%BA%E6%A8%A1/"/>
    
      <category term="诱发推荐" scheme="http://tessiehe.github.io/tags/%E8%AF%B1%E5%8F%91%E6%8E%A8%E8%8D%90/"/>
    
  </entry>
  
  <entry>
    <title>2022[STOSA](WWW)(spotity)</title>
    <link href="http://tessiehe.github.io/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022%5BICL%5D(WWW)/"/>
    <id>http://tessiehe.github.io/2022/05/20/2_算法相关/2022[ICL](WWW)/</id>
    <published>2022-05-20T03:32:38.000Z</published>
    <updated>2022-07-18T08:53:33.757Z</updated>
    
    <content type="html"><![CDATA[<p>(1)</p><p>Chen, Y.; Liu, Z.; Li, J.; McAuley, J.; Xiong, C. Intent Contrastive Learning for Sequential Recommendation. In <em>Proceedings of the ACM Web Conference 2022</em>; 2022; pp 2172–2182. <a href="https://doi.org/10.1145/3485447.3512090">https://doi.org/10.1145/3485447.3512090</a>.</p><p><strong>简介</strong>：传统的RS都是通过用户历史交互item来表征用户偏好，但是不同的行为序列可能包含相同的潜在意图，最终导致一样的下一次行为。本文提出Intent Contrastive Learning (ICL)，通过聚类和对比学习方法从用户行为中学习不同用户共享的潜在意图，并融合到推荐模型中。</p><span id="more"></span><h3 id="解读"><a href="#解读" class="headerlink" title="解读"></a>解读</h3><p><strong>动机</strong></p><p>传统的RS都是通过用户历史交互item来表征用户偏好，但是不同的行为序列可能包含相同的潜在意图，最终导致一样的下一次行为。本文提出Intent Contrastive Learning (ICL)，通过聚类和对比学习方法从用户行为中学习不同用户共享的潜在意图，并融合到推荐模型中。</p><p><strong>精确的学习用户意图</strong>是比较困难的，历史工作中</p><p>潜在意图的学习存在3个难点：</p><ol><li>潜在意图没有标签</li><li>不同的行为可能包含一样的意图</li><li>有效的把意图融合到推荐系统中并不容易</li></ol><p><strong>解法</strong></p><ol><li>初始化兴趣空间c</li><li>编码用户行为j，并基于Loss优化RS、P(c|h)。其中CL包含sequence CL和<strong>intent CL</strong></li><li>基于用户行为向量h进行聚类，优化c</li></ol><p><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022[ICL](WWW)/image-20220718164630267.png" alt="image-20220718164630267" style="zoom:50%;"></p><p><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022[ICL](WWW)/image-20220718164653425.png" alt="image-20220718164653425" style="zoom:50%;"></p><p><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022[ICL](WWW)/image-20220718164720518.png" alt="image-20220718164720518" style="zoom:50%;"></p><p><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022[ICL](WWW)/image-20220718164611924.png" alt="image-20220718164611924" style="zoom:50%;"></p><p>训练复杂度是传统序列推荐的3倍，推理复杂度不变</p><p><strong>实验</strong></p><p><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022[ICL](WWW)/image-20220718163401661.png" alt="image-20220718163401661" style="zoom:50%;"></p><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022[ICL](WWW)/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022%5BICL%5D(WWW)/image-20220718163452712-8133297.png" class title="image-20220718163452712"><p><strong>评论</strong></p><ul><li>聚类和RS交替训练比较新颖</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;(1)&lt;/p&gt;
&lt;p&gt;Chen, Y.; Liu, Z.; Li, J.; McAuley, J.; Xiong, C. Intent Contrastive Learning for Sequential Recommendation. In &lt;em&gt;Proceedings of the ACM Web Conference 2022&lt;/em&gt;; 2022; pp 2172–2182. &lt;a href=&quot;https://doi.org/10.1145/3485447.3512090&quot;&gt;https://doi.org/10.1145/3485447.3512090&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;简介&lt;/strong&gt;：传统的RS都是通过用户历史交互item来表征用户偏好，但是不同的行为序列可能包含相同的潜在意图，最终导致一样的下一次行为。本文提出Intent Contrastive Learning (ICL)，通过聚类和对比学习方法从用户行为中学习不同用户共享的潜在意图，并融合到推荐模型中。&lt;/p&gt;
    
    </summary>
    
      <category term="算法相关" scheme="http://tessiehe.github.io/categories/%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/"/>
    
    
      <category term="论文阅读" scheme="http://tessiehe.github.io/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
      <category term="用户行为建模" scheme="http://tessiehe.github.io/tags/%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E5%BB%BA%E6%A8%A1/"/>
    
      <category term="表示学习" scheme="http://tessiehe.github.io/tags/%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="不确定性建模" scheme="http://tessiehe.github.io/tags/%E4%B8%8D%E7%A1%AE%E5%AE%9A%E6%80%A7%E5%BB%BA%E6%A8%A1/"/>
    
      <category term="度量学习" scheme="http://tessiehe.github.io/tags/%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>2022[MISS](ICDE)(HUAWEI)</title>
    <link href="http://tessiehe.github.io/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022%5BMISS%5D(ICDE)(HUAWEI)/"/>
    <id>http://tessiehe.github.io/2022/05/20/2_算法相关/2022[MISS](ICDE)(HUAWEI)/</id>
    <published>2022-05-20T03:32:38.000Z</published>
    <updated>2022-07-18T12:28:02.146Z</updated>
    
    <content type="html"><![CDATA[<p>(1)</p><p>Guo, W.; Zhang, C.; He, Z.; Qin, J.; Guo, H.; Chen, B.; Tang, R.; He, X.; Zhang, R. MISS: Multi-Interest Self-Supervised Learning Framework for Click-Through Rate Prediction. arXiv January 28, 2022.</p><p><strong>简介</strong>：….</p><span id="more"></span><h3 id="解读"><a href="#解读" class="headerlink" title="解读"></a>解读</h3><p><strong>动机</strong></p><p><strong>解法</strong></p><p><strong>评论</strong></p><h3 id="备注"><a href="#备注" class="headerlink" title="备注"></a>备注</h3><p>- </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;(1)&lt;/p&gt;
&lt;p&gt;Guo, W.; Zhang, C.; He, Z.; Qin, J.; Guo, H.; Chen, B.; Tang, R.; He, X.; Zhang, R. MISS: Multi-Interest Self-Supervised Learning Framework for Click-Through Rate Prediction. arXiv January 28, 2022.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;简介&lt;/strong&gt;：….&lt;/p&gt;
    
    </summary>
    
      <category term="算法相关" scheme="http://tessiehe.github.io/categories/%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/"/>
    
    
      <category term="论文阅读" scheme="http://tessiehe.github.io/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
      <category term="用户行为建模" scheme="http://tessiehe.github.io/tags/%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E5%BB%BA%E6%A8%A1/"/>
    
      <category term="对比学习" scheme="http://tessiehe.github.io/tags/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>2021[Learning List-wise Representation in Reinforcement Learning for Ads Allocation with Multiple Auxiliary Tasks](MEITUAN)</title>
    <link href="http://tessiehe.github.io/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022%5BLearning%20List-wise%20Representation%20in%20Reinforcement%20Learning%20for%20Ads%20Allocation%20with%20Multiple%20Auxiliary%20Tasks%20%5D/"/>
    <id>http://tessiehe.github.io/2022/05/20/2_算法相关/2022[Learning List-wise Representation in Reinforcement Learning for Ads Allocation with Multiple Auxiliary Tasks ]/</id>
    <published>2022-05-20T03:32:38.000Z</published>
    <updated>2022-06-10T08:10:43.346Z</updated>
    
    <content type="html"><![CDATA[<p>(1) Wang, Z.; Liao, G.; Shi, X.; Wu, X.; Zhang, C.; Wang, Y.; Wang, X.; Wang, D. Learning List-Wise Representation in Reinforcement Learning for Ads Allocation with Multiple Auxiliary Tasks. <em>arXiv:2204.00888 [cs]</em> <strong>2022</strong>.</p><p>本文是美团团队的混排模型。随着强化学习的发展，推荐场景的也对其产生了很大兴趣，主要用强化学习优化混排过程。为了实现最优分配，从point-wise的分配方式过渡为list-wise的方式。但这会导致状态-行为空间维度过高，从而模型泛化性降低，导致RL中agent的探索和采样效率低下。</p><p>为了优化以上问题，根据专家经验，本文在基于RL的重分配模型中加入三个辅助任务（reconstruction, prediction,  contrastive learning）来更学习list-wise更有效、泛化性更高的表示，从而提高RL模型的效果。</p><p>后续研究方向：1.自适应的平衡各个子任务 2.优化离线强化学习相较于在线强化学习存在的问题，如distribution shift problem</p><span id="more"></span><p>[TOC]</p><h3 id="解读"><a href="#解读" class="headerlink" title="解读"></a>解读</h3><p>加入辅助任务是优化list-wise的RL重分配模型状态-行为空间过高问题的一种通用方式，但现有工作中辅助任务的选取缺少对业务场景先验知识的运用。本文结合美团外卖场景选择了3个辅助任务。</p><p>reconstruction：学习表征</p><p>prediction：学习reward</p><p>contrastive-learning: 对状态-行为空间进行聚合和区分</p><p>本文相关的研究工作包括广告分配和表示学习两个方向</p><h4 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h4><p>基于RL的list-wise混排模型面临由于状态-决策空间过大而引起的模型泛化性能低、采样效率低、探索效率低的问题。业界处理此问题的通用做法是引入辅助任务帮助学习状态-决策的表征（representation），从而优化RL模型。现有工作中辅助任务的选取缺少场景相关的专家经验，而专家经验对于表征学习是很重要的。所以本文根据外卖场景下的专家经验选择3个辅助任务对RL模型进行优化。</p><h4 id="解法"><a href="#解法" class="headerlink" title="解法"></a>解法</h4><h5 id="问题定义"><a href="#问题定义" class="headerlink" title="问题定义"></a>问题定义</h5><p>每个页面有K个槽位（slot），有一个自然队列和广告队列，将两个队列进行合并，合并过程中不改变各自的顺序。</p><h5 id="BASE-AGENT"><a href="#BASE-AGENT" class="headerlink" title="BASE AGENT"></a>BASE AGENT</h5><p>本文的RL采用的是Q learning的方式，即优化目标为找到使得一个episode reward最大的策略$\pi$.</p><p>在数学上，广告分配问题被表述为一个 MDP，可以用一个元组 (S, A, 𝑟, 𝑃, 𝛾) 表示。</p><p>S：状态空间，包括召回的两个item队列，用户特征，用户历史行为序列，上下文特征</p><p>A: 行为空间，即K个槽位每个位置是广告还是自然结果</p><p><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022[Learning%20List-wise%20Representation%20in%20Reinforcement%20Learning%20for%20Ads%20Allocation%20with%20Multiple%20Auxiliary%20Tasks%20]/image-20220609194048401.png" alt="image-20220609194048401" style="zoom:50%;"></p><p>r:奖励，包括广告费、平台服务费、用户体验</p><p><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022[Learning%20List-wise%20Representation%20in%20Reinforcement%20Learning%20for%20Ads%20Allocation%20with%20Multiple%20Auxiliary%20Tasks%20]/image-20220609194203459.png" alt="image-20220609194203459" style="zoom:50%;"></p><p>P: 状态转移概率。用户的一次翻页是一次状态转移，当用户不再往下滑则是状态的终止。</p><p>$\gamma$:折扣系数，平衡短期奖励和长期奖励</p><p><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022[Learning%20List-wise%20Representation%20in%20Reinforcement%20Learning%20for%20Ads%20Allocation%20with%20Multiple%20Auxiliary%20Tasks%20]/image-20220609174811991.png" alt="image-20220609174811991" style="zoom:50%;"></p><p><strong>用户行为编码阶段</strong>即用队列中的item分别作为Q与用户历史行为通过attention机制对历史行为进行聚合。最终得到队列长度个向量e.</p><p><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022[Learning%20List-wise%20Representation%20in%20Reinforcement%20Learning%20for%20Ads%20Allocation%20with%20Multiple%20Auxiliary%20Tasks%20]/image-20220609195350578.png" alt="image-20220609195350578" style="zoom:50%;"></p><p><strong>序列融合阶段</strong>用action对e序列进行融合。</p><p><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022[Learning%20List-wise%20Representation%20in%20Reinforcement%20Learning%20for%20Ads%20Allocation%20with%20Multiple%20Auxiliary%20Tasks%20]/image-20220609195628089.png" alt="image-20220609195628089" style="zoom:50%;"></p><p><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022[Learning%20List-wise%20Representation%20in%20Reinforcement%20Learning%20for%20Ads%20Allocation%20with%20Multiple%20Auxiliary%20Tasks%20]/image-20220609195710776.png" alt="image-20220609195710776" style="zoom:50%;"></p><p><strong>Q value预测阶段</strong>通过MLP预测Q</p><p><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022[Learning%20List-wise%20Representation%20in%20Reinforcement%20Learning%20for%20Ads%20Allocation%20with%20Multiple%20Auxiliary%20Tasks%20]/image-20220609212721453.png" alt="image-20220609212721453" style="zoom:50%;"></p><h5 id="辅助任务"><a href="#辅助任务" class="headerlink" title="辅助任务"></a>辅助任务</h5><p>本文选取的3个辅助任务分别是重构任务（reconstruction）、预测任务（prediction）、对比学习任务（contrastive-learning）。三个任务选择动机及学习目标如下：</p><p><strong>重构任务</strong>：外卖场景下有一些对用户决策影响很大的信息，如配送费、品牌等。重构任务的加入就是为了防止学到的embedding丢失这些信息。根据专家经验选了M个最终要的因子作为重构目标。即对每个槽位的item都预测其M个核心指标。每个指标都是二分类任务。</p><p><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022[Learning%20List-wise%20Representation%20in%20Reinforcement%20Learning%20for%20Ads%20Allocation%20with%20Multiple%20Auxiliary%20Tasks%20]/image-20220609213020298.png" alt="image-20220609213020298" style="zoom:50%;"></p><p><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022[Learning%20List-wise%20Representation%20in%20Reinforcement%20Learning%20for%20Ads%20Allocation%20with%20Multiple%20Auxiliary%20Tasks%20]/image-20220609174638195.png" alt="image-20220609174638195" style="zoom:50%;"></p><p><strong>预测任务</strong>：预测任务对每个槽位的item有点击和下拉两个标签。点击标签的加入有助于学习reward。下拉标签有助于学习转移概率。</p><p><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022[Learning%20List-wise%20Representation%20in%20Reinforcement%20Learning%20for%20Ads%20Allocation%20with%20Multiple%20Auxiliary%20Tasks%20]/image-20220609213406900.png" alt="image-20220609213406900" style="zoom:50%;"></p><p><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022[Learning%20List-wise%20Representation%20in%20Reinforcement%20Learning%20for%20Ads%20Allocation%20with%20Multiple%20Auxiliary%20Tasks%20]/image-20220609213419829.png" alt="image-20220609213419829" style="zoom:50%;"></p><p><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022[Learning%20List-wise%20Representation%20in%20Reinforcement%20Learning%20for%20Ads%20Allocation%20with%20Multiple%20Auxiliary%20Tasks%20]/image-20220609213432409.png" alt="image-20220609213432409" style="zoom:50%;"></p><p><strong>对比学习任务</strong>：对比学习使得锚点接近正样本，远离负样本。本文通过引入对比学习使得状态-行为空间对存在这种相对关系，锚点就是一个sample的状态-行为对，正样本是在sample的基础上加随机扰动（把当前page中未曝光的item改成随机的），负样本是从其他请求中随机采样。</p><p><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022[Learning%20List-wise%20Representation%20in%20Reinforcement%20Learning%20for%20Ads%20Allocation%20with%20Multiple%20Auxiliary%20Tasks%20]/image-20220609214434265.png" alt="image-20220609214434265" style="zoom:50%;"></p><p><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022[Learning%20List-wise%20Representation%20in%20Reinforcement%20Learning%20for%20Ads%20Allocation%20with%20Multiple%20Auxiliary%20Tasks%20]/image-20220609214720687.png" alt="image-20220609214720687" style="zoom:50%;"></p><p><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022[Learning%20List-wise%20Representation%20in%20Reinforcement%20Learning%20for%20Ads%20Allocation%20with%20Multiple%20Auxiliary%20Tasks%20]/image-20220609214810476.png" alt="image-20220609214810476" style="zoom:50%;"></p><h5 id="离线训练"><a href="#离线训练" class="headerlink" title="离线训练"></a>离线训练</h5><p>L_dqn是为了训练模型能够准确的预测一个episode的reward。Q(s,a)由公式5得到。max Q(s’,a’) = Q(s’,a’)(j基于的假设是Q learning下的决策都是最大化Q的决策)</p><p><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022[Learning%20List-wise%20Representation%20in%20Reinforcement%20Learning%20for%20Ads%20Allocation%20with%20Multiple%20Auxiliary%20Tasks%20]/image-20220609214848521.png" alt="image-20220609214848521" style="zoom:50%;"></p><p><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022[Learning%20List-wise%20Representation%20in%20Reinforcement%20Learning%20for%20Ads%20Allocation%20with%20Multiple%20Auxiliary%20Tasks%20]/image-20220609215127472.png" alt="image-20220609215127472" style="zoom:50%;"></p><h3 id="备注"><a href="#备注" class="headerlink" title="备注"></a>备注</h3><ul><li><p>dif- ferent from dot product, distances typically satisfy triangle inequal- ity1, which transits additional collaborative closeness and benefits a lot in item cold start issue.</p><p>用距离代替点击能缓解协同传递性问题，因为距离满足三角不等式。这在冷启动场景有为有用</p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;(1) Wang, Z.; Liao, G.; Shi, X.; Wu, X.; Zhang, C.; Wang, Y.; Wang, X.; Wang, D. Learning List-Wise Representation in Reinforcement Learning for Ads Allocation with Multiple Auxiliary Tasks. &lt;em&gt;arXiv:2204.00888 [cs]&lt;/em&gt; &lt;strong&gt;2022&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;本文是美团团队的混排模型。随着强化学习的发展，推荐场景的也对其产生了很大兴趣，主要用强化学习优化混排过程。为了实现最优分配，从point-wise的分配方式过渡为list-wise的方式。但这会导致状态-行为空间维度过高，从而模型泛化性降低，导致RL中agent的探索和采样效率低下。&lt;/p&gt;
&lt;p&gt;为了优化以上问题，根据专家经验，本文在基于RL的重分配模型中加入三个辅助任务（reconstruction, prediction,  contrastive learning）来更学习list-wise更有效、泛化性更高的表示，从而提高RL模型的效果。&lt;/p&gt;
&lt;p&gt;后续研究方向：1.自适应的平衡各个子任务 2.优化离线强化学习相较于在线强化学习存在的问题，如distribution shift problem&lt;/p&gt;
    
    </summary>
    
      <category term="算法相关" scheme="http://tessiehe.github.io/categories/%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/"/>
    
    
      <category term="论文阅读" scheme="http://tessiehe.github.io/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
      <category term="表示学习" scheme="http://tessiehe.github.io/tags/%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="强化学习" scheme="http://tessiehe.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="组合预估" scheme="http://tessiehe.github.io/tags/%E7%BB%84%E5%90%88%E9%A2%84%E4%BC%B0/"/>
    
      <category term="推荐算法" scheme="http://tessiehe.github.io/tags/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/"/>
    
      <category term="多任务学习" scheme="http://tessiehe.github.io/tags/%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>2022[RACP](WSDM)(Alibaba)</title>
    <link href="http://tessiehe.github.io/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022%5BRACP%5D(WSDM)/"/>
    <id>http://tessiehe.github.io/2022/05/20/2_算法相关/2022[RACP](WSDM)/</id>
    <published>2022-05-20T03:32:38.000Z</published>
    <updated>2022-06-20T09:06:49.366Z</updated>
    
    <content type="html"><![CDATA[<p>(1) Fan, Z. Modeling Users’ Contextualized Page-Wise Feedback for Click-Through Rate Prediction in E-Commerce Search. 9.</p><p>本文是来自于淘宝搜索团队的CTR预估模型。建模用户的历史行为对个性化搜索和推荐都很重要，现有方法主要是对用户历史正反馈的建模（点击序列），忽略了产生反馈的上下文信息。本文通过加入历史<strong>页面维度的曝光和反馈</strong>做一位用户历史行为序列，提出了一种新的上下文感知的用户行为建模方式。通过捕捉页面内的信息和页面间的演化可以更详细的学习用户的偏好。 RACP(Recurrent Attention over Contextualized Page sequence)模型通过<strong>page-context aware attention</strong> 学习页面内的关，<strong>recurrent attention</strong>学习页面间的关系</p><p>代码： <a href="https://github.com/racp-submission/racp">https://github.com/racp-submission/racp</a></p><span id="more"></span><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p><strong>动机</strong></p><p>用户行为建模中很少引入用户负反馈，即便一些工作使用了负反馈也是作为一个单独的序列，忽略了正负反馈间的交互及其他上下文信息。实际上用户的反馈受到周边item的影响，且不同的行为间也存在联系，</p><p>本文从页面的角度对用户行为进行编码。</p><p><strong>解法</strong></p><p>模型自下而上可分为4个部分：Embedding Layer, Intra-page Context-aware Interest Layer, Inter-page Interest Backtracking Layer, and Page-level Interest Aggregation Layer.</p><ul><li><p>embedding</p></li><li><p>页面内聚合</p></li></ul><p>页面信息通过attention聚合，权重由item，item的反馈，该item的上下文决定。item的上下文特征包括1，query；2.该页面点击的数量；3.页面跟该item同品牌item数量；4.页面内跟该item同商家的数量；5.页面内该item的价格、销量等排序</p><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022[RACP](WSDM)/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022%5BRACP%5D(WSDM)/image-20220613205723447.png" class title="image-20220613205723447"><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022[RACP](WSDM)/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022%5BRACP%5D(WSDM)/image-20220613205749193.png" class title="image-20220613205749193"><ul><li>页面间聚合</li></ul><p>采用GRU进行融合，GRU输入页面间聚合向量p和当前query Q，其中Q是上一层GRU的隐层输出，p是基于Q的attention融合</p><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022[RACP](WSDM)/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022%5BRACP%5D(WSDM)/image-20220613210502928.png" class title="image-20220613210502928"><ul><li>页面维度兴趣提取</li></ul><p>每个页面是一个兴趣向量p，对p进行聚合</p><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022[RACP](WSDM)/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022%5BRACP%5D(WSDM)/image-20220613210847327.png" class title="image-20220613210847327"><p><strong>评论</strong></p><p>第四个消融实验表明把所有序列变成长序列并用vanilla attention AUC降低一个千分点。看起来降的并不多，但模型复杂度降低很多。所以是否采用page wise建模序列有待商讨。另一方面表明模型相较于DIN等对比模型的增益可能来源于数据（side info）</p><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022[RACP](WSDM)/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022%5BRACP%5D(WSDM)/image-20220613212149010.png" class title="image-20220613212149010"><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022[RACP](WSDM)/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022%5BRACP%5D(WSDM)/image-20220613211911322.png" class title="image-20220613211911322"><h3 id="quote"><a href="#quote" class="headerlink" title="quote"></a><strong>quote</strong></h3><ul><li>“However, they treat users’ positive and negative feedback separately, and rep- resent users’ feedback as a clicked item sequence and a non-clicked item sequence, which cannot generate the mutual context between clicks and non-clicks and ignores other page context information in the page-sequence” 历史工作很少考虑负反馈，即便考虑也是和正反馈分开处理的，这忽略了正负反馈之间的相互作用</li><li>页面信息的增益：1）正反馈是有噪音的，避免过拟合。一个用户点了一个品牌不一定是他就偏好这个品牌，有可能是整个页面都是这个品牌 2) 用户对item的行为受曝光的其他item影响</li><li>页面间的增益：搜索场景下用户的行为和意图是一个逐渐收敛的过程。例如：搜索—-曝光—-点击—-搜索—-曝光—-点击—-购买</li><li>“Recently, some pioneering work (<strong>DFN [33], DSTN</strong> [25]) high- light the importance of modeling both users’ positive and negative feedback for CTR prediction.” 一些负反馈的工作</li><li><strong>DFN [33]: DFN treats click behaviors as strong feedback to guide the positive preference extraction from unclicked behavior sequence.</strong></li><li><strong>DSTN [25]: DSTN considers the clicked and unclicked be- haviors as heterogeneous auxiliary data to help the user preference modeling.</strong></li><li>item画像：item id,品类id,shop id,统计类（成单量等）</li><li>query画像：query id,字符串，分词，类别</li><li>页内的attention聚合+页间兴趣回溯(GRU，由下一个page表征当前的query) + 页间兴趣融合(attention)</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;(1) Fan, Z. Modeling Users’ Contextualized Page-Wise Feedback for Click-Through Rate Prediction in E-Commerce Search. 9.&lt;/p&gt;
&lt;p&gt;本文是来自于淘宝搜索团队的CTR预估模型。建模用户的历史行为对个性化搜索和推荐都很重要，现有方法主要是对用户历史正反馈的建模（点击序列），忽略了产生反馈的上下文信息。本文通过加入历史&lt;strong&gt;页面维度的曝光和反馈&lt;/strong&gt;做一位用户历史行为序列，提出了一种新的上下文感知的用户行为建模方式。通过捕捉页面内的信息和页面间的演化可以更详细的学习用户的偏好。 RACP(Recurrent Attention over Contextualized Page sequence)模型通过&lt;strong&gt;page-context aware attention&lt;/strong&gt; 学习页面内的关，&lt;strong&gt;recurrent attention&lt;/strong&gt;学习页面间的关系&lt;/p&gt;
&lt;p&gt;代码： &lt;a href=&quot;https://github.com/racp-submission/racp&quot;&gt;https://github.com/racp-submission/racp&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="算法相关" scheme="http://tessiehe.github.io/categories/%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/"/>
    
    
      <category term="论文阅读" scheme="http://tessiehe.github.io/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
      <category term="用户行为建模" scheme="http://tessiehe.github.io/tags/%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E5%BB%BA%E6%A8%A1/"/>
    
  </entry>
  
  <entry>
    <title>2022[Re4](WWW)</title>
    <link href="http://tessiehe.github.io/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022%5BRe4%5D(WWW)/"/>
    <id>http://tessiehe.github.io/2022/05/20/2_算法相关/2022[Re4](WWW)/</id>
    <published>2022-05-20T03:32:38.000Z</published>
    <updated>2022-07-19T03:08:06.430Z</updated>
    
    <content type="html"><![CDATA[<p>(1)</p><p>Zhang, S.; Yang, L.; Yao, D.; Lu, Y.; Feng, F.; Zhao, Z.; Chua, T.; Wu, F. Re4: Learning to Re-Contrast, Re-Attend, Re-Construct for Multi-Interest Recommendation. In <em>Proceedings of the ACM Web Conference 2022</em>; 2022; pp 2216–2226.</p><p><strong>简介</strong>：现有的多兴趣建模尽管很有效，但是仅使用编码器利用前向流（forward-flow）表征用户多兴趣。由于没有加入显示的约束，此方式无法保证兴趣向量的正交，也无法保证兴趣向量和历史item在语义空间的关系。本文提出Re4的框架，通过加入3个反向流（backward-flow）任务使得兴趣向量更有区分性，更有效。</p><span id="more"></span><h3 id="解读"><a href="#解读" class="headerlink" title="解读"></a>解读</h3><p><strong>解法</strong></p><p>现有的多兴趣建模尽管很有效，但是仅使用编码器利用前向流（forward-flow）表征用户多兴趣。由于没有加入显示的约束，此方式无法保证兴趣向量的正交，也无法保证兴趣向量和历史item在语义空间的关系。本文提出Re4的框架，通过加入3个反向流（backward-flow）任务，重新检验兴趣向量和item的关系，使得兴趣向量更有区分性，更有效。</p><p>本文通过3个反向的辅助任务对用户多兴趣表征进行约束，使兴趣向量能1）捕捉不同方面的兴趣  2）反应相关的item 3）行为编码（forward flow）中的attention和最终推荐任务的相关性指标一致。</p><p><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022[Re4](WWW)/image-20220719110738434.png" alt="image-20220719110738434" style="zoom:50%;"></p><ul><li>Re-contrast：通过兴趣向量的对比保证兴趣向量的区分性</li></ul><p>正样本对：兴趣向量；兴趣向量相关item</p><p>负样本：兴趣向量：其他兴趣向量；随机sample item</p><p><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022[Re4](WWW)/image-20220718192342966.png" alt="image-20220718192342966" style="zoom:50%;"></p><ul><li>Re-attend: 使得编码中的相关性度量和最后RS的相关性度量（如Match阶段）匹配</li></ul><p>item i 和兴趣的权重 和 RS计算相似度的方式计算的权重</p><p><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022[Re4](WWW)/image-20220718192005631.png" alt="image-20220718192005631" style="zoom:50%;"></p><p><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022[Re4](WWW)/image-20220718192125534.png" alt="image-20220718192125534" style="zoom:50%;"></p><p><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022[Re4](WWW)/image-20220718192320834.png" alt="image-20220718192320834" style="zoom:50%;"></p><ul><li>Re-construct: 通过兴趣向量反向预估item embedding，保证兴趣向量能还原相关item</li></ul><p><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022[Re4](WWW)/image-20220718192544491.png" alt="image-20220718192544491" style="zoom:50%;"></p><p><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022[Re4](WWW)/image-20220718192602342.png" alt="image-20220718192602342" style="zoom:50%;"></p><p><strong>实验</strong></p><p><img src="/2022/05/20/2_%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/2022[Re4](WWW)/image-20220718193024716.png" alt="image-20220718193024716" style="zoom:50%;"></p><p><strong>评论</strong></p><p>通过Re-contrast增加不同兴趣的区分性。有点兴趣解耦的意思</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;(1)&lt;/p&gt;
&lt;p&gt;Zhang, S.; Yang, L.; Yao, D.; Lu, Y.; Feng, F.; Zhao, Z.; Chua, T.; Wu, F. Re4: Learning to Re-Contrast, Re-Attend, Re-Construct for Multi-Interest Recommendation. In &lt;em&gt;Proceedings of the ACM Web Conference 2022&lt;/em&gt;; 2022; pp 2216–2226.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;简介&lt;/strong&gt;：现有的多兴趣建模尽管很有效，但是仅使用编码器利用前向流（forward-flow）表征用户多兴趣。由于没有加入显示的约束，此方式无法保证兴趣向量的正交，也无法保证兴趣向量和历史item在语义空间的关系。本文提出Re4的框架，通过加入3个反向流（backward-flow）任务使得兴趣向量更有区分性，更有效。&lt;/p&gt;
    
    </summary>
    
      <category term="算法相关" scheme="http://tessiehe.github.io/categories/%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3/"/>
    
    
      <category term="论文阅读" scheme="http://tessiehe.github.io/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
      <category term="用户行为建模" scheme="http://tessiehe.github.io/tags/%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E5%BB%BA%E6%A8%A1/"/>
    
      <category term="对比学习" scheme="http://tessiehe.github.io/tags/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="多兴趣建模" scheme="http://tessiehe.github.io/tags/%E5%A4%9A%E5%85%B4%E8%B6%A3%E5%BB%BA%E6%A8%A1/"/>
    
  </entry>
  
</feed>
